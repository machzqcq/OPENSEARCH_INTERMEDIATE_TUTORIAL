{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3087d74",
   "metadata": {},
   "source": [
    "# Build Meta Dictionary Using DeepSeek API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df554b04",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365c6847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìä Pandas version: 2.2.3\n",
      "üóÑÔ∏è SQLAlchemy version: 2.0.42\n",
      "üîå psycopg2 version: 2.9.11 (dt dec pq3 ext lo64)\n",
      "‚úÖ Required database environment variables found\n",
      "‚úÖ OpenAI API key found\n",
      "‚úÖ Anthropic API key found\n",
      "‚úÖ Google API key found\n",
      "‚úÖ DeepSeek API key found\n",
      "Available LLM providers: OpenAI, Anthropic, Google, DeepSeek\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for data manipulation and database connectivity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connectivity\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text, MetaData, inspect\n",
    "import psycopg2\n",
    "import urllib.parse\n",
    "\n",
    "# Excel file handling\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Environment variables and configuration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM API calls\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Set up plotting styles\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üóÑÔ∏è SQLAlchemy version: {sqlalchemy.__version__}\")\n",
    "print(f\"üîå psycopg2 version: {psycopg2.__version__}\")\n",
    "\n",
    "# Check for required environment variables\n",
    "required_env_vars = ['POSTGRES_HOST', 'POSTGRES_DB']\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\\\n‚ö†Ô∏è  Missing required environment variables: {missing_vars}\")\n",
    "    print(\"Please set these in your .env file or environment\")\n",
    "else:\n",
    "    print(\"‚úÖ Required database environment variables found\")\n",
    "\n",
    "# Check for LLM API keys\n",
    "llm_providers = {\n",
    "    'OpenAI': 'OPENAI_API_KEY',\n",
    "    'Anthropic': 'ANTHROPIC_API_KEY', \n",
    "    'Google': 'GOOGLE_API_KEY',\n",
    "    'DeepSeek': 'DEEPSEEK_API_KEY'\n",
    "}\n",
    "\n",
    "available_providers = []\n",
    "for provider, env_var in llm_providers.items():\n",
    "    if os.getenv(env_var):\n",
    "        available_providers.append(provider)\n",
    "        print(f\"‚úÖ {provider} API key found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {provider} API key missing ({env_var})\")\n",
    "\n",
    "if not available_providers:\n",
    "    print(\"No LLM API keys found! Please set at least one API key.\")\n",
    "else:\n",
    "    print(f\"Available LLM providers: {', '.join(available_providers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ca315",
   "metadata": {},
   "source": [
    "# Connect to PostgreSQL Database\n",
    "\n",
    "This section establishes a connection to the PostgreSQL database using SQLAlchemy with the psycopg2 driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a9dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to PostgreSQL database!\n",
      "üìä Server: localhost:5432\n",
      "üóÑÔ∏è Database: Adventureworks\n",
      "\n",
      "üöÄ Database connector ready for use!\n"
     ]
    }
   ],
   "source": [
    "class PostgreSQLConnector:\n",
    "    \"\"\"\n",
    "    PostgreSQL Database Connector using SQLAlchemy and psycopg2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.server = os.getenv('POSTGRES_HOST')\n",
    "        self.database = os.getenv('POSTGRES_DB')\n",
    "        self.username = os.getenv('POSTGRES_USER')\n",
    "        self.password = os.getenv('POSTGRES_PASSWORD')\n",
    "        self.port = int(os.getenv('POSTGRES_PORT', '5432'))\n",
    "        # PostgreSQL doesn't use Windows Authentication\n",
    "        \n",
    "        self.engine = None\n",
    "        self.connection_string = None\n",
    "        \n",
    "    def create_connection_string(self):\n",
    "        \"\"\"Create the connection string for PostgreSQL using psycopg2\"\"\"\n",
    "        # PostgreSQL uses standard username/password authentication\n",
    "        if not self.username or not self.password:\n",
    "            raise ValueError(\"Username and password required for PostgreSQL authentication\")\n",
    "        \n",
    "        # URL encode the password to handle special characters\n",
    "        encoded_password = urllib.parse.quote_plus(self.password)\n",
    "        encoded_username = urllib.parse.quote_plus(self.username)\n",
    "        \n",
    "        # psycopg2 connection string format\n",
    "        self.connection_string = (\n",
    "            f\"postgresql+psycopg2://{encoded_username}:{encoded_password}@\"\n",
    "            f\"{self.server}:{self.port}/{self.database}\"\n",
    "        )\n",
    "        \n",
    "        return self.connection_string\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection\"\"\"\n",
    "        try:\n",
    "            if not self.connection_string:\n",
    "                self.create_connection_string()\n",
    "            \n",
    "            self.engine = create_engine(self.connection_string)\n",
    "            \n",
    "            # Test the connection\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT 1 as test\")).fetchone()\n",
    "                if result[0] == 1:\n",
    "                    print(\"‚úÖ Successfully connected to PostgreSQL database!\")\n",
    "                    print(f\"üìä Server: {self.server}:{self.port}\")\n",
    "                    print(f\"üóÑÔ∏è Database: {self.database}\")\n",
    "                    return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to connect to database: {str(e)}\")\n",
    "            print(\"\\nüí° Troubleshooting tips:\")\n",
    "            print(\"1. Check if PostgreSQL is running\")\n",
    "            print(\"2. Verify server name and database name\")\n",
    "            print(\"3. Check credentials (username and password)\")\n",
    "            print(\"4. Ensure PostgreSQL is configured to allow connections\")\n",
    "            print(\"5. Check firewall settings and network connectivity\")\n",
    "            print(\"6. Verify the port number (default: 5432)\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Execute a SQL query and return results as DataFrame\"\"\"\n",
    "        try:\n",
    "            if not self.engine:\n",
    "                print(\"‚ùå No database connection. Call connect() first.\")\n",
    "                return None\n",
    "            \n",
    "            if params:\n",
    "                df = pd.read_sql_query(text(query), self.engine, params=params)\n",
    "            else:\n",
    "                df = pd.read_sql_query(text(query), self.engine)\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error executing query: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_tables(self):\n",
    "        \"\"\"Get list of all tables in the database\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            TABLE_SCHEMA,\n",
    "            TABLE_NAME,\n",
    "            TABLE_TYPE\n",
    "        FROM INFORMATION_SCHEMA.TABLES\n",
    "        WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "        ORDER BY TABLE_SCHEMA, TABLE_NAME\n",
    "        \"\"\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_columns(self, schema_name=None, table_name=None):\n",
    "        \"\"\"Get column information for tables\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            TABLE_SCHEMA,\n",
    "            TABLE_NAME,\n",
    "            COLUMN_NAME,\n",
    "            DATA_TYPE,\n",
    "            CHARACTER_MAXIMUM_LENGTH,\n",
    "            IS_NULLABLE,\n",
    "            COLUMN_DEFAULT,\n",
    "            ORDINAL_POSITION\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        \"\"\"\n",
    "        \n",
    "        conditions = []\n",
    "        params = {}\n",
    "        \n",
    "        if schema_name:\n",
    "            conditions.append(\"TABLE_SCHEMA = :schema_name\")\n",
    "            params['schema_name'] = schema_name\n",
    "            \n",
    "        if table_name:\n",
    "            conditions.append(\"TABLE_NAME = :table_name\")\n",
    "            params['table_name'] = table_name\n",
    "        \n",
    "        if conditions:\n",
    "            query += \" WHERE \" + \" AND \".join(conditions)\n",
    "        \n",
    "        query += \" ORDER BY TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION\"\n",
    "        \n",
    "        return self.execute_query(query, params if params else None)\n",
    "\n",
    "# Initialize the database connector\n",
    "db_connector = PostgreSQLConnector()\n",
    "\n",
    "# Test the connection\n",
    "if db_connector.connect():\n",
    "    print(\"\\nüöÄ Database connector ready for use!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Database connection failed. Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6997c3",
   "metadata": {},
   "source": [
    "# Extract Database Metadata and Save to local Excel File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8dfe890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting database metadata...\n",
      "üö´ Excluding schemas: information_schema, pg_catalog\n",
      "üîÑ Standardizing column names...\n",
      "‚úÖ Successfully extracted metadata for 456 columns\n",
      "üìä Found 68 tables\n",
      "üè∑Ô∏è Schemas: humanresources, person, production, purchasing, sales\n",
      "\n",
      "üìã Sample metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_SCHEMA</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>CHARACTER_MAXIMUM_LENGTH</th>\n",
       "      <th>NUMERIC_PRECISION</th>\n",
       "      <th>NUMERIC_SCALE</th>\n",
       "      <th>IS_NULLABLE</th>\n",
       "      <th>COLUMN_DEFAULT</th>\n",
       "      <th>ORDINAL_POSITION</th>\n",
       "      <th>COLUMN_DESCRIPTION</th>\n",
       "      <th>TABLE_TYPE</th>\n",
       "      <th>FULL_DATA_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>departmentid</td>\n",
       "      <td>integer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>nextval('humanresources.department_departmenti...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>integer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>name</td>\n",
       "      <td>character varying</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>character varying(50)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>groupname</td>\n",
       "      <td>character varying</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>character varying(50)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>modifieddate</td>\n",
       "      <td>timestamp without time zone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>now()</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>timestamp without time zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>integer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>integer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>nationalidnumber</td>\n",
       "      <td>character varying</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>character varying(15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>loginid</td>\n",
       "      <td>character varying</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>character varying(256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>jobtitle</td>\n",
       "      <td>character varying</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>character varying(50)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>birthdate</td>\n",
       "      <td>date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>maritalstatus</td>\n",
       "      <td>character</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>character(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TABLE_SCHEMA  TABLE_NAME       COLUMN_NAME                    DATA_TYPE  \\\n",
       "0  humanresources  department      departmentid                      integer   \n",
       "1  humanresources  department              name            character varying   \n",
       "2  humanresources  department         groupname            character varying   \n",
       "3  humanresources  department      modifieddate  timestamp without time zone   \n",
       "4  humanresources    employee  businessentityid                      integer   \n",
       "5  humanresources    employee  nationalidnumber            character varying   \n",
       "6  humanresources    employee           loginid            character varying   \n",
       "7  humanresources    employee          jobtitle            character varying   \n",
       "8  humanresources    employee         birthdate                         date   \n",
       "9  humanresources    employee     maritalstatus                    character   \n",
       "\n",
       "   CHARACTER_MAXIMUM_LENGTH  NUMERIC_PRECISION  NUMERIC_SCALE IS_NULLABLE  \\\n",
       "0                       NaN               32.0            0.0          NO   \n",
       "1                      50.0                NaN            NaN          NO   \n",
       "2                      50.0                NaN            NaN          NO   \n",
       "3                       NaN                NaN            NaN          NO   \n",
       "4                       NaN               32.0            0.0          NO   \n",
       "5                      15.0                NaN            NaN          NO   \n",
       "6                     256.0                NaN            NaN          NO   \n",
       "7                      50.0                NaN            NaN          NO   \n",
       "8                       NaN                NaN            NaN          NO   \n",
       "9                       1.0                NaN            NaN          NO   \n",
       "\n",
       "                                      COLUMN_DEFAULT  ORDINAL_POSITION  \\\n",
       "0  nextval('humanresources.department_departmenti...                 1   \n",
       "1                                               None                 2   \n",
       "2                                               None                 3   \n",
       "3                                              now()                 4   \n",
       "4                                               None                 1   \n",
       "5                                               None                 2   \n",
       "6                                               None                 3   \n",
       "7                                               None                 6   \n",
       "8                                               None                 7   \n",
       "9                                               None                 8   \n",
       "\n",
       "  COLUMN_DESCRIPTION  TABLE_TYPE               FULL_DATA_TYPE  \n",
       "0                     BASE TABLE                      integer  \n",
       "1                     BASE TABLE        character varying(50)  \n",
       "2                     BASE TABLE        character varying(50)  \n",
       "3                     BASE TABLE  timestamp without time zone  \n",
       "4                     BASE TABLE                      integer  \n",
       "5                     BASE TABLE        character varying(15)  \n",
       "6                     BASE TABLE       character varying(256)  \n",
       "7                     BASE TABLE        character varying(50)  \n",
       "8                     BASE TABLE                         date  \n",
       "9                     BASE TABLE                 character(1)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata saved to: /home/ubuntu/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/opensearch-POSTGRES-RAG/metadata_Adventureworks.xlsx\n",
      "üìä File size: 32982 bytes\n",
      "\n",
      "üíæ Metadata successfully saved to Excel file!\n",
      "\n",
      "üí° Schema Exclusion Options:\n",
      "   ‚Ä¢ Default: excludes 'information_schema' and 'pg_catalog'\n",
      "   ‚Ä¢ Custom: metadata_extractor.extract_full_metadata(exclude_schemas=['schema1', 'schema2'])\n",
      "   ‚Ä¢ All schemas: metadata_extractor.extract_full_metadata(exclude_schemas=[])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "class DatabaseMetadataExtractor:\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from PostgreSQL database\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connector):\n",
    "        self.db_connector = db_connector\n",
    "        self.metadata_df = None\n",
    "        \n",
    "    def extract_full_metadata(self, exclude_schemas=None):\n",
    "        \"\"\"\n",
    "        Extract comprehensive metadata from PostgreSQL\n",
    "        \n",
    "        Parameters:\n",
    "        - exclude_schemas: List of schema names to exclude (default: ['information_schema', 'pg_catalog'])\n",
    "                          Set to [] to include all schemas\n",
    "        \"\"\"\n",
    "        if exclude_schemas is None:\n",
    "            # Default system schemas to exclude\n",
    "            exclude_schemas = ['information_schema', 'pg_catalog']\n",
    "        \n",
    "        # Build the WHERE clause for schema exclusion\n",
    "        if exclude_schemas:\n",
    "            schema_exclusion = \" AND t.TABLE_SCHEMA NOT IN ('\" + \"', '\".join(exclude_schemas) + \"')\"\n",
    "        else:\n",
    "            schema_exclusion = \"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            t.TABLE_SCHEMA,\n",
    "            t.TABLE_NAME,\n",
    "            c.COLUMN_NAME,\n",
    "            c.DATA_TYPE,\n",
    "            c.CHARACTER_MAXIMUM_LENGTH,\n",
    "            c.NUMERIC_PRECISION,\n",
    "            c.NUMERIC_SCALE,\n",
    "            c.IS_NULLABLE,\n",
    "            c.COLUMN_DEFAULT,\n",
    "            c.ORDINAL_POSITION,\n",
    "            -- PostgreSQL doesn't have extended properties like MSSQL\n",
    "            '' as COLUMN_DESCRIPTION,\n",
    "            -- Additional table information\n",
    "            t.TABLE_TYPE,\n",
    "            -- Create a readable data type description\n",
    "            CASE \n",
    "                WHEN c.DATA_TYPE IN ('character varying', 'character', 'varchar', 'char') \n",
    "                    THEN c.DATA_TYPE || '(' || c.CHARACTER_MAXIMUM_LENGTH::varchar || ')'\n",
    "                WHEN c.DATA_TYPE IN ('decimal', 'numeric') \n",
    "                    THEN c.DATA_TYPE || '(' || c.NUMERIC_PRECISION::varchar || ',' || c.NUMERIC_SCALE::varchar || ')'\n",
    "                ELSE c.DATA_TYPE\n",
    "            END as FULL_DATA_TYPE\n",
    "        FROM INFORMATION_SCHEMA.TABLES t\n",
    "        INNER JOIN INFORMATION_SCHEMA.COLUMNS c \n",
    "            ON t.TABLE_SCHEMA = c.TABLE_SCHEMA \n",
    "            AND t.TABLE_NAME = c.TABLE_NAME\n",
    "        WHERE t.TABLE_TYPE = 'BASE TABLE'{schema_exclusion}\n",
    "        ORDER BY t.TABLE_SCHEMA, t.TABLE_NAME, c.ORDINAL_POSITION\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîç Extracting database metadata...\")\n",
    "        if exclude_schemas:\n",
    "            print(f\"üö´ Excluding schemas: {', '.join(exclude_schemas)}\")\n",
    "        else:\n",
    "            print(\"üìã Including all schemas\")\n",
    "            \n",
    "        self.metadata_df = self.db_connector.execute_query(query)\n",
    "        \n",
    "        if self.metadata_df is not None:\n",
    "            # PostgreSQL returns lowercase column names, so let's standardize them\n",
    "            print(\"üîÑ Standardizing column names...\")\n",
    "            \n",
    "            # Create a mapping for column name standardization\n",
    "            column_mapping = {}\n",
    "            for col in self.metadata_df.columns:\n",
    "                column_mapping[col] = col.upper()\n",
    "            \n",
    "            # Rename columns to uppercase for consistency\n",
    "            self.metadata_df = self.metadata_df.rename(columns=column_mapping)\n",
    "            \n",
    "            print(f\"‚úÖ Successfully extracted metadata for {len(self.metadata_df)} columns\")\n",
    "            print(f\"üìä Found {self.metadata_df['TABLE_NAME'].nunique()} tables\")\n",
    "            print(f\"üè∑Ô∏è Schemas: {', '.join(self.metadata_df['TABLE_SCHEMA'].unique())}\")\n",
    "            \n",
    "            return self.metadata_df\n",
    "        else:\n",
    "            print(\"‚ùå Failed to extract metadata\")\n",
    "            return None\n",
    "    \n",
    "    def save_to_excel(self, filename=\"database_metadata.xlsx\"):\n",
    "        \"\"\"Save metadata to Excel file\"\"\"\n",
    "        if self.metadata_df is None:\n",
    "            print(\"‚ùå No metadata to save. Run extract_full_metadata() first.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            filepath = os.path.join(os.getcwd(), filename)\n",
    "            \n",
    "            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "                # Main metadata sheet\n",
    "                self.metadata_df.to_excel(writer, sheet_name='Metadata', index=False)\n",
    "                \n",
    "                # Summary sheet\n",
    "                summary_data = {\n",
    "                    'Metric': [\n",
    "                        'Total Tables',\n",
    "                        'Total Columns', \n",
    "                        'Schemas',\n",
    "                        'Data Types Used',\n",
    "                        'Tables with Descriptions',\n",
    "                        'Columns with Descriptions'\n",
    "                    ],\n",
    "                    'Value': [\n",
    "                        self.metadata_df['TABLE_NAME'].nunique(),\n",
    "                        len(self.metadata_df),\n",
    "                        ', '.join(self.metadata_df['TABLE_SCHEMA'].unique()),\n",
    "                        ', '.join(self.metadata_df['DATA_TYPE'].unique()),\n",
    "                        len(self.metadata_df[self.metadata_df['COLUMN_DESCRIPTION'].notna()]['TABLE_NAME'].unique()),\n",
    "                        len(self.metadata_df[self.metadata_df['COLUMN_DESCRIPTION'].notna()])\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "                \n",
    "                # Table list sheet\n",
    "                table_summary = self.metadata_df.groupby(['TABLE_SCHEMA', 'TABLE_NAME']).agg({\n",
    "                    'COLUMN_NAME': 'count',\n",
    "                    'COLUMN_DESCRIPTION': lambda x: x.notna().sum()\n",
    "                }).rename(columns={\n",
    "                    'COLUMN_NAME': 'Column_Count',\n",
    "                    'COLUMN_DESCRIPTION': 'Described_Columns'\n",
    "                }).reset_index()\n",
    "                \n",
    "                table_summary.to_excel(writer, sheet_name='Tables', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Metadata saved to: {filepath}\")\n",
    "            print(f\"üìä File size: {os.path.getsize(filepath)} bytes\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving to Excel: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Initialize metadata extractor\n",
    "metadata_extractor = DatabaseMetadataExtractor(db_connector)\n",
    "\n",
    "# Extract metadata if database connection is available\n",
    "if db_connector.engine:\n",
    "    # Example usage with custom schema exclusions:\n",
    "    # metadata_df = metadata_extractor.extract_full_metadata(exclude_schemas=['information_schema', 'pg_catalog', 'custom_schema'])\n",
    "    # Or to include all schemas:\n",
    "    # metadata_df = metadata_extractor.extract_full_metadata(exclude_schemas=[])\n",
    "    \n",
    "    metadata_df = metadata_extractor.extract_full_metadata()  # Uses default exclusions\n",
    "    \n",
    "    if metadata_df is not None:\n",
    "        print(\"\\nüìã Sample metadata:\")\n",
    "        display(metadata_df.head(10))\n",
    "        \n",
    "        # Save to Excel\n",
    "        if metadata_extractor.save_to_excel(f\"metadata_{db_connector.database}.xlsx\"):\n",
    "            print(\"\\nüíæ Metadata successfully saved to Excel file!\")\n",
    "            print(\"\\nüí° Schema Exclusion Options:\")\n",
    "            print(\"   ‚Ä¢ Default: excludes 'information_schema' and 'pg_catalog'\")\n",
    "            print(\"   ‚Ä¢ Custom: metadata_extractor.extract_full_metadata(exclude_schemas=['schema1', 'schema2'])\")\n",
    "            print(\"   ‚Ä¢ All schemas: metadata_extractor.extract_full_metadata(exclude_schemas=[])\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to extract metadata\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No database connection available. Skipping metadata extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41275905",
   "metadata": {},
   "source": [
    "# Build column descriptions metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48352376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Setup\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "SAMPLING_COUNT = 10\n",
    "SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA = [\"dbo\"]\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "# You'll need to set your DeepSeek API key here\n",
    "# Get your API key from: https://platform.deepseek.com/\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")  # Replace with your actual API key\n",
    "\n",
    "if DEEPSEEK_API_KEY == \"your_deepseek_api_key_here\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: Please set your DeepSeek API key in the DEEPSEEK_API_KEY variable\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4c91d",
   "metadata": {},
   "source": [
    "# Sequential calls to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f41bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting inference of column descriptions using DeepSeek API...\n",
      "üìä Total columns in metadata: 1078\n",
      "üö´ Excluded schemas: dbo\n",
      "‚öôÔ∏è  Columns to process (after exclusion): 1078\n",
      "‚è≠Ô∏è  Columns skipped: 0\n",
      "  üîç [1/1078] Processing humanresources.department.departmentid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This integer column stores unique identifiers for departments, serving as a prim...\n",
      "  üîç [2/1078] Processing humanresources.department.name\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This integer column stores unique identifiers for departments, serving as a prim...\n",
      "  üîç [2/1078] Processing humanresources.department.name\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names within an organization, representing funct...\n",
      "  üîç [3/1078] Processing humanresources.department.groupname\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names within an organization, representing funct...\n",
      "  üîç [3/1078] Processing humanresources.department.groupname\n",
      "    üìà Retrieved 10 sample values\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Execute the workflow\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmetadata_df\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m metadata_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# Generate inferred descriptions with schema exclusions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     metadata_df_enhanced = \u001b[43madd_inferred_descriptions_to_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdb_connector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschemas_to_exclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# Display sample results\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìã Sample results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36madd_inferred_descriptions_to_metadata\u001b[39m\u001b[34m(metadata_df, db_connector, schemas_to_exclude)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    üìà Retrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sample values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Generate description using DeepSeek API\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m description = \u001b[43mcall_deepseek_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m df_to_process.at[idx, \u001b[33m'\u001b[39m\u001b[33mINFERRED_COLUMN_DESCRIPTION\u001b[39m\u001b[33m'\u001b[39m] = description\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    ‚úÖ Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription[:\u001b[32m80\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mcall_deepseek_api\u001b[39m\u001b[34m(sample_values, column_name, table_name, data_type)\u001b[39m\n\u001b[32m     25\u001b[39m headers = {\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEEPSEEK_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m }\n\u001b[32m     30\u001b[39m data = {\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeepseek-chat\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m\n\u001b[32m     40\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEEPSEEK_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     45\u001b[39m     result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/http/client.py:1378\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1380\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/http/client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/http/client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define DeepSeek API Call Function\n",
    "def call_deepseek_api(sample_values, column_name, table_name, data_type):\n",
    "    \"\"\"\n",
    "    Call DeepSeek API to generate a description for the sampled data\n",
    "    \"\"\"\n",
    "    if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == \"your_deepseek_api_key_here\":\n",
    "        # Return a placeholder description if API key is not set\n",
    "        return f\"AI description needed for {column_name} column\"\n",
    "    \n",
    "    try:\n",
    "        # Prepare the prompt with sample values\n",
    "        sample_str = \", \".join([str(val) for val in sample_values[:10] if val is not None])\n",
    "        \n",
    "        if not sample_str:\n",
    "            return f\"{data_type} column in {table_name}\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on these sample values from the column '{column_name}' (data type: {data_type}) in the table '{table_name}':\n",
    "        {sample_str}\n",
    "        \n",
    "        Please provide a concise description of what this column contains in less than 40 words.\n",
    "        Focus on the type of data, its purpose, and any patterns you observe.\n",
    "        \"\"\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"deepseek-chat\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            description = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return description\n",
    "        else:\n",
    "            print(f\"  ‚ùå API Error {response.status_code}: {response.text}\")\n",
    "            return f\"API error for {column_name}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Exception calling API for {column_name}: {str(e)}\")\n",
    "        return f\"Error generating description for {column_name}\"\n",
    "\n",
    "\n",
    "def get_sample_values_from_db(db_connector, schema, table, column, sample_size=SAMPLING_COUNT):\n",
    "    \"\"\"\n",
    "    Get random sample values from the database for a specific column (PostgreSQL version)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PostgreSQL query to get random sample values\n",
    "        query = f\"\"\"\n",
    "        SELECT \"{column}\"\n",
    "        FROM \"{schema}\".\"{table}\"\n",
    "        WHERE \"{column}\" IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        result_df = db_connector.execute_query(query)\n",
    "        \n",
    "        if result_df is not None and not result_df.empty:\n",
    "            # Return list of values (handle both uppercase and lowercase column names)\n",
    "            column_lower = column.lower()\n",
    "            if column_lower in result_df.columns:\n",
    "                return result_df[column_lower].tolist()\n",
    "            elif column in result_df.columns:\n",
    "                return result_df[column].tolist()\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Column {column} not found in result\")\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error sampling {schema}.{table}.{column}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def add_inferred_descriptions_to_metadata(metadata_df, db_connector, schemas_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Add INFERRED_COLUMN_DESCRIPTION column to metadata_df by querying the database\n",
    "    and using DeepSeek API to generate descriptions\n",
    "    \n",
    "    Parameters:\n",
    "    - metadata_df: DataFrame with metadata (must have TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE columns)\n",
    "    - db_connector: PostgreSQLConnector instance with active connection\n",
    "    - schemas_to_exclude: List of schema names to skip (won't call DeepSeek API for these)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df with new INFERRED_COLUMN_DESCRIPTION column\n",
    "    \"\"\"\n",
    "    if schemas_to_exclude is None:\n",
    "        schemas_to_exclude = []\n",
    "    \n",
    "    # Filter out excluded schemas\n",
    "    df_to_process = metadata_df[~metadata_df['TABLE_SCHEMA'].isin(schemas_to_exclude)].copy()\n",
    "    df_excluded = metadata_df[metadata_df['TABLE_SCHEMA'].isin(schemas_to_exclude)].copy()\n",
    "    \n",
    "    print(\"üöÄ Starting inference of column descriptions using DeepSeek API...\")\n",
    "    print(f\"üìä Total columns in metadata: {len(metadata_df)}\")\n",
    "    print(f\"üö´ Excluded schemas: {', '.join(schemas_to_exclude) if schemas_to_exclude else 'None'}\")\n",
    "    print(f\"‚öôÔ∏è  Columns to process (after exclusion): {len(df_to_process)}\")\n",
    "    print(f\"‚è≠Ô∏è  Columns skipped: {len(df_excluded)}\")\n",
    "    \n",
    "    # Add new column for inferred descriptions\n",
    "    df_to_process['INFERRED_COLUMN_DESCRIPTION'] = \"\"\n",
    "    df_excluded['INFERRED_COLUMN_DESCRIPTION'] = \"Excluded schema - not processed\"\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df_to_process.iterrows():\n",
    "        schema = row['TABLE_SCHEMA']\n",
    "        table = row['TABLE_NAME']\n",
    "        column = row['COLUMN_NAME']\n",
    "        data_type = row['DATA_TYPE']\n",
    "        \n",
    "        # Get the position in the filtered dataframe\n",
    "        position = list(df_to_process.index).index(idx) + 1\n",
    "        print(f\"  üîç [{position}/{len(df_to_process)}] Processing {schema}.{table}.{column}\")\n",
    "        \n",
    "        # Get sample values from database\n",
    "        sample_values = get_sample_values_from_db(db_connector, schema, table, column, sample_size=SAMPLING_COUNT)\n",
    "        \n",
    "        if sample_values:\n",
    "            print(f\"    üìà Retrieved {len(sample_values)} sample values\")\n",
    "            \n",
    "            # Generate description using DeepSeek API\n",
    "            description = call_deepseek_api(sample_values, column, f\"{schema}.{table}\", data_type)\n",
    "            df_to_process.at[idx, 'INFERRED_COLUMN_DESCRIPTION'] = description\n",
    "            print(f\"    ‚úÖ Generated: {description[:80]}...\")\n",
    "        else:\n",
    "            # Use a basic description if no sample data\n",
    "            df_to_process.at[idx, 'INFERRED_COLUMN_DESCRIPTION'] = f\"{data_type} column in {schema}.{table}\"\n",
    "            print(f\"    ‚ö†Ô∏è  No sample data available, using basic description\")\n",
    "    \n",
    "    # Combine processed and excluded dataframes\n",
    "    result_df = pd.concat([df_to_process, df_excluded], ignore_index=False).sort_index()\n",
    "    \n",
    "    print(f\"\\nüéâ Completed!\")\n",
    "    print(f\"‚úÖ Generated descriptions for {len(df_to_process)} columns\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped {len(df_excluded)} columns from excluded schemas\")\n",
    "    print(f\"üìä Total rows in result: {len(result_df)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def save_enhanced_metadata_to_excel(metadata_df_enhanced, db_connector):\n",
    "    \"\"\"\n",
    "    Save the enhanced metadata with inferred descriptions to a new sheet in the existing Excel file\n",
    "    \n",
    "    Parameters:\n",
    "    - metadata_df_enhanced: DataFrame with INFERRED_DESCRIPTION column\n",
    "    - db_connector: PostgreSQLConnector instance (to get database name)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean indicating success\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle working directory issues\n",
    "        try:\n",
    "            current_dir = os.getcwd()\n",
    "        except FileNotFoundError:\n",
    "            import tempfile\n",
    "            current_dir = tempfile.gettempdir()\n",
    "        \n",
    "        filename = f\"metadata_{db_connector.database}.xlsx\"\n",
    "        filepath = os.path.join(current_dir, filename)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ö†Ô∏è  File {filename} not found. Creating new file...\")\n",
    "            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "                metadata_df_enhanced.to_excel(writer, sheet_name='Metadata_Enhanced', index=False)\n",
    "            print(f\"‚úÖ New file created: {filepath}\")\n",
    "            return True\n",
    "        \n",
    "        # Read existing Excel file\n",
    "        print(f\"üìÇ Opening existing file: {filename}\")\n",
    "        \n",
    "        # Use openpyxl to preserve existing sheets\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            # Add or replace the Metadata_Enhanced sheet\n",
    "            metadata_df_enhanced.to_excel(writer, sheet_name='Metadata_Enhanced', index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Enhanced metadata saved to sheet 'Metadata_Enhanced' in: {filepath}\")\n",
    "        print(f\"üìä File size: {os.path.getsize(filepath)} bytes\")\n",
    "        print(f\"üìã Total rows saved: {len(metadata_df_enhanced)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving enhanced metadata: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Execute the workflow\n",
    "if 'metadata_df' in locals() and metadata_df is not None:\n",
    "    # Generate inferred descriptions with schema exclusions\n",
    "    metadata_df_enhanced = add_inferred_descriptions_to_metadata(\n",
    "        metadata_df, \n",
    "        db_connector,\n",
    "        schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\n",
    "    )\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nüìã Sample results:\")\n",
    "    display(metadata_df_enhanced[['TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'INFERRED_COLUMN_DESCRIPTION']].head(10))\n",
    "    \n",
    "    # Save to Excel\n",
    "    print(\"\\nüíæ Saving enhanced metadata to Excel...\")\n",
    "    if save_enhanced_metadata_to_excel(metadata_df_enhanced, db_connector):\n",
    "        print(\"\\nüéâ Success! Enhanced metadata with AI-generated descriptions has been saved.\")\n",
    "        print(f\"üìÅ Check the 'Metadata_Enhanced' sheet in metadata_{db_connector.database}.xlsx\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to save enhanced metadata.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  metadata_df not available. Run the metadata extraction cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c9d23",
   "metadata": {},
   "source": [
    "# Build Table Descriptions Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662a977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table description functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# Table-Level Description Functions\n",
    "def get_sample_rows_from_table(db_connector, schema, table, sample_size=5):\n",
    "    \"\"\"\n",
    "    Get random sample rows from a table to understand its structure and data\n",
    "    \n",
    "    Parameters:\n",
    "    - db_connector: PostgreSQLConnector instance\n",
    "    - schema: Schema name\n",
    "    - table: Table name\n",
    "    - sample_size: Number of rows to sample (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with sample rows\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PostgreSQL query to get random sample rows\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM \"{schema}\".\"{table}\"\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        result_df = db_connector.execute_query(query)\n",
    "        \n",
    "        if result_df is not None and not result_df.empty:\n",
    "            return result_df\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error sampling table {schema}.{table}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def call_deepseek_api_for_table(sample_df, schema, table, column_list):\n",
    "    \"\"\"\n",
    "    Call DeepSeek API to generate a table-level description based on sample rows\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_df: DataFrame with sample rows\n",
    "    - schema: Schema name\n",
    "    - table: Table name\n",
    "    - column_list: List of column names in the table\n",
    "    \n",
    "    Returns:\n",
    "    - String description of the table\n",
    "    \"\"\"\n",
    "    if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == \"your_deepseek_api_key_here\":\n",
    "        return f\"AI table description needed for {schema}.{table}\"\n",
    "    \n",
    "    try:\n",
    "        # Convert sample data to readable format\n",
    "        sample_summary = []\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            row_data = {col: str(val)[:50] for col, val in row.items()}  # Truncate long values\n",
    "            sample_summary.append(row_data)\n",
    "        \n",
    "        # Create a concise representation\n",
    "        sample_str = json.dumps(sample_summary[:3], indent=2)[:2000]  # Limit to 2000 chars\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on this sample data from the table '{schema}.{table}':\n",
    "        \n",
    "        Columns: {', '.join(column_list[:10])}{'...' if len(column_list) > 10 else ''}\n",
    "        Total Columns: {len(column_list)}\n",
    "        \n",
    "        Sample Rows:\n",
    "        {sample_str}\n",
    "        \n",
    "        Please provide a concise description (max 60 words) of what this table stores, \n",
    "        its business purpose, and the type of data it contains.\n",
    "        \"\"\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"deepseek-chat\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": 400,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            description = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return description\n",
    "        else:\n",
    "            print(f\"  ‚ùå Table API Error {response.status_code}: {response.text}\")\n",
    "            return f\"API error for table {schema}.{table}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Exception calling API for table {schema}.{table}: {str(e)}\")\n",
    "        return f\"Error generating description for table {schema}.{table}\"\n",
    "\n",
    "\n",
    "def add_table_descriptions_to_metadata(metadata_df, db_connector, schemas_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Add INFERRED_TABLE_DESCRIPTION to metadata_df by sampling table rows\n",
    "    \n",
    "    Parameters:\n",
    "    - metadata_df: DataFrame with metadata\n",
    "    - db_connector: PostgreSQLConnector instance\n",
    "    - schemas_to_exclude: List of schema names to skip\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df with INFERRED_TABLE_DESCRIPTION column\n",
    "    \"\"\"\n",
    "    if schemas_to_exclude is None:\n",
    "        schemas_to_exclude = []\n",
    "    \n",
    "    print(\"üöÄ Starting table-level description generation...\")\n",
    "    \n",
    "    # Get unique tables\n",
    "    tables_df = metadata_df[['TABLE_SCHEMA', 'TABLE_NAME']].drop_duplicates()\n",
    "    \n",
    "    # Filter out excluded schemas\n",
    "    tables_to_process = tables_df[~tables_df['TABLE_SCHEMA'].isin(schemas_to_exclude)]\n",
    "    tables_excluded = tables_df[tables_df['TABLE_SCHEMA'].isin(schemas_to_exclude)]\n",
    "    \n",
    "    print(f\"üìä Total unique tables: {len(tables_df)}\")\n",
    "    print(f\"‚öôÔ∏è  Tables to process: {len(tables_to_process)}\")\n",
    "    print(f\"‚è≠Ô∏è  Tables skipped: {len(tables_excluded)}\")\n",
    "    \n",
    "    # Create a dictionary to store table descriptions\n",
    "    table_descriptions = {}\n",
    "    \n",
    "    # Process each table\n",
    "    for idx, (_, row) in enumerate(tables_to_process.iterrows(), 1):\n",
    "        schema = row['TABLE_SCHEMA']\n",
    "        table = row['TABLE_NAME']\n",
    "        \n",
    "        print(f\"  üîç [{idx}/{len(tables_to_process)}] Processing table {schema}.{table}\")\n",
    "        \n",
    "        # Get sample rows\n",
    "        sample_df = get_sample_rows_from_table(db_connector, schema, table, sample_size=5)\n",
    "        \n",
    "        if sample_df is not None:\n",
    "            # Get column list for this table\n",
    "            table_columns = metadata_df[\n",
    "                (metadata_df['TABLE_SCHEMA'] == schema) & \n",
    "                (metadata_df['TABLE_NAME'] == table)\n",
    "            ]['COLUMN_NAME'].tolist()\n",
    "            \n",
    "            print(f\"    üìà Retrieved {len(sample_df)} sample rows with {len(table_columns)} columns\")\n",
    "            \n",
    "            # Generate table description\n",
    "            description = call_deepseek_api_for_table(sample_df, schema, table, table_columns)\n",
    "            table_descriptions[f\"{schema}.{table}\"] = description\n",
    "            print(f\"    ‚úÖ Generated: {description[:80]}...\")\n",
    "        else:\n",
    "            table_descriptions[f\"{schema}.{table}\"] = f\"Unable to sample data from {schema}.{table}\"\n",
    "            print(f\"    ‚ö†Ô∏è  No sample data available\")\n",
    "    \n",
    "    # Add excluded table descriptions\n",
    "    for _, row in tables_excluded.iterrows():\n",
    "        schema = row['TABLE_SCHEMA']\n",
    "        table = row['TABLE_NAME']\n",
    "        table_descriptions[f\"{schema}.{table}\"] = \"Excluded schema - not processed\"\n",
    "    \n",
    "    # Add table descriptions to metadata_df\n",
    "    metadata_df['INFERRED_TABLE_DESCRIPTION'] = metadata_df.apply(\n",
    "        lambda row: table_descriptions.get(f\"{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}\", \"\"),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Completed table description generation!\")\n",
    "    print(f\"‚úÖ Generated descriptions for {len(tables_to_process)} tables\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped {len(tables_excluded)} tables\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Table description functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c19169",
   "metadata": {},
   "source": [
    "## SEQUENTIAL: Execute Complete Workflow: Column + Table Descriptions\n",
    "\n",
    "- This cell will generate both column-level and table-level descriptions.\n",
    "- **Kill** it because it will take too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36959c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ COMPLETE METADATA ENHANCEMENT WORKFLOW\n",
      "================================================================================\n",
      "\n",
      "üìù STEP 1: Generating Column Descriptions...\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting inference of column descriptions using DeepSeek API...\n",
      "üìä Total columns in metadata: 1078\n",
      "üö´ Excluded schemas: dbo\n",
      "‚öôÔ∏è  Columns to process (after exclusion): 1078\n",
      "‚è≠Ô∏è  Columns skipped: 0\n",
      "  üîç [1/1078] Processing humanresources.department.departmentid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique integer identifiers representing different departmen...\n",
      "  üîç [2/1078] Processing humanresources.department.name\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique integer identifiers representing different departmen...\n",
      "  üîç [2/1078] Processing humanresources.department.name\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names within an organization, listing various fu...\n",
      "  üîç [3/1078] Processing humanresources.department.groupname\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names within an organization, listing various fu...\n",
      "  üîç [3/1078] Processing humanresources.department.groupname\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names categorizing employees by business functio...\n",
      "  üîç [4/1078] Processing humanresources.department.modifieddate\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names categorizing employees by business functio...\n",
      "  üîç [4/1078] Processing humanresources.department.modifieddate\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This timestamp column contains identical date values for all records, likely ind...\n",
      "  üîç [5/1078] Processing humanresources.employee.businessentityid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This timestamp column contains identical date values for all records, likely ind...\n",
      "  üîç [5/1078] Processing humanresources.employee.businessentityid\n",
      "    üìà Retrieved 10 sample values\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìù STEP 1: Generating Column Descriptions...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m metadata_df_enhanced = \u001b[43madd_inferred_descriptions_to_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_connector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschemas_to_exclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Step 2: Generate table descriptions\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36madd_inferred_descriptions_to_metadata\u001b[39m\u001b[34m(metadata_df, db_connector, schemas_to_exclude)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    üìà Retrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sample values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Generate description using DeepSeek API\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m description = \u001b[43mcall_deepseek_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m df_to_process.at[idx, \u001b[33m'\u001b[39m\u001b[33mINFERRED_COLUMN_DESCRIPTION\u001b[39m\u001b[33m'\u001b[39m] = description\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    ‚úÖ Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription[:\u001b[32m80\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mcall_deepseek_api\u001b[39m\u001b[34m(sample_values, column_name, table_name, data_type)\u001b[39m\n\u001b[32m     25\u001b[39m headers = {\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEEPSEEK_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m }\n\u001b[32m     30\u001b[39m data = {\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeepseek-chat\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m\n\u001b[32m     40\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEEPSEEK_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     45\u001b[39m     result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execute complete workflow: Column + Table descriptions\n",
    "if 'metadata_df' in locals() and metadata_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ COMPLETE METADATA ENHANCEMENT WORKFLOW\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Generate column descriptions\n",
    "    print(\"\\nüìù STEP 1: Generating Column Descriptions...\")\n",
    "    print(\"-\" * 80)\n",
    "    metadata_df_enhanced = add_inferred_descriptions_to_metadata(\n",
    "        metadata_df, \n",
    "        db_connector,\n",
    "        schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate table descriptions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä STEP 2: Generating Table Descriptions...\")\n",
    "    print(\"-\" * 80)\n",
    "    metadata_df_enhanced = add_table_descriptions_to_metadata(\n",
    "        metadata_df_enhanced,\n",
    "        db_connector,\n",
    "        schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\n",
    "    )\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìã SAMPLE RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    display(metadata_df_enhanced[[\n",
    "        'TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE',\n",
    "        'INFERRED_COLUMN_DESCRIPTION', 'INFERRED_TABLE_DESCRIPTION'\n",
    "    ]].head(10))\n",
    "    \n",
    "    # Show table-level summary\n",
    "    print(\"\\nüìä TABLE-LEVEL SUMMARY:\")\n",
    "    table_summary = metadata_df_enhanced[['TABLE_SCHEMA', 'TABLE_NAME', 'INFERRED_TABLE_DESCRIPTION']].drop_duplicates()\n",
    "    display(table_summary.head(5))\n",
    "    \n",
    "    # Save to Excel\n",
    "    print(\"\\nüíæ Saving enhanced metadata to Excel...\")\n",
    "    if save_enhanced_metadata_to_excel(metadata_df_enhanced, db_connector):\n",
    "        print(\"\\nüéâ SUCCESS! Complete metadata enhancement finished!\")\n",
    "        print(f\"üìÅ Check 'Metadata_Enhanced' sheet in metadata_{db_connector.database}.xlsx\")\n",
    "        print(f\"‚úÖ Columns with descriptions: {len(metadata_df_enhanced)}\")\n",
    "        print(f\"‚úÖ Tables with descriptions: {len(table_summary)}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to save enhanced metadata.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  metadata_df not available. Run the metadata extraction cells first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° WHAT WAS GENERATED:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚Ä¢ INFERRED_COLUMN_DESCRIPTION: AI-generated description for each column\")\n",
    "print(\"‚Ä¢ INFERRED_TABLE_DESCRIPTION: AI-generated description for each table\")\n",
    "print(\"‚Ä¢ Both descriptions are based on sample data from the database\")\n",
    "print(\"‚Ä¢ Excluded schemas are marked as 'Excluded schema - not processed'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435be46f",
   "metadata": {},
   "source": [
    "# Performance Optimization Strategies\n",
    "\n",
    "This section demonstrates multiple strategies to speed up the DeepSeek API calls:\n",
    "\n",
    "1. **Batch Processing**: Process multiple columns in a single API call\n",
    "2. **Parallel Processing**: Use concurrent API calls with threading\n",
    "3. **Smart Caching**: Cache similar column patterns\n",
    "4. **Hybrid Approach**: Combine batch + parallel processing\n",
    "\n",
    "These strategies can reduce processing time from hours to minutes for large databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49bb4367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimization strategies loaded!\n",
      "\n",
      "üìñ Available strategies:\n",
      "  1. 'batch' - Process 5 columns per API call (2-3x faster)\n",
      "  2. 'parallel' - Process 5 columns simultaneously (2-3x faster)\n",
      "  3. 'hybrid' - Batch + Parallel (5-10x faster) ‚≠ê RECOMMENDED\n",
      "  4. 'original' - Sequential processing (baseline)\n",
      "\n",
      "üí° Usage example:\n",
      "  metadata_enhanced = add_inferred_descriptions_optimized(\n",
      "      metadata_df, db_connector,\n",
      "      schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA,\n",
      "      strategy='hybrid',  # Choose: 'batch', 'parallel', 'hybrid', 'original'\n",
      "      batch_size=5,       # Columns per batch\n",
      "      max_workers=3       # Parallel workers\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Batch Processing - Process multiple columns in one API call\n",
    "import concurrent.futures\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def call_deepseek_api_batch(column_batch: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Call DeepSeek API with multiple columns in a single request\n",
    "    \n",
    "    Parameters:\n",
    "    - column_batch: List of dicts with keys: schema, table, column, data_type, sample_values\n",
    "    \n",
    "    Returns:\n",
    "    - List of descriptions in the same order as input\n",
    "    \"\"\"\n",
    "    if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == \"your_deepseek_api_key_here\":\n",
    "        return [f\"AI description needed for {col['column']}\" for col in column_batch]\n",
    "    \n",
    "    try:\n",
    "        # Build a comprehensive prompt for all columns\n",
    "        batch_prompt = \"I have multiple database columns to describe. For each column, provide a concise description (max 40 words) of what it contains based on the sample values.\\n\\n\"\n",
    "        \n",
    "        for idx, col in enumerate(column_batch, 1):\n",
    "            sample_str = \", \".join([str(val) for val in col['sample_values'][:10] if val is not None])\n",
    "            if sample_str:\n",
    "                batch_prompt += f\"{idx}. Column: {col['schema']}.{col['table']}.{col['column']} (Type: {col['data_type']})\\n\"\n",
    "                batch_prompt += f\"   Samples: {sample_str}\\n\\n\"\n",
    "        \n",
    "        batch_prompt += \"\\nProvide descriptions in numbered format (1., 2., etc.) matching the column order above.\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"deepseek-chat\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": batch_prompt}],\n",
    "            \"max_tokens\": 150 * len(column_batch),  # Scale tokens with batch size\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            full_response = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            \n",
    "            # Parse the numbered responses\n",
    "            descriptions = []\n",
    "            lines = full_response.split('\\n')\n",
    "            current_desc = \"\"\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                # Check if line starts with a number followed by period or parenthesis\n",
    "                if line and (line[0].isdigit() or line.startswith('(')):\n",
    "                    if current_desc:\n",
    "                        descriptions.append(current_desc.strip())\n",
    "                    # Remove number prefix (e.g., \"1. \" or \"(1) \")\n",
    "                    current_desc = line.split('.', 1)[-1].strip() if '.' in line else line.split(')', 1)[-1].strip()\n",
    "                elif current_desc:\n",
    "                    current_desc += \" \" + line\n",
    "            \n",
    "            if current_desc:\n",
    "                descriptions.append(current_desc.strip())\n",
    "            \n",
    "            # Ensure we have descriptions for all columns\n",
    "            while len(descriptions) < len(column_batch):\n",
    "                descriptions.append(\"Description unavailable\")\n",
    "            \n",
    "            return descriptions[:len(column_batch)]\n",
    "        else:\n",
    "            print(f\"  ‚ùå Batch API Error {response.status_code}: {response.text}\")\n",
    "            return [f\"API error for {col['column']}\" for col in column_batch]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Batch API Exception: {str(e)}\")\n",
    "        return [f\"Error for {col['column']}\" for col in column_batch]\n",
    "\n",
    "\n",
    "# Strategy 2: Parallel Processing with Threading\n",
    "def call_deepseek_api_single_threadsafe(column_info: Dict) -> Tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Thread-safe version that returns index with description\n",
    "    \"\"\"\n",
    "    idx = column_info['index']\n",
    "    schema = column_info['schema']\n",
    "    table = column_info['table']\n",
    "    column = column_info['column']\n",
    "    data_type = column_info['data_type']\n",
    "    sample_values = column_info['sample_values']\n",
    "    \n",
    "    if sample_values:\n",
    "        description = call_deepseek_api(sample_values, column, f\"{schema}.{table}\", data_type)\n",
    "    else:\n",
    "        description = f\"{data_type} column in {schema}.{table}\"\n",
    "    \n",
    "    return (idx, description)\n",
    "\n",
    "\n",
    "def process_columns_parallel(column_infos: List[Dict], max_workers: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process columns in parallel using ThreadPoolExecutor\n",
    "    \n",
    "    Parameters:\n",
    "    - column_infos: List of column info dicts\n",
    "    - max_workers: Number of parallel threads (default: 5, adjust based on API rate limits)\n",
    "    \n",
    "    Returns:\n",
    "    - List of descriptions in order\n",
    "    \"\"\"\n",
    "    descriptions = [\"\"] * len(column_infos)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_col = {\n",
    "            executor.submit(call_deepseek_api_single_threadsafe, col_info): col_info \n",
    "            for col_info in column_infos\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(future_to_col):\n",
    "            try:\n",
    "                idx, description = future.result()\n",
    "                descriptions[idx] = description\n",
    "                completed += 1\n",
    "                if completed % 10 == 0:\n",
    "                    print(f\"  ‚úÖ Completed {completed}/{len(column_infos)} columns\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error in parallel processing: {str(e)}\")\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "\n",
    "# Strategy 3: Hybrid Batch + Parallel Processing\n",
    "def add_inferred_descriptions_optimized(\n",
    "    metadata_df, \n",
    "    db_connector, \n",
    "    schemas_to_exclude=None,\n",
    "    strategy='hybrid',\n",
    "    batch_size=5,\n",
    "    max_workers=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized version with multiple strategies\n",
    "    \n",
    "    Parameters:\n",
    "    - strategy: 'batch' (batch processing), 'parallel' (parallel single calls), \n",
    "                'hybrid' (batch + parallel), 'original' (sequential)\n",
    "    - batch_size: Number of columns per batch (for batch/hybrid strategies)\n",
    "    - max_workers: Number of parallel workers (for parallel/hybrid strategies)\n",
    "    \n",
    "    Performance comparison (example for 100 columns):\n",
    "    - original: ~10 minutes (1 call per column, sequential)\n",
    "    - batch: ~2 minutes (5 columns per call, sequential)\n",
    "    - parallel: ~2 minutes (1 call per column, 5 parallel)\n",
    "    - hybrid: ~30 seconds (5 columns per call, 3 parallel batches)\n",
    "    \"\"\"\n",
    "    if schemas_to_exclude is None:\n",
    "        schemas_to_exclude = []\n",
    "    \n",
    "    # Filter out excluded schemas\n",
    "    df_to_process = metadata_df[~metadata_df['TABLE_SCHEMA'].isin(schemas_to_exclude)].copy()\n",
    "    df_excluded = metadata_df[metadata_df['TABLE_SCHEMA'].isin(schemas_to_exclude)].copy()\n",
    "    \n",
    "    print(f\"üöÄ Starting OPTIMIZED inference using '{strategy.upper()}' strategy...\")\n",
    "    print(f\"üìä Total columns: {len(metadata_df)} | Processing: {len(df_to_process)} | Excluded: {len(df_excluded)}\")\n",
    "    \n",
    "    if strategy == 'batch':\n",
    "        print(f\"üì¶ Batch size: {batch_size} columns per API call\")\n",
    "    elif strategy == 'parallel':\n",
    "        print(f\"‚ö° Parallel workers: {max_workers} concurrent API calls\")\n",
    "    elif strategy == 'hybrid':\n",
    "        print(f\"üî• Hybrid mode: {batch_size} columns per batch, {max_workers} parallel batches\")\n",
    "    \n",
    "    df_to_process['INFERRED_COLUMN_DESCRIPTION'] = \"\"\n",
    "    df_excluded['INFERRED_COLUMN_DESCRIPTION'] = \"Excluded schema - not processed\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prepare column data\n",
    "    column_data = []\n",
    "    for idx, row in df_to_process.iterrows():\n",
    "        sample_values = get_sample_values_from_db(\n",
    "            db_connector, \n",
    "            row['TABLE_SCHEMA'], \n",
    "            row['TABLE_NAME'], \n",
    "            row['COLUMN_NAME'], \n",
    "            sample_size=SAMPLING_COUNT\n",
    "        )\n",
    "        \n",
    "        column_data.append({\n",
    "            'index': idx,\n",
    "            'schema': row['TABLE_SCHEMA'],\n",
    "            'table': row['TABLE_NAME'],\n",
    "            'column': row['COLUMN_NAME'],\n",
    "            'data_type': row['DATA_TYPE'],\n",
    "            'sample_values': sample_values\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Sampled data from database for {len(column_data)} columns\")\n",
    "    \n",
    "    # Execute based on strategy\n",
    "    if strategy == 'original':\n",
    "        # Sequential processing (original method)\n",
    "        for i, col_info in enumerate(column_data, 1):\n",
    "            print(f\"  üîç [{i}/{len(column_data)}] Processing {col_info['schema']}.{col_info['table']}.{col_info['column']}\")\n",
    "            if col_info['sample_values']:\n",
    "                desc = call_deepseek_api(\n",
    "                    col_info['sample_values'], \n",
    "                    col_info['column'], \n",
    "                    f\"{col_info['schema']}.{col_info['table']}\", \n",
    "                    col_info['data_type']\n",
    "                )\n",
    "            else:\n",
    "                desc = f\"{col_info['data_type']} column in {col_info['schema']}.{col_info['table']}\"\n",
    "            df_to_process.at[col_info['index'], 'INFERRED_COLUMN_DESCRIPTION'] = desc\n",
    "    \n",
    "    elif strategy == 'batch':\n",
    "        # Batch processing - sequential batches\n",
    "        batches = [column_data[i:i+batch_size] for i in range(0, len(column_data), batch_size)]\n",
    "        print(f\"üì¶ Processing {len(batches)} batches...\")\n",
    "        \n",
    "        for batch_num, batch in enumerate(batches, 1):\n",
    "            print(f\"  üì¶ Batch {batch_num}/{len(batches)} ({len(batch)} columns)\")\n",
    "            descriptions = call_deepseek_api_batch(batch)\n",
    "            \n",
    "            for col_info, desc in zip(batch, descriptions):\n",
    "                df_to_process.at[col_info['index'], 'INFERRED_COLUMN_DESCRIPTION'] = desc\n",
    "    \n",
    "    elif strategy == 'parallel':\n",
    "        # Parallel processing - concurrent single calls\n",
    "        print(f\"‚ö° Processing with {max_workers} parallel workers...\")\n",
    "        descriptions = process_columns_parallel(column_data, max_workers=max_workers)\n",
    "        \n",
    "        for col_info, desc in zip(column_data, descriptions):\n",
    "            df_to_process.at[col_info['index'], 'INFERRED_COLUMN_DESCRIPTION'] = desc\n",
    "    \n",
    "    elif strategy == 'hybrid':\n",
    "        # Hybrid: Batch + Parallel\n",
    "        batches = [column_data[i:i+batch_size] for i in range(0, len(column_data), batch_size)]\n",
    "        print(f\"üî• Processing {len(batches)} batches with {max_workers} parallel workers...\")\n",
    "        \n",
    "        def process_batch_wrapper(batch_info):\n",
    "            batch_num, batch = batch_info\n",
    "            print(f\"  üì¶ Processing batch {batch_num} ({len(batch)} columns)\")\n",
    "            descriptions = call_deepseek_api_batch(batch)\n",
    "            return [(col_info['index'], desc) for col_info, desc in zip(batch, descriptions)]\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            batch_infos = [(i+1, batch) for i, batch in enumerate(batches)]\n",
    "            results = executor.map(process_batch_wrapper, batch_infos)\n",
    "            \n",
    "            for batch_results in results:\n",
    "                for idx, desc in batch_results:\n",
    "                    df_to_process.at[idx, 'INFERRED_COLUMN_DESCRIPTION'] = desc\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Combine results\n",
    "    result_df = pd.concat([df_to_process, df_excluded], ignore_index=False).sort_index()\n",
    "    \n",
    "    print(f\"\\nüéâ Completed in {elapsed_time:.2f} seconds!\")\n",
    "    print(f\"‚úÖ Processed: {len(df_to_process)} columns\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped: {len(df_excluded)} columns\")\n",
    "    print(f\"‚ö° Speed: {len(df_to_process)/elapsed_time:.2f} columns/second\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Optimization strategies loaded!\")\n",
    "print(\"\\nüìñ Available strategies:\")\n",
    "print(\"  1. 'batch' - Process 5 columns per API call (2-3x faster)\")\n",
    "print(\"  2. 'parallel' - Process 5 columns simultaneously (2-3x faster)\")\n",
    "print(\"  3. 'hybrid' - Batch + Parallel (5-10x faster) ‚≠ê RECOMMENDED\")\n",
    "print(\"  4. 'original' - Sequential processing (baseline)\")\n",
    "print(\"\\nüí° Usage example:\")\n",
    "print(\"  metadata_enhanced = add_inferred_descriptions_optimized(\")\n",
    "print(\"      metadata_df, db_connector,\")\n",
    "print(\"      schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA,\")\n",
    "print(\"      strategy='hybrid',  # Choose: 'batch', 'parallel', 'hybrid', 'original'\")\n",
    "print(\"      batch_size=5,       # Columns per batch\")\n",
    "print(\"      max_workers=3       # Parallel workers\")\n",
    "print(\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04893b26",
   "metadata": {},
   "source": [
    "## Performance Comparison & Benchmarking\n",
    "\n",
    "- Run this cell to compare different strategies on your actual data.\n",
    "- This might take time, but will show you eventually that a hybrid approach (batch + parallel with appropriate values) might be the best strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833974b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Benchmarking on 20 columns...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing: ORIGINAL\n",
      "============================================================\n",
      "üöÄ Starting OPTIMIZED inference using 'ORIGINAL' strategy...\n",
      "üìä Total columns: 20 | Processing: 20 | Excluded: 0\n",
      "‚úÖ Sampled data from database for 20 columns\n",
      "  üîç [1/20] Processing humanresources.department.departmentid\n",
      "  üîç [2/20] Processing humanresources.department.name\n",
      "  üîç [2/20] Processing humanresources.department.name\n",
      "  üîç [3/20] Processing humanresources.department.groupname\n",
      "  üîç [3/20] Processing humanresources.department.groupname\n",
      "  üîç [4/20] Processing humanresources.department.modifieddate\n",
      "  üîç [4/20] Processing humanresources.department.modifieddate\n",
      "  üîç [5/20] Processing humanresources.employee.businessentityid\n",
      "  üîç [5/20] Processing humanresources.employee.businessentityid\n",
      "  üîç [6/20] Processing humanresources.employee.nationalidnumber\n",
      "  üîç [6/20] Processing humanresources.employee.nationalidnumber\n",
      "  üîç [7/20] Processing humanresources.employee.loginid\n",
      "  üîç [7/20] Processing humanresources.employee.loginid\n",
      "  üîç [8/20] Processing humanresources.employee.jobtitle\n",
      "  üîç [8/20] Processing humanresources.employee.jobtitle\n",
      "  üîç [9/20] Processing humanresources.employee.birthdate\n",
      "  üîç [9/20] Processing humanresources.employee.birthdate\n",
      "  üîç [10/20] Processing humanresources.employee.maritalstatus\n",
      "  üîç [10/20] Processing humanresources.employee.maritalstatus\n",
      "  üîç [11/20] Processing humanresources.employee.gender\n",
      "  üîç [11/20] Processing humanresources.employee.gender\n",
      "  üîç [12/20] Processing humanresources.employee.hiredate\n",
      "  üîç [12/20] Processing humanresources.employee.hiredate\n",
      "  üîç [13/20] Processing humanresources.employee.salariedflag\n",
      "  üîç [13/20] Processing humanresources.employee.salariedflag\n",
      "  üîç [14/20] Processing humanresources.employee.vacationhours\n",
      "  üîç [14/20] Processing humanresources.employee.vacationhours\n",
      "  üîç [15/20] Processing humanresources.employee.sickleavehours\n",
      "  üîç [15/20] Processing humanresources.employee.sickleavehours\n",
      "  üîç [16/20] Processing humanresources.employee.currentflag\n",
      "  üîç [16/20] Processing humanresources.employee.currentflag\n",
      "  üîç [17/20] Processing humanresources.employee.rowguid\n",
      "  üîç [17/20] Processing humanresources.employee.rowguid\n",
      "  üîç [18/20] Processing humanresources.employee.modifieddate\n",
      "  üîç [18/20] Processing humanresources.employee.modifieddate\n",
      "  üîç [19/20] Processing humanresources.employee.organizationnode\n",
      "  üîç [19/20] Processing humanresources.employee.organizationnode\n",
      "  üîç [20/20] Processing humanresources.employeedepartmenthistory.businessentityid\n",
      "  üîç [20/20] Processing humanresources.employeedepartmenthistory.businessentityid\n",
      "\n",
      "üéâ Completed in 52.33 seconds!\n",
      "‚úÖ Processed: 20 columns\n",
      "‚è≠Ô∏è  Skipped: 0 columns\n",
      "‚ö° Speed: 0.38 columns/second\n",
      "\n",
      "============================================================\n",
      "Testing: BATCH\n",
      "============================================================\n",
      "üöÄ Starting OPTIMIZED inference using 'BATCH' strategy...\n",
      "üìä Total columns: 20 | Processing: 20 | Excluded: 0\n",
      "üì¶ Batch size: 5 columns per API call\n",
      "‚úÖ Sampled data from database for 20 columns\n",
      "üì¶ Processing 4 batches...\n",
      "  üì¶ Batch 1/4 (5 columns)\n",
      "\n",
      "üéâ Completed in 52.33 seconds!\n",
      "‚úÖ Processed: 20 columns\n",
      "‚è≠Ô∏è  Skipped: 0 columns\n",
      "‚ö° Speed: 0.38 columns/second\n",
      "\n",
      "============================================================\n",
      "Testing: BATCH\n",
      "============================================================\n",
      "üöÄ Starting OPTIMIZED inference using 'BATCH' strategy...\n",
      "üìä Total columns: 20 | Processing: 20 | Excluded: 0\n",
      "üì¶ Batch size: 5 columns per API call\n",
      "‚úÖ Sampled data from database for 20 columns\n",
      "üì¶ Processing 4 batches...\n",
      "  üì¶ Batch 1/4 (5 columns)\n",
      "  üì¶ Batch 2/4 (5 columns)\n",
      "  üì¶ Batch 2/4 (5 columns)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Example usage - uncomment to run benchmark\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmetadata_df\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m metadata_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     benchmark_results = \u001b[43mbenchmark_strategies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_connector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è  Run metadata extraction first!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mbenchmark_strategies\u001b[39m\u001b[34m(metadata_df, db_connector, sample_size)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     30\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m result_df = \u001b[43madd_inferred_descriptions_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_connector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschemas_to_exclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     39\u001b[39m results[name] = {\n\u001b[32m     40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m: elapsed,\n\u001b[32m     41\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(result_df),\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mspeed\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(result_df) / elapsed\n\u001b[32m     43\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36madd_inferred_descriptions_optimized\u001b[39m\u001b[34m(metadata_df, db_connector, schemas_to_exclude, strategy, batch_size, max_workers)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_num, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches, \u001b[32m1\u001b[39m):\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  üì¶ Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     descriptions = \u001b[43mcall_deepseek_api_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col_info, desc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, descriptions):\n\u001b[32m    231\u001b[39m         df_to_process.at[col_info[\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mINFERRED_COLUMN_DESCRIPTION\u001b[39m\u001b[33m'\u001b[39m] = desc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mcall_deepseek_api_batch\u001b[39m\u001b[34m(column_batch)\u001b[39m\n\u001b[32m     31\u001b[39m headers = {\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEEPSEEK_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m }\n\u001b[32m     36\u001b[39m data = {\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeepseek-chat\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: batch_prompt}],\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m150\u001b[39m * \u001b[38;5;28mlen\u001b[39m(column_batch),  \u001b[38;5;66;03m# Scale tokens with batch size\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m\n\u001b[32m     41\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEEPSEEK_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     46\u001b[39m     result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Benchmark and Compare Strategies\n",
    "def benchmark_strategies(metadata_df, db_connector, sample_size=20):\n",
    "    \"\"\"\n",
    "    Compare performance of different strategies on a sample of your data\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_size: Number of columns to test (default: 20)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Take a sample for testing\n",
    "    test_df = metadata_df[~metadata_df['TABLE_SCHEMA'].isin(SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA)].head(sample_size)\n",
    "    \n",
    "    print(f\"üß™ Benchmarking on {len(test_df)} columns...\\n\")\n",
    "    \n",
    "    strategies = [\n",
    "        ('original', {'strategy': 'original'}),\n",
    "        ('batch', {'strategy': 'batch', 'batch_size': 5}),\n",
    "        ('parallel', {'strategy': 'parallel', 'max_workers': 3}),\n",
    "        ('hybrid', {'strategy': 'hybrid', 'batch_size': 5, 'max_workers': 3})\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, params in strategies:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {name.upper()}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result_df = add_inferred_descriptions_optimized(\n",
    "            test_df, \n",
    "            db_connector,\n",
    "            schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA,\n",
    "            **params\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results[name] = {\n",
    "            'time': elapsed,\n",
    "            'columns': len(result_df),\n",
    "            'speed': len(result_df) / elapsed\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"üìä BENCHMARK RESULTS\")\n",
    "    print('='*60)\n",
    "    \n",
    "    baseline_time = results['original']['time']\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        speedup = baseline_time / metrics['time']\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        print(f\"  ‚è±Ô∏è  Time: {metrics['time']:.2f}s\")\n",
    "        print(f\"  ‚ö° Speed: {metrics['speed']:.2f} columns/second\")\n",
    "        print(f\"  üöÄ Speedup: {speedup:.2f}x faster than original\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    names = list(results.keys())\n",
    "    times = [results[n]['time'] for n in names]\n",
    "    speeds = [results[n]['speed'] for n in names]\n",
    "    \n",
    "    # Time comparison\n",
    "    bars1 = ax1.bar(names, times, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\n",
    "    ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    ax1.set_title('Processing Time Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}s',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Speed comparison\n",
    "    bars2 = ax2.bar(names, speeds, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\n",
    "    ax2.set_ylabel('Columns per Second', fontsize=12)\n",
    "    ax2.set_title('Processing Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate estimated time for full dataset\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"üìà ESTIMATED TIME FOR FULL DATASET\")\n",
    "    print('='*60)\n",
    "    \n",
    "    full_count = len(metadata_df[~metadata_df['TABLE_SCHEMA'].isin(SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA)])\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        estimated_time = full_count / metrics['speed']\n",
    "        minutes = int(estimated_time // 60)\n",
    "        seconds = int(estimated_time % 60)\n",
    "        \n",
    "        print(f\"\\n{name.upper()}: ~{minutes}m {seconds}s for {full_count} columns\")\n",
    "    \n",
    "    print(\"\\nüí° RECOMMENDATION: Use 'hybrid' strategy for best performance!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage - uncomment to run benchmark\n",
    "if 'metadata_df' in locals() and metadata_df is not None:\n",
    "    benchmark_results = benchmark_strategies(metadata_df, db_connector, sample_size=20)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run metadata extraction first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d0250",
   "metadata": {},
   "source": [
    "## Execute Optimized Workflow: Column + Table Descriptions (RECOMMENDED)\n",
    "\n",
    "- Use the hybrid strategy for faster processing of both column and table descriptions.\n",
    "- Play with different values of **batch_size** and **max_workers** to arrive at optimized for your environment profile (aka. bechmark). \n",
    "- **I found batch_size=3 and max_workers=2 to work well**\n",
    "- Cost of execution was ~ 5-6 cents & input+output+cache tokens ~ 227k\n",
    "- With batch_size=5 and max_workers=3 (with a non-enterprise deep seek account) , we might sometimes run into `‚ùå Batch API Error 400: {\"error\":{\"message\":\"This model's maximum context length is 131072 tokens. However, you requested 404004 tokens (403254 in the messages, 750 in the completion). Please reduce the length of the messages or completion.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":\"invalid_request_error\"}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35952c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ OPTIMIZED METADATA ENHANCEMENT WORKFLOW (HYBRID STRATEGY)\n",
      "================================================================================\n",
      "\n",
      "üìù STEP 1: Generating Column Descriptions (Optimized)...\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting OPTIMIZED inference using 'HYBRID' strategy...\n",
      "üìä Total columns: 456 | Processing: 456 | Excluded: 0\n",
      "üî• Hybrid mode: 3 columns per batch, 2 parallel batches\n",
      "‚úÖ Sampled data from database for 456 columns\n",
      "üî• Processing 152 batches with 2 parallel workers...\n",
      "  üì¶ Processing batch 1 (3 columns)\n",
      "  üì¶ Processing batch 2 (3 columns)\n",
      "‚úÖ Sampled data from database for 456 columns\n",
      "üî• Processing 152 batches with 2 parallel workers...\n",
      "  üì¶ Processing batch 1 (3 columns)\n",
      "  üì¶ Processing batch 2 (3 columns)\n",
      "  üì¶ Processing batch 3 (3 columns)\n",
      "  üì¶ Processing batch 3 (3 columns)\n",
      "  üì¶ Processing batch 4 (3 columns)\n",
      "  üì¶ Processing batch 4 (3 columns)\n",
      "  üì¶ Processing batch 5 (3 columns)\n",
      "  üì¶ Processing batch 5 (3 columns)\n",
      "  üì¶ Processing batch 6 (3 columns)\n",
      "  üì¶ Processing batch 6 (3 columns)\n",
      "  üì¶ Processing batch 7 (3 columns)\n",
      "  üì¶ Processing batch 7 (3 columns)\n",
      "  üì¶ Processing batch 8 (3 columns)\n",
      "  üì¶ Processing batch 8 (3 columns)\n",
      "  üì¶ Processing batch 9 (3 columns)\n",
      "  üì¶ Processing batch 9 (3 columns)\n",
      "  üì¶ Processing batch 10 (3 columns)\n",
      "  üì¶ Processing batch 10 (3 columns)\n",
      "  üì¶ Processing batch 11 (3 columns)\n",
      "  üì¶ Processing batch 11 (3 columns)\n",
      "  üì¶ Processing batch 12 (3 columns)\n",
      "  üì¶ Processing batch 12 (3 columns)\n",
      "  üì¶ Processing batch 13 (3 columns)\n",
      "  üì¶ Processing batch 13 (3 columns)\n",
      "  üì¶ Processing batch 14 (3 columns)\n",
      "  üì¶ Processing batch 14 (3 columns)\n",
      "  üì¶ Processing batch 15 (3 columns)\n",
      "  üì¶ Processing batch 15 (3 columns)\n",
      "  üì¶ Processing batch 16 (3 columns)\n",
      "  üì¶ Processing batch 16 (3 columns)\n",
      "  üì¶ Processing batch 17 (3 columns)\n",
      "  üì¶ Processing batch 17 (3 columns)\n",
      "  üì¶ Processing batch 18 (3 columns)\n",
      "  üì¶ Processing batch 18 (3 columns)\n",
      "  üì¶ Processing batch 19 (3 columns)\n",
      "  üì¶ Processing batch 19 (3 columns)\n",
      "  üì¶ Processing batch 20 (3 columns)\n",
      "  üì¶ Processing batch 20 (3 columns)\n",
      "  üì¶ Processing batch 21 (3 columns)\n",
      "  üì¶ Processing batch 21 (3 columns)\n",
      "  üì¶ Processing batch 22 (3 columns)\n",
      "  üì¶ Processing batch 22 (3 columns)\n",
      "  üì¶ Processing batch 23 (3 columns)\n",
      "  üì¶ Processing batch 23 (3 columns)\n",
      "  üì¶ Processing batch 24 (3 columns)\n",
      "  üì¶ Processing batch 24 (3 columns)\n",
      "  üì¶ Processing batch 25 (3 columns)\n",
      "  üì¶ Processing batch 25 (3 columns)\n",
      "  üì¶ Processing batch 26 (3 columns)\n",
      "  üì¶ Processing batch 26 (3 columns)\n",
      "  üì¶ Processing batch 27 (3 columns)\n",
      "  üì¶ Processing batch 27 (3 columns)\n",
      "  üì¶ Processing batch 28 (3 columns)\n",
      "  üì¶ Processing batch 28 (3 columns)\n",
      "  üì¶ Processing batch 29 (3 columns)\n",
      "  üì¶ Processing batch 29 (3 columns)\n",
      "  üì¶ Processing batch 30 (3 columns)\n",
      "  üì¶ Processing batch 30 (3 columns)\n",
      "  üì¶ Processing batch 31 (3 columns)\n",
      "  üì¶ Processing batch 31 (3 columns)\n",
      "  üì¶ Processing batch 32 (3 columns)\n",
      "  üì¶ Processing batch 32 (3 columns)\n",
      "  üì¶ Processing batch 33 (3 columns)\n",
      "  üì¶ Processing batch 33 (3 columns)\n",
      "  üì¶ Processing batch 34 (3 columns)\n",
      "  üì¶ Processing batch 34 (3 columns)\n",
      "  üì¶ Processing batch 35 (3 columns)\n",
      "  üì¶ Processing batch 35 (3 columns)\n",
      "  üì¶ Processing batch 36 (3 columns)\n",
      "  üì¶ Processing batch 36 (3 columns)\n",
      "  üì¶ Processing batch 37 (3 columns)\n",
      "  üì¶ Processing batch 38 (3 columns)\n",
      "  üì¶ Processing batch 37 (3 columns)\n",
      "  üì¶ Processing batch 38 (3 columns)\n",
      "  üì¶ Processing batch 39 (3 columns)\n",
      "  üì¶ Processing batch 39 (3 columns)\n",
      "  üì¶ Processing batch 40 (3 columns)\n",
      "  üì¶ Processing batch 40 (3 columns)\n",
      "  üì¶ Processing batch 41 (3 columns)\n",
      "  üì¶ Processing batch 41 (3 columns)\n",
      "  üì¶ Processing batch 42 (3 columns)\n",
      "  üì¶ Processing batch 42 (3 columns)\n",
      "  üì¶ Processing batch 43 (3 columns)\n",
      "  üì¶ Processing batch 44 (3 columns)\n",
      "  üì¶ Processing batch 43 (3 columns)\n",
      "  üì¶ Processing batch 44 (3 columns)\n",
      "  üì¶ Processing batch 45 (3 columns)\n",
      "  üì¶ Processing batch 45 (3 columns)\n",
      "  üì¶ Processing batch 46 (3 columns)\n",
      "  üì¶ Processing batch 46 (3 columns)\n",
      "  üì¶ Processing batch 47 (3 columns)\n",
      "  üì¶ Processing batch 47 (3 columns)\n",
      "  üì¶ Processing batch 48 (3 columns)\n",
      "  üì¶ Processing batch 48 (3 columns)\n",
      "  üì¶ Processing batch 49 (3 columns)\n",
      "  üì¶ Processing batch 49 (3 columns)\n",
      "  üì¶ Processing batch 50 (3 columns)\n",
      "  üì¶ Processing batch 50 (3 columns)\n",
      "  üì¶ Processing batch 51 (3 columns)\n",
      "  üì¶ Processing batch 51 (3 columns)\n",
      "  üì¶ Processing batch 52 (3 columns)\n",
      "  üì¶ Processing batch 52 (3 columns)\n",
      "  üì¶ Processing batch 53 (3 columns)\n",
      "  üì¶ Processing batch 53 (3 columns)\n",
      "  üì¶ Processing batch 54 (3 columns)\n",
      "  üì¶ Processing batch 54 (3 columns)\n",
      "  üì¶ Processing batch 55 (3 columns)\n",
      "  üì¶ Processing batch 55 (3 columns)\n",
      "  üì¶ Processing batch 56 (3 columns)\n",
      "  üì¶ Processing batch 56 (3 columns)\n",
      "  üì¶ Processing batch 57 (3 columns)\n",
      "  üì¶ Processing batch 57 (3 columns)\n",
      "  üì¶ Processing batch 58 (3 columns)\n",
      "  üì¶ Processing batch 58 (3 columns)\n",
      "  üì¶ Processing batch 59 (3 columns)\n",
      "  üì¶ Processing batch 59 (3 columns)\n",
      "  üì¶ Processing batch 60 (3 columns)\n",
      "  üì¶ Processing batch 60 (3 columns)\n",
      "  üì¶ Processing batch 61 (3 columns)\n",
      "  üì¶ Processing batch 61 (3 columns)\n",
      "  üì¶ Processing batch 62 (3 columns)\n",
      "  üì¶ Processing batch 62 (3 columns)\n",
      "  üì¶ Processing batch 63 (3 columns)\n",
      "  üì¶ Processing batch 63 (3 columns)\n",
      "  üì¶ Processing batch 64 (3 columns)\n",
      "  üì¶ Processing batch 64 (3 columns)\n",
      "  üì¶ Processing batch 65 (3 columns)\n",
      "  üì¶ Processing batch 65 (3 columns)\n",
      "  üì¶ Processing batch 66 (3 columns)\n",
      "  üì¶ Processing batch 66 (3 columns)\n",
      "  üì¶ Processing batch 67 (3 columns)\n",
      "  üì¶ Processing batch 67 (3 columns)\n",
      "  üì¶ Processing batch 68 (3 columns)\n",
      "  üì¶ Processing batch 68 (3 columns)\n",
      "  üì¶ Processing batch 69 (3 columns)\n",
      "  üì¶ Processing batch 69 (3 columns)\n",
      "  üì¶ Processing batch 70 (3 columns)\n",
      "  üì¶ Processing batch 70 (3 columns)\n",
      "  üì¶ Processing batch 71 (3 columns)\n",
      "  üì¶ Processing batch 71 (3 columns)\n",
      "  üì¶ Processing batch 72 (3 columns)\n",
      "  üì¶ Processing batch 72 (3 columns)\n",
      "  üì¶ Processing batch 73 (3 columns)\n",
      "  üì¶ Processing batch 73 (3 columns)\n",
      "  üì¶ Processing batch 74 (3 columns)\n",
      "  üì¶ Processing batch 74 (3 columns)\n",
      "  üì¶ Processing batch 75 (3 columns)\n",
      "  üì¶ Processing batch 75 (3 columns)\n",
      "  üì¶ Processing batch 76 (3 columns)\n",
      "  üì¶ Processing batch 76 (3 columns)\n",
      "  üì¶ Processing batch 77 (3 columns)\n",
      "  üì¶ Processing batch 78 (3 columns)\n",
      "  üì¶ Processing batch 77 (3 columns)\n",
      "  üì¶ Processing batch 78 (3 columns)\n",
      "  üì¶ Processing batch 79 (3 columns)\n",
      "  üì¶ Processing batch 79 (3 columns)\n",
      "  üì¶ Processing batch 80 (3 columns)\n",
      "  üì¶ Processing batch 80 (3 columns)\n",
      "  üì¶ Processing batch 81 (3 columns)\n",
      "  üì¶ Processing batch 81 (3 columns)\n",
      "  üì¶ Processing batch 82 (3 columns)\n",
      "  üì¶ Processing batch 82 (3 columns)\n",
      "  üì¶ Processing batch 83 (3 columns)\n",
      "  üì¶ Processing batch 83 (3 columns)\n",
      "  üì¶ Processing batch 84 (3 columns)\n",
      "  üì¶ Processing batch 84 (3 columns)\n",
      "  üì¶ Processing batch 85 (3 columns)\n",
      "  üì¶ Processing batch 85 (3 columns)\n",
      "  üì¶ Processing batch 86 (3 columns)\n",
      "  üì¶ Processing batch 86 (3 columns)\n",
      "  üì¶ Processing batch 87 (3 columns)\n",
      "  üì¶ Processing batch 87 (3 columns)\n",
      "  üì¶ Processing batch 88 (3 columns)\n",
      "  üì¶ Processing batch 88 (3 columns)\n",
      "  üì¶ Processing batch 89 (3 columns)\n",
      "  üì¶ Processing batch 89 (3 columns)\n",
      "  üì¶ Processing batch 90 (3 columns)\n",
      "  üì¶ Processing batch 90 (3 columns)\n",
      "  üì¶ Processing batch 91 (3 columns)\n",
      "  üì¶ Processing batch 91 (3 columns)\n",
      "  üì¶ Processing batch 92 (3 columns)\n",
      "  üì¶ Processing batch 92 (3 columns)\n",
      "  üì¶ Processing batch 93 (3 columns)\n",
      "  üì¶ Processing batch 93 (3 columns)\n",
      "  üì¶ Processing batch 94 (3 columns)\n",
      "  üì¶ Processing batch 94 (3 columns)\n",
      "  üì¶ Processing batch 95 (3 columns)\n",
      "  üì¶ Processing batch 95 (3 columns)\n",
      "  üì¶ Processing batch 96 (3 columns)\n",
      "  üì¶ Processing batch 96 (3 columns)\n",
      "  üì¶ Processing batch 97 (3 columns)\n",
      "  üì¶ Processing batch 97 (3 columns)\n",
      "  üì¶ Processing batch 98 (3 columns)\n",
      "  üì¶ Processing batch 98 (3 columns)\n",
      "  üì¶ Processing batch 99 (3 columns)\n",
      "  üì¶ Processing batch 99 (3 columns)\n",
      "  üì¶ Processing batch 100 (3 columns)\n",
      "  üì¶ Processing batch 100 (3 columns)\n",
      "  üì¶ Processing batch 101 (3 columns)\n",
      "  üì¶ Processing batch 101 (3 columns)\n",
      "  üì¶ Processing batch 102 (3 columns)\n",
      "  üì¶ Processing batch 102 (3 columns)\n",
      "  üì¶ Processing batch 103 (3 columns)\n",
      "  üì¶ Processing batch 103 (3 columns)\n",
      "  üì¶ Processing batch 104 (3 columns)\n",
      "  üì¶ Processing batch 104 (3 columns)\n",
      "  üì¶ Processing batch 105 (3 columns)\n",
      "  üì¶ Processing batch 105 (3 columns)\n",
      "  üì¶ Processing batch 106 (3 columns)\n",
      "  üì¶ Processing batch 106 (3 columns)\n",
      "  üì¶ Processing batch 107 (3 columns)\n",
      "  üì¶ Processing batch 107 (3 columns)\n",
      "  üì¶ Processing batch 108 (3 columns)\n",
      "  üì¶ Processing batch 108 (3 columns)\n",
      "  üì¶ Processing batch 109 (3 columns)\n",
      "  üì¶ Processing batch 109 (3 columns)\n",
      "  üì¶ Processing batch 110 (3 columns)\n",
      "  üì¶ Processing batch 110 (3 columns)\n",
      "  üì¶ Processing batch 111 (3 columns)\n",
      "  üì¶ Processing batch 111 (3 columns)\n",
      "  üì¶ Processing batch 112 (3 columns)\n",
      "  üì¶ Processing batch 112 (3 columns)\n",
      "  üì¶ Processing batch 113 (3 columns)\n",
      "  üì¶ Processing batch 113 (3 columns)\n",
      "  üì¶ Processing batch 114 (3 columns)\n",
      "  üì¶ Processing batch 114 (3 columns)\n",
      "  üì¶ Processing batch 115 (3 columns)\n",
      "  üì¶ Processing batch 115 (3 columns)\n",
      "  üì¶ Processing batch 116 (3 columns)\n",
      "  üì¶ Processing batch 116 (3 columns)\n",
      "  üì¶ Processing batch 117 (3 columns)\n",
      "  üì¶ Processing batch 117 (3 columns)\n",
      "  üì¶ Processing batch 118 (3 columns)\n",
      "  üì¶ Processing batch 118 (3 columns)\n",
      "  üì¶ Processing batch 119 (3 columns)\n",
      "  üì¶ Processing batch 119 (3 columns)\n",
      "  üì¶ Processing batch 120 (3 columns)\n",
      "  üì¶ Processing batch 121 (3 columns)\n",
      "  üì¶ Processing batch 120 (3 columns)\n",
      "  üì¶ Processing batch 121 (3 columns)\n",
      "  üì¶ Processing batch 122 (3 columns)\n",
      "  üì¶ Processing batch 122 (3 columns)\n",
      "  üì¶ Processing batch 123 (3 columns)\n",
      "  üì¶ Processing batch 123 (3 columns)\n",
      "  üì¶ Processing batch 124 (3 columns)\n",
      "  üì¶ Processing batch 124 (3 columns)\n",
      "  üì¶ Processing batch 125 (3 columns)\n",
      "  üì¶ Processing batch 125 (3 columns)\n",
      "  üì¶ Processing batch 126 (3 columns)\n",
      "  üì¶ Processing batch 126 (3 columns)\n",
      "  üì¶ Processing batch 127 (3 columns)\n",
      "  üì¶ Processing batch 127 (3 columns)\n",
      "  üì¶ Processing batch 128 (3 columns)\n",
      "  üì¶ Processing batch 128 (3 columns)\n",
      "  üì¶ Processing batch 129 (3 columns)\n",
      "  üì¶ Processing batch 129 (3 columns)\n",
      "  üì¶ Processing batch 130 (3 columns)\n",
      "  üì¶ Processing batch 130 (3 columns)\n",
      "  üì¶ Processing batch 131 (3 columns)\n",
      "  üì¶ Processing batch 132 (3 columns)\n",
      "  üì¶ Processing batch 131 (3 columns)\n",
      "  üì¶ Processing batch 132 (3 columns)\n",
      "  üì¶ Processing batch 133 (3 columns)\n",
      "  üì¶ Processing batch 133 (3 columns)\n",
      "  üì¶ Processing batch 134 (3 columns)\n",
      "  üì¶ Processing batch 134 (3 columns)\n",
      "  üì¶ Processing batch 135 (3 columns)\n",
      "  üì¶ Processing batch 135 (3 columns)\n",
      "  üì¶ Processing batch 136 (3 columns)\n",
      "  üì¶ Processing batch 136 (3 columns)\n",
      "  üì¶ Processing batch 137 (3 columns)\n",
      "  üì¶ Processing batch 137 (3 columns)\n",
      "  üì¶ Processing batch 138 (3 columns)\n",
      "  üì¶ Processing batch 138 (3 columns)\n",
      "  üì¶ Processing batch 139 (3 columns)\n",
      "  üì¶ Processing batch 139 (3 columns)\n",
      "  üì¶ Processing batch 140 (3 columns)\n",
      "  üì¶ Processing batch 140 (3 columns)\n",
      "  üì¶ Processing batch 141 (3 columns)\n",
      "  üì¶ Processing batch 141 (3 columns)\n",
      "  üì¶ Processing batch 142 (3 columns)\n",
      "  üì¶ Processing batch 142 (3 columns)\n",
      "  üì¶ Processing batch 143 (3 columns)\n",
      "  üì¶ Processing batch 143 (3 columns)\n",
      "  üì¶ Processing batch 144 (3 columns)\n",
      "  üì¶ Processing batch 144 (3 columns)\n",
      "  üì¶ Processing batch 145 (3 columns)\n",
      "  üì¶ Processing batch 145 (3 columns)\n",
      "  üì¶ Processing batch 146 (3 columns)\n",
      "  üì¶ Processing batch 146 (3 columns)\n",
      "  üì¶ Processing batch 147 (3 columns)\n",
      "  üì¶ Processing batch 147 (3 columns)\n",
      "  üì¶ Processing batch 148 (3 columns)\n",
      "  üì¶ Processing batch 148 (3 columns)\n",
      "  üì¶ Processing batch 149 (3 columns)\n",
      "  üì¶ Processing batch 149 (3 columns)\n",
      "  üì¶ Processing batch 150 (3 columns)\n",
      "  üì¶ Processing batch 150 (3 columns)\n",
      "  üì¶ Processing batch 151 (3 columns)\n",
      "  üì¶ Processing batch 151 (3 columns)\n",
      "  üì¶ Processing batch 152 (3 columns)\n",
      "  üì¶ Processing batch 152 (3 columns)\n",
      "\n",
      "üéâ Completed in 337.41 seconds!\n",
      "‚úÖ Processed: 456 columns\n",
      "‚è≠Ô∏è  Skipped: 0 columns\n",
      "‚ö° Speed: 1.35 columns/second\n",
      "\n",
      "================================================================================\n",
      "üìä STEP 2: Generating Table Descriptions...\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting table-level description generation...\n",
      "üìä Total unique tables: 68\n",
      "‚öôÔ∏è  Tables to process: 68\n",
      "‚è≠Ô∏è  Tables skipped: 0\n",
      "  üîç [1/68] Processing table humanresources.department\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "\n",
      "üéâ Completed in 337.41 seconds!\n",
      "‚úÖ Processed: 456 columns\n",
      "‚è≠Ô∏è  Skipped: 0 columns\n",
      "‚ö° Speed: 1.35 columns/second\n",
      "\n",
      "================================================================================\n",
      "üìä STEP 2: Generating Table Descriptions...\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting table-level description generation...\n",
      "üìä Total unique tables: 68\n",
      "‚öôÔ∏è  Tables to process: 68\n",
      "‚è≠Ô∏è  Tables skipped: 0\n",
      "  üîç [1/68] Processing table humanresources.department\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores department master data for organizational management. It track...\n",
      "  üîç [2/68] Processing table humanresources.employee\n",
      "    üìà Retrieved 5 sample rows with 15 columns\n",
      "    ‚úÖ Generated: This table stores department master data for organizational management. It track...\n",
      "  üîç [2/68] Processing table humanresources.employee\n",
      "    üìà Retrieved 5 sample rows with 15 columns\n",
      "    ‚úÖ Generated: This table stores core employee master data for HR management. It contains perso...\n",
      "  üîç [3/68] Processing table humanresources.employeedepartmenthistory\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores core employee master data for HR management. It contains perso...\n",
      "  üîç [3/68] Processing table humanresources.employeedepartmenthistory\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table tracks employee department and shift assignments over time. It record...\n",
      "  üîç [4/68] Processing table humanresources.employeepayhistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table tracks employee department and shift assignments over time. It record...\n",
      "  üîç [4/68] Processing table humanresources.employeepayhistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "  ‚ùå Exception calling API for table humanresources.employeepayhistory: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out.\n",
      "    ‚úÖ Generated: Error generating description for table humanresources.employeepayhistory...\n",
      "  üîç [5/68] Processing table humanresources.jobcandidate\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "  ‚ùå Exception calling API for table humanresources.employeepayhistory: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out.\n",
      "    ‚úÖ Generated: Error generating description for table humanresources.employeepayhistory...\n",
      "  üîç [5/68] Processing table humanresources.jobcandidate\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "  ‚ùå Exception calling API for table humanresources.jobcandidate: HTTPSConnectionPool(host='api.deepseek.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7b24c7f81f50>: Failed to resolve 'api.deepseek.com' ([Errno -3] Temporary failure in name resolution)\"))\n",
      "    ‚úÖ Generated: Error generating description for table humanresources.jobcandidate...\n",
      "  üîç [6/68] Processing table humanresources.shift\n",
      "    üìà Retrieved 3 sample rows with 5 columns\n",
      "  ‚ùå Exception calling API for table humanresources.jobcandidate: HTTPSConnectionPool(host='api.deepseek.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7b24c7f81f50>: Failed to resolve 'api.deepseek.com' ([Errno -3] Temporary failure in name resolution)\"))\n",
      "    ‚úÖ Generated: Error generating description for table humanresources.jobcandidate...\n",
      "  üîç [6/68] Processing table humanresources.shift\n",
      "    üìà Retrieved 3 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores shift schedule definitions for workforce management. It contai...\n",
      "  üîç [7/68] Processing table person.address\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores shift schedule definitions for workforce management. It contai...\n",
      "  üîç [7/68] Processing table person.address\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores physical address information for business entities. It contain...\n",
      "  üîç [8/68] Processing table person.addresstype\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores physical address information for business entities. It contain...\n",
      "  üîç [8/68] Processing table person.addresstype\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores reference data for different types of addresses used within th...\n",
      "  üîç [9/68] Processing table person.businessentity\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores reference data for different types of addresses used within th...\n",
      "  üîç [9/68] Processing table person.businessentity\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores unique business entities, serving as a master record for any p...\n",
      "  üîç [10/68] Processing table person.businessentityaddress\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores unique business entities, serving as a master record for any p...\n",
      "  üîç [10/68] Processing table person.businessentityaddress\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores the relationships between business entities and their addresse...\n",
      "  üîç [11/68] Processing table person.businessentitycontact\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores the relationships between business entities and their addresse...\n",
      "  üîç [11/68] Processing table person.businessentitycontact\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores contact relationships between business entities and people, de...\n",
      "  üîç [12/68] Processing table person.contacttype\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores contact relationships between business entities and people, de...\n",
      "  üîç [12/68] Processing table person.contacttype\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores a lookup list of contact role types used within the business. ...\n",
      "  üîç [13/68] Processing table person.countryregion\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores a lookup list of contact role types used within the business. ...\n",
      "  üîç [13/68] Processing table person.countryregion\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores reference data for countries and regions, serving as a lookup ...\n",
      "  üîç [14/68] Processing table person.emailaddress\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores reference data for countries and regions, serving as a lookup ...\n",
      "  üîç [14/68] Processing table person.emailaddress\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores email addresses associated with people (businessentityid) in t...\n",
      "  üîç [15/68] Processing table person.password\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores email addresses associated with people (businessentityid) in t...\n",
      "  üîç [15/68] Processing table person.password\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores password security data for business entities. It contains hash...\n",
      "  üîç [16/68] Processing table person.person\n",
      "    üìà Retrieved 5 sample rows with 13 columns\n",
      "    ‚úÖ Generated: This table stores password security data for business entities. It contains hash...\n",
      "  üîç [16/68] Processing table person.person\n",
      "    üìà Retrieved 5 sample rows with 13 columns\n",
      "    ‚úÖ Generated: This table stores core personal information for individuals associated with a bu...\n",
      "  üîç [17/68] Processing table person.personphone\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores core personal information for individuals associated with a bu...\n",
      "  üîç [17/68] Processing table person.personphone\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores phone contact information for business entities. It links phon...\n",
      "  üîç [18/68] Processing table person.phonenumbertype\n",
      "    üìà Retrieved 3 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores phone contact information for business entities. It links phon...\n",
      "  üîç [18/68] Processing table person.phonenumbertype\n",
      "    üìà Retrieved 3 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores a lookup list of phone number categories used in the system. I...\n",
      "  üîç [19/68] Processing table person.stateprovince\n",
      "    üìà Retrieved 5 sample rows with 8 columns\n",
      "    ‚úÖ Generated: This table stores a lookup list of phone number categories used in the system. I...\n",
      "  üîç [19/68] Processing table person.stateprovince\n",
      "    üìà Retrieved 5 sample rows with 8 columns\n",
      "    ‚úÖ Generated: This table stores geographic subdivisions (states/provinces) for different count...\n",
      "  üîç [20/68] Processing table production.billofmaterials\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores geographic subdivisions (states/provinces) for different count...\n",
      "  üîç [20/68] Processing table production.billofmaterials\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores bill of materials (BOM) data, defining product structures by l...\n",
      "  üîç [21/68] Processing table production.culture\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores bill of materials (BOM) data, defining product structures by l...\n",
      "  üîç [21/68] Processing table production.culture\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores language and cultural identifiers used within the business sys...\n",
      "  üîç [22/68] Processing table production.document\n",
      "    ‚ö†Ô∏è  No sample data available\n",
      "  üîç [23/68] Processing table production.illustration\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores language and cultural identifiers used within the business sys...\n",
      "  üîç [22/68] Processing table production.document\n",
      "    ‚ö†Ô∏è  No sample data available\n",
      "  üîç [23/68] Processing table production.illustration\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores technical diagram illustrations, likely vector graphics export...\n",
      "  üîç [24/68] Processing table production.location\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores technical diagram illustrations, likely vector graphics export...\n",
      "  üîç [24/68] Processing table production.location\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores manufacturing facility locations and their operational metrics...\n",
      "  üîç [25/68] Processing table production.product\n",
      "    üìà Retrieved 5 sample rows with 25 columns\n",
      "    ‚úÖ Generated: This table stores manufacturing facility locations and their operational metrics...\n",
      "  üîç [25/68] Processing table production.product\n",
      "    üìà Retrieved 5 sample rows with 25 columns\n",
      "    ‚úÖ Generated: This table stores product master data for a manufacturing business, containing a...\n",
      "  üîç [26/68] Processing table production.productcategory\n",
      "    üìà Retrieved 4 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores product master data for a manufacturing business, containing a...\n",
      "  üîç [26/68] Processing table production.productcategory\n",
      "    üìà Retrieved 4 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores product category master data for business classification. It c...\n",
      "  üîç [27/68] Processing table production.productcosthistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores product category master data for business classification. It c...\n",
      "  üîç [27/68] Processing table production.productcosthistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table tracks historical standard cost changes for products over time. It st...\n",
      "  üîç [28/68] Processing table production.productdescription\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table tracks historical standard cost changes for products over time. It st...\n",
      "  üîç [28/68] Processing table production.productdescription\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores product description text for a product catalog system. It cont...\n",
      "  üîç [29/68] Processing table production.productdocument\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores product description text for a product catalog system. It cont...\n",
      "  üîç [29/68] Processing table production.productdocument\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores associations between products and documents in a manufacturing...\n",
      "  üîç [30/68] Processing table production.productinventory\n",
      "    üìà Retrieved 5 sample rows with 7 columns\n",
      "    ‚úÖ Generated: This table stores associations between products and documents in a manufacturing...\n",
      "  üîç [30/68] Processing table production.productinventory\n",
      "    üìà Retrieved 5 sample rows with 7 columns\n",
      "    ‚úÖ Generated: This table tracks physical inventory of products across different warehouse loca...\n",
      "  üîç [31/68] Processing table production.productlistpricehistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table tracks physical inventory of products across different warehouse loca...\n",
      "  üîç [31/68] Processing table production.productlistpricehistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table tracks historical price changes for products over time. It stores pro...\n",
      "  üîç [32/68] Processing table production.productmodel\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table tracks historical price changes for products over time. It stores pro...\n",
      "  üîç [32/68] Processing table production.productmodel\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores product model definitions for a manufacturing or retail system...\n",
      "  üîç [33/68] Processing table production.productmodelillustration\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores product model definitions for a manufacturing or retail system...\n",
      "  üîç [33/68] Processing table production.productmodelillustration\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores relationships between product models and their corresponding i...\n",
      "  üîç [34/68] Processing table production.productmodelproductdescriptionculture\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores relationships between product models and their corresponding i...\n",
      "  üîç [34/68] Processing table production.productmodelproductdescriptionculture\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores multilingual product descriptions by linking product models to...\n",
      "  üîç [35/68] Processing table production.productphoto\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores multilingual product descriptions by linking product models to...\n",
      "  üîç [35/68] Processing table production.productphoto\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores product photo metadata for an e-commerce system. It contains i...\n",
      "  üîç [36/68] Processing table production.productproductphoto\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores product photo metadata for an e-commerce system. It contains i...\n",
      "  üîç [36/68] Processing table production.productproductphoto\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores associations between products and their photos in a product ca...\n",
      "  üîç [37/68] Processing table production.productreview\n",
      "    üìà Retrieved 4 sample rows with 8 columns\n",
      "    ‚úÖ Generated: This table stores associations between products and their photos in a product ca...\n",
      "  üîç [37/68] Processing table production.productreview\n",
      "    üìà Retrieved 4 sample rows with 8 columns\n",
      "    ‚úÖ Generated: This table stores customer product reviews, capturing ratings, comments, and rev...\n",
      "  üîç [38/68] Processing table production.productsubcategory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores customer product reviews, capturing ratings, comments, and rev...\n",
      "  üîç [38/68] Processing table production.productsubcategory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores product subcategories that classify products into more specifi...\n",
      "  üîç [39/68] Processing table production.scrapreason\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores product subcategories that classify products into more specifi...\n",
      "  üîç [39/68] Processing table production.scrapreason\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores standardized reasons for scrapped manufacturing items. It prov...\n",
      "  üîç [40/68] Processing table production.transactionhistory\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores standardized reasons for scrapped manufacturing items. It prov...\n",
      "  üîç [40/68] Processing table production.transactionhistory\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table tracks inventory transactions including sales, work orders, and trans...\n",
      "  üîç [41/68] Processing table production.transactionhistoryarchive\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table tracks inventory transactions including sales, work orders, and trans...\n",
      "  üîç [41/68] Processing table production.transactionhistoryarchive\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores historical transaction records for manufacturing operations. I...\n",
      "  üîç [42/68] Processing table production.unitmeasure\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores historical transaction records for manufacturing operations. I...\n",
      "  üîç [42/68] Processing table production.unitmeasure\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores standard units of measurement used across business operations....\n",
      "  üîç [43/68] Processing table production.workorder\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores standard units of measurement used across business operations....\n",
      "  üîç [43/68] Processing table production.workorder\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table tracks manufacturing work orders, including quantities ordered and sc...\n",
      "  üîç [44/68] Processing table production.workorderrouting\n",
      "    üìà Retrieved 5 sample rows with 12 columns\n",
      "    ‚úÖ Generated: This table tracks manufacturing work orders, including quantities ordered and sc...\n",
      "  üîç [44/68] Processing table production.workorderrouting\n",
      "    üìà Retrieved 5 sample rows with 12 columns\n",
      "    ‚úÖ Generated: This table tracks the detailed routing and scheduling of manufacturing operation...\n",
      "  üîç [45/68] Processing table purchasing.productvendor\n",
      "    üìà Retrieved 5 sample rows with 11 columns\n",
      "    ‚úÖ Generated: This table tracks the detailed routing and scheduling of manufacturing operation...\n",
      "  üîç [45/68] Processing table purchasing.productvendor\n",
      "    üìà Retrieved 5 sample rows with 11 columns\n",
      "    ‚úÖ Generated: This table stores vendor-specific product purchasing information. It tracks supp...\n",
      "  üîç [46/68] Processing table purchasing.purchaseorderdetail\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores vendor-specific product purchasing information. It tracks supp...\n",
      "  üîç [46/68] Processing table purchasing.purchaseorderdetail\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores individual line items for purchase orders. It tracks ordered v...\n",
      "  üîç [47/68] Processing table purchasing.purchaseorderheader\n",
      "    üìà Retrieved 5 sample rows with 12 columns\n",
      "    ‚úÖ Generated: This table stores individual line items for purchase orders. It tracks ordered v...\n",
      "  üîç [47/68] Processing table purchasing.purchaseorderheader\n",
      "    üìà Retrieved 5 sample rows with 12 columns\n",
      "    ‚úÖ Generated: This table stores purchase order header information, tracking the lifecycle of p...\n",
      "  üîç [48/68] Processing table purchasing.shipmethod\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores purchase order header information, tracking the lifecycle of p...\n",
      "  üîç [48/68] Processing table purchasing.shipmethod\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores shipping method options for purchases. It contains shipping se...\n",
      "  üîç [49/68] Processing table purchasing.vendor\n",
      "    üìà Retrieved 5 sample rows with 8 columns\n",
      "    ‚úÖ Generated: This table stores shipping method options for purchases. It contains shipping se...\n",
      "  üîç [49/68] Processing table purchasing.vendor\n",
      "    üìà Retrieved 5 sample rows with 8 columns\n",
      "    ‚úÖ Generated: This table stores master data for suppliers/vendors that the company purchases f...\n",
      "  üîç [50/68] Processing table sales.countryregioncurrency\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores master data for suppliers/vendors that the company purchases f...\n",
      "  üîç [50/68] Processing table sales.countryregioncurrency\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores currency associations for different countries/regions. It maps...\n",
      "  üîç [51/68] Processing table sales.creditcard\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores currency associations for different countries/regions. It maps...\n",
      "  üîç [51/68] Processing table sales.creditcard\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores credit card information for customers. Its business purpose is...\n",
      "  üîç [52/68] Processing table sales.currency\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores credit card information for customers. Its business purpose is...\n",
      "  üîç [52/68] Processing table sales.currency\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores reference data for world currencies used in business transacti...\n",
      "  üîç [53/68] Processing table sales.currencyrate\n",
      "    üìà Retrieved 5 sample rows with 7 columns\n",
      "    ‚úÖ Generated: This table stores reference data for world currencies used in business transacti...\n",
      "  üîç [53/68] Processing table sales.currencyrate\n",
      "    üìà Retrieved 5 sample rows with 7 columns\n",
      "    ‚úÖ Generated: This table stores daily currency exchange rates between different currencies, wi...\n",
      "  üîç [54/68] Processing table sales.customer\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores daily currency exchange rates between different currencies, wi...\n",
      "  üîç [54/68] Processing table sales.customer\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores customer records for a sales system, tracking both individual ...\n",
      "  üîç [55/68] Processing table sales.personcreditcard\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores customer records for a sales system, tracking both individual ...\n",
      "  üîç [55/68] Processing table sales.personcreditcard\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores associations between salespeople (businessentityid) and their ...\n",
      "  üîç [56/68] Processing table sales.salesorderdetail\n",
      "    üìà Retrieved 5 sample rows with 10 columns\n",
      "    ‚úÖ Generated: This table stores associations between salespeople (businessentityid) and their ...\n",
      "  üîç [56/68] Processing table sales.salesorderdetail\n",
      "    üìà Retrieved 5 sample rows with 10 columns\n",
      "    ‚úÖ Generated: This table stores individual line items for sales orders, representing each prod...\n",
      "  üîç [57/68] Processing table sales.salesorderheader\n",
      "    üìà Retrieved 5 sample rows with 25 columns\n",
      "    ‚úÖ Generated: This table stores individual line items for sales orders, representing each prod...\n",
      "  üîç [57/68] Processing table sales.salesorderheader\n",
      "    üìà Retrieved 5 sample rows with 25 columns\n",
      "    ‚úÖ Generated: This table stores sales order header information, serving as the master record f...\n",
      "  üîç [58/68] Processing table sales.salesorderheadersalesreason\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores sales order header information, serving as the master record f...\n",
      "  üîç [58/68] Processing table sales.salesorderheadersalesreason\n",
      "    üìà Retrieved 5 sample rows with 3 columns\n",
      "    ‚úÖ Generated: This table stores the sales reasons associated with specific sales orders, actin...\n",
      "  üîç [59/68] Processing table sales.salesperson\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores the sales reasons associated with specific sales orders, actin...\n",
      "  üîç [59/68] Processing table sales.salesperson\n",
      "    üìà Retrieved 5 sample rows with 9 columns\n",
      "    ‚úÖ Generated: This table stores sales performance data for sales personnel. It tracks key metr...\n",
      "  üîç [60/68] Processing table sales.salespersonquotahistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table stores sales performance data for sales personnel. It tracks key metr...\n",
      "  üîç [60/68] Processing table sales.salespersonquotahistory\n",
      "    üìà Retrieved 5 sample rows with 5 columns\n",
      "    ‚úÖ Generated: This table tracks historical sales quota targets assigned to salespeople over ti...\n",
      "  üîç [61/68] Processing table sales.salesreason\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table tracks historical sales quota targets assigned to salespeople over ti...\n",
      "  üîç [61/68] Processing table sales.salesreason\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores standardized reasons for sales transactions in a business syst...\n",
      "  üîç [62/68] Processing table sales.salestaxrate\n",
      "    üìà Retrieved 5 sample rows with 7 columns\n",
      "    ‚úÖ Generated: This table stores standardized reasons for sales transactions in a business syst...\n",
      "  üîç [62/68] Processing table sales.salestaxrate\n",
      "    üìà Retrieved 5 sample rows with 7 columns\n",
      "    ‚úÖ Generated: This table stores sales tax rates for different state/province jurisdictions. It...\n",
      "  üîç [63/68] Processing table sales.salesterritory\n",
      "    üìà Retrieved 5 sample rows with 10 columns\n",
      "    ‚úÖ Generated: This table stores sales tax rates for different state/province jurisdictions. It...\n",
      "  üîç [63/68] Processing table sales.salesterritory\n",
      "    üìà Retrieved 5 sample rows with 10 columns\n",
      "    ‚úÖ Generated: This table stores sales territory information including geographic regions, thei...\n",
      "  üîç [64/68] Processing table sales.salesterritoryhistory\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores sales territory information including geographic regions, thei...\n",
      "  üîç [64/68] Processing table sales.salesterritoryhistory\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table tracks the assignment history of salespeople to territories over time...\n",
      "  üîç [65/68] Processing table sales.shoppingcartitem\n",
      "    üìà Retrieved 3 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table tracks the assignment history of salespeople to territories over time...\n",
      "  üîç [65/68] Processing table sales.shoppingcartitem\n",
      "    üìà Retrieved 3 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores individual items placed in shopping carts by customers. It tra...\n",
      "  üîç [66/68] Processing table sales.specialoffer\n",
      "    üìà Retrieved 5 sample rows with 11 columns\n",
      "    ‚úÖ Generated: This table stores individual items placed in shopping carts by customers. It tra...\n",
      "  üîç [66/68] Processing table sales.specialoffer\n",
      "    üìà Retrieved 5 sample rows with 11 columns\n",
      "    ‚úÖ Generated: This table stores promotional offers and discounts for products. Its business pu...\n",
      "  üîç [67/68] Processing table sales.specialofferproduct\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores promotional offers and discounts for products. Its business pu...\n",
      "  üîç [67/68] Processing table sales.specialofferproduct\n",
      "    üìà Retrieved 5 sample rows with 4 columns\n",
      "    ‚úÖ Generated: This table stores associations between special offers and products in a sales sy...\n",
      "  üîç [68/68] Processing table sales.store\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores associations between special offers and products in a sales sy...\n",
      "  üîç [68/68] Processing table sales.store\n",
      "    üìà Retrieved 5 sample rows with 6 columns\n",
      "    ‚úÖ Generated: This table stores information about retail stores, including their names, assign...\n",
      "\n",
      "üéâ Completed table description generation!\n",
      "‚úÖ Generated descriptions for 68 tables\n",
      "‚è≠Ô∏è  Skipped 0 tables\n",
      "\n",
      "================================================================================\n",
      "üìã SAMPLE RESULTS\n",
      "================================================================================\n",
      "    ‚úÖ Generated: This table stores information about retail stores, including their names, assign...\n",
      "\n",
      "üéâ Completed table description generation!\n",
      "‚úÖ Generated descriptions for 68 tables\n",
      "‚è≠Ô∏è  Skipped 0 tables\n",
      "\n",
      "================================================================================\n",
      "üìã SAMPLE RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_SCHEMA</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>INFERRED_COLUMN_DESCRIPTION</th>\n",
       "      <th>INFERRED_TABLE_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>departmentid</td>\n",
       "      <td>integer</td>\n",
       "      <td>A unique numerical identifier for each departm...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>name</td>\n",
       "      <td>character varying</td>\n",
       "      <td>The official, descriptive name of a specific d...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>groupname</td>\n",
       "      <td>character varying</td>\n",
       "      <td>The broader organizational group or category t...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>modifieddate</td>\n",
       "      <td>timestamp without time zone</td>\n",
       "      <td>The date and time when the department record w...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>businessentityid</td>\n",
       "      <td>integer</td>\n",
       "      <td>A unique numerical identifier for each employe...</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>nationalidnumber</td>\n",
       "      <td>character varying</td>\n",
       "      <td>A government-issued national identification nu...</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>loginid</td>\n",
       "      <td>character varying</td>\n",
       "      <td>The network login ID for the employee, formatt...</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>jobtitle</td>\n",
       "      <td>character varying</td>\n",
       "      <td>The official job title or role of the employee...</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>birthdate</td>\n",
       "      <td>date</td>\n",
       "      <td>The employee's date of birth, stored in the st...</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>maritalstatus</td>\n",
       "      <td>character</td>\n",
       "      <td>Indicates the employee's marital status, where...</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TABLE_SCHEMA  TABLE_NAME       COLUMN_NAME                    DATA_TYPE  \\\n",
       "0  humanresources  department      departmentid                      integer   \n",
       "1  humanresources  department              name            character varying   \n",
       "2  humanresources  department         groupname            character varying   \n",
       "3  humanresources  department      modifieddate  timestamp without time zone   \n",
       "4  humanresources    employee  businessentityid                      integer   \n",
       "5  humanresources    employee  nationalidnumber            character varying   \n",
       "6  humanresources    employee           loginid            character varying   \n",
       "7  humanresources    employee          jobtitle            character varying   \n",
       "8  humanresources    employee         birthdate                         date   \n",
       "9  humanresources    employee     maritalstatus                    character   \n",
       "\n",
       "                         INFERRED_COLUMN_DESCRIPTION  \\\n",
       "0  A unique numerical identifier for each departm...   \n",
       "1  The official, descriptive name of a specific d...   \n",
       "2  The broader organizational group or category t...   \n",
       "3  The date and time when the department record w...   \n",
       "4  A unique numerical identifier for each employe...   \n",
       "5  A government-issued national identification nu...   \n",
       "6  The network login ID for the employee, formatt...   \n",
       "7  The official job title or role of the employee...   \n",
       "8  The employee's date of birth, stored in the st...   \n",
       "9  Indicates the employee's marital status, where...   \n",
       "\n",
       "                          INFERRED_TABLE_DESCRIPTION  \n",
       "0  This table stores department master data for o...  \n",
       "1  This table stores department master data for o...  \n",
       "2  This table stores department master data for o...  \n",
       "3  This table stores department master data for o...  \n",
       "4  This table stores core employee master data fo...  \n",
       "5  This table stores core employee master data fo...  \n",
       "6  This table stores core employee master data fo...  \n",
       "7  This table stores core employee master data fo...  \n",
       "8  This table stores core employee master data fo...  \n",
       "9  This table stores core employee master data fo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TABLE-LEVEL SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_SCHEMA</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>INFERRED_TABLE_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employee</td>\n",
       "      <td>This table stores core employee master data fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employeedepartmenthistory</td>\n",
       "      <td>This table tracks employee department and shif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>employeepayhistory</td>\n",
       "      <td>Error generating description for table humanre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>jobcandidate</td>\n",
       "      <td>Error generating description for table humanre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>sales</td>\n",
       "      <td>salesterritoryhistory</td>\n",
       "      <td>This table tracks the assignment history of sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>sales</td>\n",
       "      <td>shoppingcartitem</td>\n",
       "      <td>This table stores individual items placed in s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>sales</td>\n",
       "      <td>specialoffer</td>\n",
       "      <td>This table stores promotional offers and disco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>sales</td>\n",
       "      <td>specialofferproduct</td>\n",
       "      <td>This table stores associations between special...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>sales</td>\n",
       "      <td>store</td>\n",
       "      <td>This table stores information about retail sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TABLE_SCHEMA                 TABLE_NAME  \\\n",
       "0    humanresources                 department   \n",
       "4    humanresources                   employee   \n",
       "19   humanresources  employeedepartmenthistory   \n",
       "25   humanresources         employeepayhistory   \n",
       "30   humanresources               jobcandidate   \n",
       "..              ...                        ...   \n",
       "423           sales      salesterritoryhistory   \n",
       "429           sales           shoppingcartitem   \n",
       "435           sales               specialoffer   \n",
       "446           sales        specialofferproduct   \n",
       "450           sales                      store   \n",
       "\n",
       "                            INFERRED_TABLE_DESCRIPTION  \n",
       "0    This table stores department master data for o...  \n",
       "4    This table stores core employee master data fo...  \n",
       "19   This table tracks employee department and shif...  \n",
       "25   Error generating description for table humanre...  \n",
       "30   Error generating description for table humanre...  \n",
       "..                                                 ...  \n",
       "423  This table tracks the assignment history of sa...  \n",
       "429  This table stores individual items placed in s...  \n",
       "435  This table stores promotional offers and disco...  \n",
       "446  This table stores associations between special...  \n",
       "450  This table stores information about retail sto...  \n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving enhanced metadata to Excel...\n",
      "üìÇ Opening existing file: metadata_Adventureworks.xlsx\n",
      "‚úÖ Enhanced metadata saved to sheet 'Metadata_Enhanced' in: /home/ubuntu/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/opensearch-POSTGRES-RAG/metadata_Adventureworks.xlsx\n",
      "üìä File size: 85354 bytes\n",
      "üìã Total rows saved: 456\n",
      "\n",
      "================================================================================\n",
      "üéâ SUCCESS! OPTIMIZED METADATA ENHANCEMENT COMPLETE!\n",
      "================================================================================\n",
      "üìÅ File: metadata_Adventureworks.xlsx\n",
      "üìÑ Sheet: Metadata_Enhanced\n",
      "‚úÖ Columns processed: 456\n",
      "‚úÖ Tables processed: 68\n",
      "\n",
      "üìä Columns in Excel:\n",
      "   ‚Ä¢ TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME\n",
      "   ‚Ä¢ DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT\n",
      "   ‚Ä¢ INFERRED_COLUMN_DESCRIPTION (AI-generated)\n",
      "   ‚Ä¢ INFERRED_TABLE_DESCRIPTION (AI-generated)\n",
      "\n",
      "================================================================================\n",
      "‚ö° PERFORMANCE BENEFITS:\n",
      "================================================================================\n",
      "‚Ä¢ Hybrid strategy: 5-10x faster than sequential processing\n",
      "‚Ä¢ Batch processing: Reduces API calls significantly\n",
      "‚Ä¢ Parallel execution: Utilizes concurrent API requests\n",
      "‚Ä¢ Smart sampling: Uses limited data for faster processing\n",
      "\n",
      "üí∞ COST SAVINGS:\n",
      "‚Ä¢ Batch processing reduces total API calls\n",
      "‚Ä¢ Schema exclusion skips unnecessary tables\n",
      "‚Ä¢ Typical cost: $0.05-0.10 for 100-200 columns\n"
     ]
    }
   ],
   "source": [
    "# Execute OPTIMIZED workflow: Column + Table descriptions (5-10x faster)\n",
    "if 'metadata_df' in locals() and metadata_df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ OPTIMIZED METADATA ENHANCEMENT WORKFLOW (HYBRID STRATEGY)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Generate column descriptions using OPTIMIZED hybrid strategy\n",
    "    print(\"\\nüìù STEP 1: Generating Column Descriptions (Optimized)...\")\n",
    "    print(\"-\" * 80)\n",
    "    metadata_df_optimized = add_inferred_descriptions_optimized(\n",
    "        metadata_df, \n",
    "        db_connector,\n",
    "        schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA,\n",
    "        strategy='hybrid',    # ‚≠ê RECOMMENDED: 5-10x faster\n",
    "        batch_size=3,         # Process 3 columns per API call (adjust for token limits)\n",
    "        max_workers=2         # Run 2 batches in parallel (adjust for rate limits)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate table descriptions (sequential is fine for fewer tables)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä STEP 2: Generating Table Descriptions...\")\n",
    "    print(\"-\" * 80)\n",
    "    metadata_df_optimized = add_table_descriptions_to_metadata(\n",
    "        metadata_df_optimized,\n",
    "        db_connector,\n",
    "        schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\n",
    "    )\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìã SAMPLE RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    display(metadata_df_optimized[[\n",
    "        'TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE',\n",
    "        'INFERRED_COLUMN_DESCRIPTION', 'INFERRED_TABLE_DESCRIPTION'\n",
    "    ]].head(10))\n",
    "    \n",
    "    # Show table-level summary\n",
    "    print(\"\\nüìä TABLE-LEVEL SUMMARY:\")\n",
    "    table_summary = metadata_df_optimized[['TABLE_SCHEMA', 'TABLE_NAME', 'INFERRED_TABLE_DESCRIPTION']].drop_duplicates()\n",
    "    display(table_summary)\n",
    "    \n",
    "    # Save to Excel\n",
    "    print(\"\\nüíæ Saving enhanced metadata to Excel...\")\n",
    "    if save_enhanced_metadata_to_excel(metadata_df_optimized, db_connector):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéâ SUCCESS! OPTIMIZED METADATA ENHANCEMENT COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìÅ File: metadata_{db_connector.database}.xlsx\")\n",
    "        print(f\"üìÑ Sheet: Metadata_Enhanced\")\n",
    "        print(f\"‚úÖ Columns processed: {len(metadata_df_optimized)}\")\n",
    "        print(f\"‚úÖ Tables processed: {len(table_summary)}\")\n",
    "        print(\"\\nüìä Columns in Excel:\")\n",
    "        print(\"   ‚Ä¢ TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME\")\n",
    "        print(\"   ‚Ä¢ DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT\")\n",
    "        print(\"   ‚Ä¢ INFERRED_COLUMN_DESCRIPTION (AI-generated)\")\n",
    "        print(\"   ‚Ä¢ INFERRED_TABLE_DESCRIPTION (AI-generated)\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to save enhanced metadata.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  metadata_df not available. Run the metadata extraction cells first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ö° PERFORMANCE BENEFITS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚Ä¢ Hybrid strategy: 5-10x faster than sequential processing\")\n",
    "print(\"‚Ä¢ Batch processing: Reduces API calls significantly\")\n",
    "print(\"‚Ä¢ Parallel execution: Utilizes concurrent API requests\")\n",
    "print(\"‚Ä¢ Smart sampling: Uses limited data for faster processing\")\n",
    "print(\"\\nüí∞ COST SAVINGS:\")\n",
    "print(\"‚Ä¢ Batch processing reduces total API calls\")\n",
    "print(\"‚Ä¢ Schema exclusion skips unnecessary tables\")\n",
    "print(\"‚Ä¢ Typical cost: $0.05-0.10 for 100-200 columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09780a6",
   "metadata": {},
   "source": [
    "!\"metadata\".(attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4a8b9",
   "metadata": {},
   "source": [
    "## üê≥ Start Docker Environment\n",
    "\n",
    "First, start the OpenSearch cluster using the optimized docker-compose file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0f3627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network 4text_to_sql_visualization_insights_agent_opensearch-net  Creating\n",
      " Network 4text_to_sql_visualization_insights_agent_opensearch-net  Created\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data1\"  Creating\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data1\"  Created\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data2\"  Creating\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data2\"  Created\n",
      "time=\"2025-10-30T11:48:34-04:00\" level=warning msg=\"Found orphan containers ([4text_to_sql_visualization_insights_agent-db-1 4text_to_sql_visualization_insights_agent-adminer-1]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\"\n",
      " Container opensearch-optimized-node2  Creating\n",
      " Container opensearch-optimized-dashboards  Creating\n",
      " Container opensearch-optimized-node1  Creating\n",
      " Network 4text_to_sql_visualization_insights_agent_opensearch-net  Created\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data1\"  Creating\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data1\"  Created\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data2\"  Creating\n",
      " Volume \"4text_to_sql_visualization_insights_agent_opensearch-optimized-data2\"  Created\n",
      "time=\"2025-10-30T11:48:34-04:00\" level=warning msg=\"Found orphan containers ([4text_to_sql_visualization_insights_agent-db-1 4text_to_sql_visualization_insights_agent-adminer-1]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\"\n",
      " Container opensearch-optimized-node2  Creating\n",
      " Container opensearch-optimized-dashboards  Creating\n",
      " Container opensearch-optimized-node1  Creating\n",
      " Container opensearch-optimized-node1  Created\n",
      " Container opensearch-optimized-node2  Created\n",
      " Container opensearch-optimized-node1  Created\n",
      " Container opensearch-optimized-node2  Created\n",
      " Container opensearch-optimized-dashboards  Created\n",
      " Container opensearch-optimized-node1  Starting\n",
      " Container opensearch-optimized-node2  Starting\n",
      " Container opensearch-optimized-dashboards  Starting\n",
      " Container opensearch-optimized-dashboards  Created\n",
      " Container opensearch-optimized-node1  Starting\n",
      " Container opensearch-optimized-node2  Starting\n",
      " Container opensearch-optimized-dashboards  Starting\n",
      " Container opensearch-optimized-dashboards  Started\n",
      " Container opensearch-optimized-node2  Started\n",
      " Container opensearch-optimized-dashboards  Started\n",
      " Container opensearch-optimized-node2  Started\n",
      " Container opensearch-optimized-node1  Started\n",
      " Container opensearch-optimized-node1  Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for OpenSearch to start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0             Dload  Upload   Total   Spent    Left  Speed\n",
      "100   550  100   550    0     0   2102      0 --:--:-- --:--:-- --:--:--  2107\n",
      "100   550  100   550    0     0   2102      0 --:--:-- --:--:-- --:--:--  2107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cluster_name\" : \"opensearch-optimized-cluster\",\n",
      "  \"status\" : \"green\",\n",
      "  \"timed_out\" : false,\n",
      "  \"number_of_nodes\" : 2,\n",
      "  \"number_of_data_nodes\" : 2,\n",
      "  \"discovered_master\" : true,\n",
      "  \"discovered_cluster_manager\" : true,\n",
      "  \"active_primary_shards\" : 3,\n",
      "  \"active_shards\" : 6,\n",
      "  \"relocating_shards\" : 0,\n",
      "  \"initializing_shards\" : 0,\n",
      "  \"unassigned_shards\" : 0,\n",
      "  \"delayed_unassigned_shards\" : 0,\n",
      "  \"number_of_pending_tasks\" : 0,\n",
      "  \"number_of_in_flight_fetch\" : 0,\n",
      "  \"task_max_waiting_in_queue_millis\" : 0,\n",
      "  \"active_shards_percent_as_number\" : 100.0\n",
      "}\n",
      "\"number_of_nodes\" : 2,\n",
      "  \"number_of_data_nodes\" : 2,\n",
      "  \"discovered_master\" : true,\n",
      "  \"discovered_cluster_manager\" : true,\n",
      "  \"active_primary_shards\" : 3,\n",
      "  \"active_shards\" : 6,\n",
      "  \"relocating_shards\" : 0,\n",
      "  \"initializing_shards\" : 0,\n",
      "  \"unassigned_shards\" : 0,\n",
      "  \"delayed_unassigned_shards\" : 0,\n",
      "  \"number_of_pending_tasks\" : 0,\n",
      "  \"number_of_in_flight_fetch\" : 0,\n",
      "  \"task_max_waiting_in_queue_millis\" : 0,\n",
      "  \"active_shards_percent_as_number\" : 100.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Navigate to the docker compose directory and start the cluster\n",
    "cd ../\n",
    "docker compose -f docker-compose-fully-optimized.yml up -d\n",
    "\n",
    "# Wait for the cluster to be ready\n",
    "echo \"Waiting for OpenSearch to start...\"\n",
    "sleep 30\n",
    "\n",
    "# Check cluster health\n",
    "curl -k -u admin:Developer@123 https://localhost:9200/_cluster/health?pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b1de7",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41803e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "from opensearchpy import OpenSearch, helpers\n",
    "from opensearchpy import AsyncOpenSearch\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d530a4",
   "metadata": {},
   "source": [
    "## üîåConnect to OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3973116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to OpenSearch cluster: opensearch-optimized-cluster\n",
      "üìä Version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# OpenSearch connection configuration\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{'host': 'localhost', 'port': 9200}],\n",
    "    http_auth=('admin', 'Developer@123'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# Async client for performance comparison\n",
    "async_client = AsyncOpenSearch(\n",
    "    hosts=[{'host': 'localhost', 'port': 9200}],\n",
    "    http_auth=('admin', 'Developer@123'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# Initialize ML Commons client\n",
    "ml_client = MLCommonClient(os_client)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    info = os_client.info()\n",
    "    print(f\"‚úÖ Connected to OpenSearch cluster: {info['cluster_name']}\")\n",
    "    print(f\"üìä Version: {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66fcbe",
   "metadata": {},
   "source": [
    "# Read Meta Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da40045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded enhanced metadata with 456 columns from Excel.\n",
      "üìã Columns: ['TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'INFERRED_COLUMN_DESCRIPTION', 'INFERRED_TABLE_DESCRIPTION']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_SCHEMA</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>INFERRED_COLUMN_DESCRIPTION</th>\n",
       "      <th>INFERRED_TABLE_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>departmentid</td>\n",
       "      <td>integer</td>\n",
       "      <td>A unique numerical identifier for each departm...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>name</td>\n",
       "      <td>character varying</td>\n",
       "      <td>The official, descriptive name of a specific d...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humanresources</td>\n",
       "      <td>department</td>\n",
       "      <td>groupname</td>\n",
       "      <td>character varying</td>\n",
       "      <td>The broader organizational group or category t...</td>\n",
       "      <td>This table stores department master data for o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TABLE_SCHEMA  TABLE_NAME   COLUMN_NAME          DATA_TYPE  \\\n",
       "0  humanresources  department  departmentid            integer   \n",
       "1  humanresources  department          name  character varying   \n",
       "2  humanresources  department     groupname  character varying   \n",
       "\n",
       "                         INFERRED_COLUMN_DESCRIPTION  \\\n",
       "0  A unique numerical identifier for each departm...   \n",
       "1  The official, descriptive name of a specific d...   \n",
       "2  The broader organizational group or category t...   \n",
       "\n",
       "                          INFERRED_TABLE_DESCRIPTION  \n",
       "0  This table stores department master data for o...  \n",
       "1  This table stores department master data for o...  \n",
       "2  This table stores department master data for o...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata_df_optimized = pd.read_excel(f\"metadata_{db_connector.database}.xlsx\", sheet_name=\"Metadata_Enhanced\")\n",
    "metadata_df_optimized = metadata_df_optimized[(metadata_df_optimized['INFERRED_COLUMN_DESCRIPTION'] != 'Excluded schema - not processed') & \n",
    "           (metadata_df_optimized['INFERRED_TABLE_DESCRIPTION'] != 'Excluded schema - not processed')]\n",
    "metadata_df_optimized.reset_index(drop=True, inplace=True)\n",
    "NEEDED_COLUMNS = [\n",
    "    'TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE',\n",
    "    'INFERRED_COLUMN_DESCRIPTION', 'INFERRED_TABLE_DESCRIPTION'\n",
    "]\n",
    "metadata_df_optimized = metadata_df_optimized[NEEDED_COLUMNS]\n",
    "print(f\"‚úÖ Loaded enhanced metadata with {len(metadata_df_optimized)} columns from Excel.\")\n",
    "print(f\"üìã Columns: {metadata_df_optimized.columns.tolist()}\")\n",
    "metadata_df_optimized.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb17b4",
   "metadata": {},
   "source": [
    "## Create Generic Function to Generate OpenSearch Mappings from DataFrame\n",
    "\n",
    "This function analyzes a pandas DataFrame and automatically generates OpenSearch index mappings based on the column data types.\n",
    "\n",
    "**Key Features:**\n",
    "- Maps pandas dtypes to appropriate OpenSearch field types\n",
    "- Optionally creates corresponding `knn_vector` fields for text columns to support semantic search\n",
    "- The vector fields are configured with:\n",
    "  - Dimensions: 768 (standard for many embedding models)\n",
    "  - Method: HNSW (Hierarchical Navigable Small World graphs)\n",
    "  - Space type: L2 (Euclidean distance)\n",
    "  - Engine: Lucene\n",
    "- Handles nested objects and arrays by using the `nested` type\n",
    "- Returns a complete index body structure ready for index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8101963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensearch_mappings(df, create_vectors=False, pipeline_name=None, exclude_from_vectors=None):\n",
    "    \"\"\"\n",
    "    Create OpenSearch index mappings from a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to generate mappings from\n",
    "    create_vectors : bool, default=False\n",
    "        If True, creates corresponding knn_vector fields for text columns\n",
    "        with dimensions=768, method=hnsw, space_type=l2, engine=lucene\n",
    "    pipeline_name : str, optional\n",
    "        If provided, sets this as the default_pipeline in index settings.\n",
    "        Used for automatic embedding generation during ingestion.\n",
    "    exclude_from_vectors : list of str, optional\n",
    "        List of field names to exclude from vector creation.\n",
    "        Default is ['id', 'title'] if not provided.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the index body with mappings suitable for \n",
    "        OpenSearch index creation\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> mappings = create_opensearch_mappings(df, create_vectors=True, exclude_from_vectors=['id', 'title', 'metadata'])\n",
    "    >>> os_client.indices.create(index='my_index', body=mappings)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set default exclusion list if not provided\n",
    "    if exclude_from_vectors is None:\n",
    "        exclude_from_vectors = ['id']\n",
    "    \n",
    "    # Define dtype mapping from pandas to OpenSearch\n",
    "    dtype_mapping = {\n",
    "        'int64': 'long',\n",
    "        'int32': 'integer',\n",
    "        'int16': 'short',\n",
    "        'int8': 'byte',\n",
    "        'float64': 'double',\n",
    "        'float32': 'float',\n",
    "        'bool': 'boolean',\n",
    "        'datetime64[ns]': 'date',\n",
    "        'object': 'text',  # Default for object types (strings)\n",
    "    }\n",
    "    \n",
    "    properties = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        dtype_str = str(df[column].dtype)\n",
    "        \n",
    "        # Handle datetime types\n",
    "        if 'datetime' in dtype_str:\n",
    "            properties[column] = {'type': 'date'}\n",
    "        \n",
    "        # Handle boolean\n",
    "        elif dtype_str == 'bool':\n",
    "            properties[column] = {'type': 'boolean'}\n",
    "        \n",
    "        # Handle numeric types\n",
    "        elif dtype_str in ['int64', 'int32', 'int16', 'int8']:\n",
    "            properties[column] = {'type': dtype_mapping.get(dtype_str, 'long')}\n",
    "        \n",
    "        elif dtype_str in ['float64', 'float32']:\n",
    "            properties[column] = {'type': dtype_mapping.get(dtype_str, 'double')}\n",
    "        \n",
    "        # Handle object types (strings, nested structures)\n",
    "        elif dtype_str == 'object':\n",
    "            # Check if column contains nested structures (dict/list)\n",
    "            sample_value = df[column].dropna().iloc[0] if not df[column].dropna().empty else None\n",
    "            \n",
    "            if isinstance(sample_value, (dict, list)):\n",
    "                # Use nested type for complex structures\n",
    "                properties[column] = {'type': 'nested'}\n",
    "            else:\n",
    "                # Standard text field with keyword sub-field\n",
    "                properties[column] = {\n",
    "                    'type': 'text',\n",
    "                    'fields': {\n",
    "                        'keyword': {\n",
    "                            'type': 'keyword',\n",
    "                            'ignore_above': 256\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Optionally create vector field for text columns\n",
    "                # Exclude specified fields from vector creation\n",
    "                if create_vectors and column not in exclude_from_vectors:\n",
    "                    vector_field_name = f\"{column}_embedding\"\n",
    "                    properties[vector_field_name] = {\n",
    "                        'type': 'knn_vector',\n",
    "                        'dimension': 768,\n",
    "                        'method': {\n",
    "                            'name': 'hnsw',\n",
    "                            'space_type': 'l2',\n",
    "                            'engine': 'lucene',\n",
    "                            'parameters': {}\n",
    "                        }\n",
    "                    }\n",
    "        \n",
    "        # Default fallback\n",
    "        else:\n",
    "            properties[column] = {'type': 'text'}\n",
    "    \n",
    "    # Create the settings object\n",
    "    settings = {\n",
    "        'index': {\n",
    "            'number_of_shards': 1,\n",
    "            'number_of_replicas': 1,\n",
    "            'knn': create_vectors  # Enable k-NN only if vectors are being created\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add default_pipeline if provided\n",
    "    if pipeline_name:\n",
    "        settings['default_pipeline'] = pipeline_name\n",
    "    \n",
    "    # Create the complete index body\n",
    "    index_body = {\n",
    "        'settings': settings,\n",
    "        'mappings': {\n",
    "            'properties': properties\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return index_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c997dc9",
   "metadata": {},
   "source": [
    "## Setup ML Model and Ingest Pipeline for Automatic Embeddings\n",
    "\n",
    "Before creating an index with vector fields that automatically generates embeddings, we need to:\n",
    "\n",
    "1. **Register and deploy a pre-trained embedding model** from HuggingFace\n",
    "2. **Create an ingest pipeline** that uses this model to generate embeddings automatically during indexing\n",
    "3. **Configure the index** to use this pipeline as the default pipeline\n",
    "\n",
    "This setup enables automatic embedding generation during document ingestion, eliminating the need to manually create embeddings before indexing.\n",
    "\n",
    "**Model Details:**\n",
    "- Model: `huggingface/sentence-transformers/msmarco-distilbert-base-tas-b`\n",
    "- Version: 1.0.1\n",
    "- Format: TORCH_SCRIPT\n",
    "- Dimensions: 768\n",
    "- Use case: Semantic search, question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43cabc",
   "metadata": {},
   "source": [
    "## Configure ML Settings\n",
    "\n",
    "Configure OpenSearch to allow ML operations on data nodes.\n",
    "\n",
    "**Note:** In production environments with dedicated ML nodes, this configuration is not needed. For development/testing, we allow ML operations on data nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "715ec4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ML Configuration Status:\n",
      "================================================================================\n",
      "‚úì ML settings configured successfully\n",
      "  - ML operations allowed on data nodes: True\n",
      "  - Model access control: Disabled\n",
      "  - Native memory threshold: 99%\n",
      "\n",
      "‚úì Cluster is ready for ML model deployment\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure cluster to allow ML operations\n",
    "ml_settings = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.only_run_on_ml_node\": False,\n",
    "        \"plugins.ml_commons.model_access_control_enabled\": False,\n",
    "        \"plugins.ml_commons.native_memory_threshold\": 99\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = os_client.cluster.put_settings(body=ml_settings)\n",
    "    print(\"=\"*80)\n",
    "    print(\"ML Configuration Status:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úì ML settings configured successfully\")\n",
    "    print(\"  - ML operations allowed on data nodes: True\")\n",
    "    print(\"  - Model access control: Disabled\")\n",
    "    print(\"  - Native memory threshold: 99%\")\n",
    "    print(\"\\n‚úì Cluster is ready for ML model deployment\")\n",
    "    print(\"=\"*80)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Warning: Could not configure ML settings: {e}\")\n",
    "    print(\"  If ML nodes are properly configured, this error can be ignored\")\n",
    "    print(\"  Proceeding with model deployment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39c10006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Registering and deploying ML model...\n",
      "================================================================================\n",
      "Model was registered successfully. Model Id:  -4znNZoBZxSX-XzpqVND\n",
      "-4znNZoBZxSX-XzpqVND\n",
      "Task ID: uu7oNZoBZGskrW_aLsK7\n",
      "Model was registered successfully. Model Id:  -4znNZoBZxSX-XzpqVND\n",
      "-4znNZoBZxSX-XzpqVND\n",
      "Task ID: uu7oNZoBZGskrW_aLsK7\n",
      "Model deployed successfully\n",
      "Model ID: -4znNZoBZxSX-XzpqVND\n",
      "\n",
      "Waiting for model deployment...\n",
      "Current model state: DEPLOYED\n",
      "‚úì Model deployed successfully!\n",
      "\n",
      "================================================================================\n",
      "Model is ready for use\n",
      "================================================================================\n",
      "Model deployed successfully\n",
      "Model ID: -4znNZoBZxSX-XzpqVND\n",
      "\n",
      "Waiting for model deployment...\n",
      "Current model state: DEPLOYED\n",
      "‚úì Model deployed successfully!\n",
      "\n",
      "================================================================================\n",
      "Model is ready for use\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Register and deploy the sentence transformer model\n",
    "print(\"=\"*80)\n",
    "print(\"Registering and deploying ML model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_response = ml_client.register_pretrained_model(\n",
    "    model_name=\"huggingface/sentence-transformers/msmarco-distilbert-base-tas-b\",\n",
    "    model_version=\"1.0.1\",\n",
    "    model_format=\"TORCH_SCRIPT\",\n",
    "    deploy_model=True,\n",
    "    wait_until_deployed=True\n",
    ")\n",
    "model_id = model_response\n",
    "print(f\"Model ID: {model_id}\")\n",
    "\n",
    "# Step 2: Wait for model to be fully deployed\n",
    "print(\"\\nWaiting for model deployment...\")\n",
    "max_wait_time = 300  # 5 minutes max wait\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    model_info = ml_client.get_model_info(model_id)\n",
    "    model_state = model_info.get('model_state', 'UNKNOWN')\n",
    "    print(f\"Current model state: {model_state}\")\n",
    "    \n",
    "    if model_state == 'DEPLOYED':\n",
    "        print(\"‚úì Model deployed successfully!\")\n",
    "        break\n",
    "    \n",
    "    if time.time() - start_time > max_wait_time:\n",
    "        print(\"‚ö† Warning: Model deployment timeout. Proceeding anyway...\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Model is ready for use\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1782a3d",
   "metadata": {},
   "source": [
    "## Create Ingest Pipeline for Automatic Embedding Generation\n",
    "\n",
    "Create an ingest pipeline that automatically generates embeddings for text fields during document ingestion.\n",
    "\n",
    "The pipeline uses the `text_embedding` processor which:\n",
    "- Takes text from specified source fields\n",
    "- Generates 768-dimensional embeddings using the deployed model\n",
    "- Stores embeddings in corresponding vector fields\n",
    "- Runs automatically for every document ingested into indices using this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12941dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing pipeline: text_to_bi\n",
      "‚úì Ingest pipeline created: text_to_bi\n",
      "  Text fields to embed: ['TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'INFERRED_COLUMN_DESCRIPTION', 'INFERRED_TABLE_DESCRIPTION']\n",
      "  Excluded fields: []\n",
      "  Field mappings: {'TABLE_SCHEMA': 'TABLE_SCHEMA_embedding', 'TABLE_NAME': 'TABLE_NAME_embedding', 'COLUMN_NAME': 'COLUMN_NAME_embedding', 'DATA_TYPE': 'DATA_TYPE_embedding', 'INFERRED_COLUMN_DESCRIPTION': 'INFERRED_COLUMN_DESCRIPTION_embedding', 'INFERRED_TABLE_DESCRIPTION': 'INFERRED_TABLE_DESCRIPTION_embedding'}\n",
      "\n",
      "================================================================================\n",
      "Pipeline 'text_to_bi' is ready to use\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a dynamic ingest pipeline based on text fields in the DataFrame\n",
    "def create_embedding_pipeline(df, model_id, pipeline_name=\"text_to_bi\", exclude_from_embeddings=None):\n",
    "    \"\"\"\n",
    "    Create an ingest pipeline that generates embeddings for text fields.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to analyze for text fields\n",
    "    model_id : str\n",
    "        The ID of the deployed ML model\n",
    "    pipeline_name : str\n",
    "        Name for the ingest pipeline\n",
    "    exclude_from_embeddings : list of str, optional\n",
    "        List of field names to exclude from embedding generation.\n",
    "        Default is ['id'] if not provided.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : The pipeline name\n",
    "    \"\"\"\n",
    "    # Set default exclusion list if not provided\n",
    "    if exclude_from_embeddings is None:\n",
    "        exclude_from_embeddings = ['id']\n",
    "    \n",
    "    # Identify text fields (excluding nested structures and excluded fields)\n",
    "    text_fields = []\n",
    "    for column in df.columns:\n",
    "        dtype_str = str(df[column].dtype)\n",
    "        if dtype_str == 'object':\n",
    "            sample_value = df[column].dropna().iloc[0] if not df[column].dropna().empty else None\n",
    "            if not isinstance(sample_value, (dict, list)) and column not in exclude_from_embeddings:\n",
    "                text_fields.append(column)\n",
    "    \n",
    "    # Create field_map for text_embedding processor\n",
    "    field_map = {}\n",
    "    for field in text_fields:\n",
    "        field_map[field] = f\"{field}_embedding\"\n",
    "    \n",
    "    # Create pipeline body\n",
    "    pipeline_body = {\n",
    "        \"description\": f\"Embedding pipeline for {pipeline_name}\",\n",
    "        \"processors\": [\n",
    "            {\n",
    "                \"text_embedding\": {\n",
    "                    \"model_id\": model_id,\n",
    "                    \"field_map\": field_map\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Delete pipeline if it exists\n",
    "    try:\n",
    "        os_client.ingest.delete_pipeline(id=pipeline_name)\n",
    "        print(f\"Deleted existing pipeline: {pipeline_name}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create the pipeline\n",
    "    os_client.ingest.put_pipeline(id=pipeline_name, body=pipeline_body)\n",
    "    print(f\"‚úì Ingest pipeline created: {pipeline_name}\")\n",
    "    print(f\"  Text fields to embed: {text_fields}\")\n",
    "    print(f\"  Excluded fields: {exclude_from_embeddings}\")\n",
    "    print(f\"  Field mappings: {field_map}\")\n",
    "    \n",
    "    return pipeline_name\n",
    "\n",
    "# Create the pipeline with custom exclusions\n",
    "pipeline_name = create_embedding_pipeline(\n",
    "    metadata_df_optimized, \n",
    "    model_id,\n",
    "    exclude_from_embeddings=[]  # Exclude any ids or guids if needed\n",
    ")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Pipeline '{pipeline_name}' is ready to use\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb843d",
   "metadata": {},
   "source": [
    "## Create Index with Pipeline and Ingest Data with Auto-Generated Embeddings\n",
    "\n",
    "Now create an index that uses the ingest pipeline to automatically generate embeddings during document ingestion.\n",
    "\n",
    "**Key Configuration:**\n",
    "- `index.knn: true` - Enables k-NN functionality\n",
    "- `default_pipeline: \"squad_embedding_pipeline\"` - Automatically processes all documents through the pipeline\n",
    "- Vector fields are created for each text field to store the embeddings\n",
    "\n",
    "**What happens during ingestion:**\n",
    "1. Documents are sent to OpenSearch\n",
    "2. The ingest pipeline intercepts them\n",
    "3. Text fields are extracted and sent to the ML model\n",
    "4. The model generates 768-dimensional embeddings\n",
    "5. Embeddings are stored in the corresponding `_embedding` fields\n",
    "6. The complete document (with embeddings) is indexed\n",
    "\n",
    "This approach eliminates the need to manually generate embeddings before ingestion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa48ca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated OpenSearch mappings (WITH vector fields and pipeline):\n",
      "{\n",
      "  \"settings\": {\n",
      "    \"index\": {\n",
      "      \"number_of_shards\": 1,\n",
      "      \"number_of_replicas\": 1,\n",
      "      \"knn\": true\n",
      "    },\n",
      "    \"default_pipeline\": \"text_to_bi\"\n",
      "  },\n",
      "  \"mappings\": {\n",
      "    \"properties\": {\n",
      "      \"TABLE_SCHEMA\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"TABLE_SCHEMA_embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 768,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"l2\",\n",
      "          \"engine\": \"lucene\",\n",
      "          \"parameters\": {}\n",
      "        }\n",
      "      },\n",
      "      \"TABLE_NAME\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"TABLE_NAME_embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 768,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"l2\",\n",
      "          \"engine\": \"lucene\",\n",
      "          \"parameters\": {}\n",
      "        }\n",
      "      },\n",
      "      \"COLUMN_NAME\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"COLUMN_NAME_embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 768,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"l2\",\n",
      "          \"engine\": \"lucene\",\n",
      "          \"parameters\": {}\n",
      "        }\n",
      "      },\n",
      "      \"DATA_TYPE\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"DATA_TYPE_embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 768,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"l2\",\n",
      "          \"engine\": \"lucene\",\n",
      "          \"parameters\": {}\n",
      "        }\n",
      "      },\n",
      "      \"INFERRED_COLUMN_DESCRIPTION\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"INFERRED_COLUMN_DESCRIPTION_embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 768,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"l2\",\n",
      "          \"engine\": \"lucene\",\n",
      "          \"parameters\": {}\n",
      "        }\n",
      "      },\n",
      "      \"INFERRED_TABLE_DESCRIPTION\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"INFERRED_TABLE_DESCRIPTION_embedding\": {\n",
      "        \"type\": \"knn_vector\",\n",
      "        \"dimension\": 768,\n",
      "        \"method\": {\n",
      "          \"name\": \"hnsw\",\n",
      "          \"space_type\": \"l2\",\n",
      "          \"engine\": \"lucene\",\n",
      "          \"parameters\": {}\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Index settings configuration:\n",
      "  - index.knn: True\n",
      "  - default_pipeline: text_to_bi\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Creating index: adventure_works_meta_ai_ready\n",
      "Index created successfully: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'adventure_works_meta_ai_ready'}\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Starting bulk ingestion of 456 documents...\n",
      "Note: Using small sample because embedding generation takes time\n",
      "Index created successfully: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'adventure_works_meta_ai_ready'}\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Starting bulk ingestion of 456 documents...\n",
      "Note: Using small sample because embedding generation takes time\n",
      "Bulk ingestion completed in 20.34 seconds\n",
      "Successfully indexed: 456 documents\n",
      "Failed: [] documents\n",
      "Average time per document: 0.04 seconds\n",
      "================================================================================\n",
      "Bulk ingestion completed in 20.34 seconds\n",
      "Successfully indexed: 456 documents\n",
      "Failed: [] documents\n",
      "Average time per document: 0.04 seconds\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Total documents in index 'adventure_works_meta_ai_ready': 456\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Embedding fields in document:\n",
      "  - COLUMN_NAME_embedding: 768 dimensions\n",
      "    First 5 values: [0.19790447, -0.064286985, -0.11862899, -0.27485344, 0.13894795]\n",
      "  - DATA_TYPE_embedding: 768 dimensions\n",
      "    First 5 values: [-0.29084685, 0.063076235, 0.053746138, -0.14220841, -0.17461713]\n",
      "  - TABLE_NAME_embedding: 768 dimensions\n",
      "    First 5 values: [0.19790447, -0.064286985, -0.11862899, -0.27485344, 0.13894795]\n",
      "  - INFERRED_COLUMN_DESCRIPTION_embedding: 768 dimensions\n",
      "    First 5 values: [-0.38373983, -0.17351303, -0.87113184, -0.24994236, 0.34241614]\n",
      "  - INFERRED_TABLE_DESCRIPTION_embedding: 768 dimensions\n",
      "    First 5 values: [0.23172645, -0.04037999, -0.26747414, 0.13890415, 0.42223942]\n",
      "  - TABLE_SCHEMA_embedding: 768 dimensions\n",
      "    First 5 values: [0.19219078, 0.213061, -0.09494351, -0.3441935, 0.25507024]\n",
      "================================================================================\n",
      "\n",
      "Sample document with embeddings:\n",
      "{\n",
      "  \"COLUMN_NAME_embedding\": \"[768 dimensions]\",\n",
      "  \"DATA_TYPE_embedding\": \"[768 dimensions]\",\n",
      "  \"TABLE_NAME\": \"document\",\n",
      "  \"TABLE_SCHEMA\": \"production\",\n",
      "  \"COLUMN_NAME\": \"document\",\n",
      "  \"INFERRED_TABLE_DESCRIPTION\": \"Unable to sample data from production.document\",\n",
      "  \"TABLE_NAME_embedding\": \"[768 dimensions]\",\n",
      "  \"INFERRED_COLUMN_DESCRIPTION_embedding\": \"[768 dimensions]\",\n",
      "  \"INFERRED_COLUMN_DESCRIPTION\": \"Description unavailable\",\n",
      "  \"INFERRED_TABLE_DESCRIPTION_embedding\": \"[768 dimensions]\",\n",
      "  \"DATA_TYPE\": \"bytea\",\n",
      "  \"TABLE_SCHEMA_embedding\": \"[768 dimensions]\"\n",
      "}\n",
      "CPU times: user 80.1 ms, sys: 14.1 ms, total: 94.3 ms\n",
      "Wall time: 22.6 s\n",
      "\n",
      "================================================================================\n",
      "Total documents in index 'adventure_works_meta_ai_ready': 456\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Embedding fields in document:\n",
      "  - COLUMN_NAME_embedding: 768 dimensions\n",
      "    First 5 values: [0.19790447, -0.064286985, -0.11862899, -0.27485344, 0.13894795]\n",
      "  - DATA_TYPE_embedding: 768 dimensions\n",
      "    First 5 values: [-0.29084685, 0.063076235, 0.053746138, -0.14220841, -0.17461713]\n",
      "  - TABLE_NAME_embedding: 768 dimensions\n",
      "    First 5 values: [0.19790447, -0.064286985, -0.11862899, -0.27485344, 0.13894795]\n",
      "  - INFERRED_COLUMN_DESCRIPTION_embedding: 768 dimensions\n",
      "    First 5 values: [-0.38373983, -0.17351303, -0.87113184, -0.24994236, 0.34241614]\n",
      "  - INFERRED_TABLE_DESCRIPTION_embedding: 768 dimensions\n",
      "    First 5 values: [0.23172645, -0.04037999, -0.26747414, 0.13890415, 0.42223942]\n",
      "  - TABLE_SCHEMA_embedding: 768 dimensions\n",
      "    First 5 values: [0.19219078, 0.213061, -0.09494351, -0.3441935, 0.25507024]\n",
      "================================================================================\n",
      "\n",
      "Sample document with embeddings:\n",
      "{\n",
      "  \"COLUMN_NAME_embedding\": \"[768 dimensions]\",\n",
      "  \"DATA_TYPE_embedding\": \"[768 dimensions]\",\n",
      "  \"TABLE_NAME\": \"document\",\n",
      "  \"TABLE_SCHEMA\": \"production\",\n",
      "  \"COLUMN_NAME\": \"document\",\n",
      "  \"INFERRED_TABLE_DESCRIPTION\": \"Unable to sample data from production.document\",\n",
      "  \"TABLE_NAME_embedding\": \"[768 dimensions]\",\n",
      "  \"INFERRED_COLUMN_DESCRIPTION_embedding\": \"[768 dimensions]\",\n",
      "  \"INFERRED_COLUMN_DESCRIPTION\": \"Description unavailable\",\n",
      "  \"INFERRED_TABLE_DESCRIPTION_embedding\": \"[768 dimensions]\",\n",
      "  \"DATA_TYPE\": \"bytea\",\n",
      "  \"TABLE_SCHEMA_embedding\": \"[768 dimensions]\"\n",
      "}\n",
      "CPU times: user 80.1 ms, sys: 14.1 ms, total: 94.3 ms\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generate_bulk_data(df, index_name):\n",
    "    \"\"\"\n",
    "    Generator function to prepare data for bulk ingestion.\n",
    "    Yields documents in the format required by opensearch helpers.bulk()\n",
    "    \"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        # Convert row to dictionary\n",
    "        doc = row.to_dict()\n",
    "        \n",
    "        # Convert numpy types to native Python types\n",
    "        for key, value in doc.items():\n",
    "            if isinstance(value, (np.integer, np.floating)):\n",
    "                doc[key] = value.item()\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                doc[key] = value.tolist()\n",
    "        \n",
    "        # Yield document with index name and _id\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc.get('id', idx),  # Use 'id' field if available, otherwise use index\n",
    "            \"_source\": doc\n",
    "        }\n",
    "\n",
    "# Define index name\n",
    "index_name_with_pipeline = \"adventure_works_meta_ai_ready\"\n",
    "\n",
    "# Step 1: Generate mappings with vector fields AND pipeline configuration\n",
    "mappings_with_pipeline = create_opensearch_mappings(\n",
    "    metadata_df_optimized, \n",
    "    create_vectors=True,\n",
    "    pipeline_name=pipeline_name\n",
    ")\n",
    "\n",
    "print(\"Generated OpenSearch mappings (WITH vector fields and pipeline):\")\n",
    "print(json.dumps(mappings_with_pipeline, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Verify the settings include both knn and default_pipeline\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Index settings configuration:\")\n",
    "print(f\"  - index.knn: {mappings_with_pipeline['settings']['index']['knn']}\")\n",
    "print(f\"  - default_pipeline: {mappings_with_pipeline['settings'].get('default_pipeline', 'Not set')}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Step 2: Delete index if it exists\n",
    "if os_client.indices.exists(index=index_name_with_pipeline):\n",
    "    print(f\"\\nDeleting existing index: {index_name_with_pipeline}\")\n",
    "    os_client.indices.delete(index=index_name_with_pipeline)\n",
    "    print(f\"Index deleted successfully\")\n",
    "\n",
    "# Step 3: Create the index with pipeline-enabled mappings\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Creating index: {index_name_with_pipeline}\")\n",
    "response = os_client.indices.create(index=index_name_with_pipeline, body=mappings_with_pipeline)\n",
    "print(f\"Index created successfully: {response}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Step 4: Ingest a SMALL sample (to test embedding generation)\n",
    "# Note: Using only 100 documents for testing because embedding generation is compute-intensive\n",
    "df_small_sample = metadata_df_optimized.head(1000)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting bulk ingestion of {len(df_small_sample)} documents...\")\n",
    "print(\"Note: Using small sample because embedding generation takes time\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use bulk helper - the pipeline will automatically generate embeddings\n",
    "success, failed = helpers.bulk(\n",
    "    os_client,\n",
    "    generate_bulk_data(df_small_sample, index_name_with_pipeline),\n",
    "    chunk_size=5,  # Smaller chunks for embedding generation\n",
    "    request_timeout=120,  # Longer timeout for model inference\n",
    "    raise_on_error=False,\n",
    "    raise_on_exception=False\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Bulk ingestion completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Successfully indexed: {success} documents\")\n",
    "print(f\"Failed: {failed} documents\")\n",
    "print(f\"Average time per document: {elapsed_time/len(df_small_sample):.2f} seconds\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Step 5: Verify ingestion and check embeddings\n",
    "time.sleep(2)  # Wait for refresh\n",
    "os_client.indices.refresh(index=index_name_with_pipeline)\n",
    "count_response = os_client.count(index=index_name_with_pipeline)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total documents in index '{index_name_with_pipeline}': {count_response['count']}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Fetch a document to verify embeddings were generated\n",
    "search_response = os_client.search(\n",
    "    index=index_name_with_pipeline, \n",
    "    body={\"query\": {\"match_all\": {}}, \"size\": 1}\n",
    ")\n",
    "\n",
    "if search_response['hits']['hits']:\n",
    "    doc = search_response['hits']['hits'][0]['_source']\n",
    "    \n",
    "    # Check which embedding fields exist\n",
    "    embedding_fields = [k for k in doc.keys() if k.endswith('_embedding')]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Embedding fields in document:\")\n",
    "    for field in embedding_fields:\n",
    "        embedding = doc[field]\n",
    "        if isinstance(embedding, list):\n",
    "            print(f\"  - {field}: {len(embedding)} dimensions\")\n",
    "            print(f\"    First 5 values: {embedding[:5]}\")\n",
    "        else:\n",
    "            print(f\"  - {field}: {embedding}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nSample document with embeddings:\")\n",
    "    # Show document without full embedding arrays for readability\n",
    "    doc_summary = {k: v if not k.endswith('_embedding') else f\"[{len(v)} dimensions]\" \n",
    "                   for k, v in doc.items()}\n",
    "    print(json.dumps(doc_summary, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92edd59",
   "metadata": {},
   "source": [
    "# Hybrid Search (Keyword + Semantic)\n",
    "\n",
    "Hybrid search combines the best of both worlds:\n",
    "- **Keyword Search (BM25)**: Exact term matching, good for specific queries\n",
    "- **Semantic Search (k-NN)**: Meaning-based matching, good for conceptual queries\n",
    "\n",
    "**Benefits:**\n",
    "- Better recall: Finds documents that keyword search might miss\n",
    "- Better precision: Combines semantic similarity with keyword relevance\n",
    "- Flexible scoring: Can adjust weights between keyword and semantic components\n",
    "\n",
    "**Implementation:**\n",
    "We'll use a `bool` query with `should` clauses to combine both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc3ecd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üîç HYBRID SEARCH EXAMPLE 1: Equal Keyword + Semantic Weights\n",
      "====================================================================================================\n",
      "\n",
      "Query: 'Which table has product details?'\n",
      "Keyword Boost: 1.0, Semantic Boost: 1.0\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Found 25 results\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìÑ Result 1 (Score: 1.3326)\n",
      "   Table Schema: production\n",
      "   Table Name: product\n",
      "   Column Name: productid\n",
      "   Data Type: integer\n",
      "   Column Description: A unique numerical identifier for each product in the product catalog. The values are non-sequential, indicating they are arbitrary surrogate keys....\n",
      "   Table Description: This table stores product master data for a manufacturing business, containing all product definitions and attributes. It tracks finished goods and co...\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 2 (Score: 1.3326)\n",
      "   Table Schema: production\n",
      "   Table Name: product\n",
      "   Column Name: name\n",
      "   Data Type: character varying\n",
      "   Column Description: The descriptive name of the product, which often includes the model, size, color, or specific product type and variant....\n",
      "   Table Description: This table stores product master data for a manufacturing business, containing all product definitions and attributes. It tracks finished goods and co...\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 3 (Score: 1.3326)\n",
      "   Table Schema: production\n",
      "   Table Name: product\n",
      "   Column Name: productnumber\n",
      "   Data Type: character varying\n",
      "   Column Description: A unique alphanumeric identifier for products, often using a prefix and code structure to categorize items like bikes, accessories, and clothing....\n",
      "   Table Description: This table stores product master data for a manufacturing business, containing all product definitions and attributes. It tracks finished goods and co...\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 4 (Score: 1.3326)\n",
      "   Table Schema: production\n",
      "   Table Name: product\n",
      "   Column Name: makeflag\n",
      "   Data Type: boolean\n",
      "   Column Description: Indicates if the product is manufactured in-house (True) or purchased from an external supplier (False)....\n",
      "   Table Description: This table stores product master data for a manufacturing business, containing all product definitions and attributes. It tracks finished goods and co...\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 5 (Score: 1.3326)\n",
      "   Table Schema: production\n",
      "   Table Name: product\n",
      "   Column Name: finishedgoodsflag\n",
      "   Data Type: boolean\n",
      "   Column Description: Specifies if the product is a sellable, finished good (True) or a component used in the manufacturing process (False)....\n",
      "   Table Description: This table stores product master data for a manufacturing business, containing all product definitions and attributes. It tracks finished goods and co...\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search(query_text, index_name, fields_to_search=[\"title\", \"context\", \"question\"], \n",
    "                  k=5, model_id=None, keyword_boost=1.0, semantic_boost=1.0):\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining keyword (BM25) and semantic (k-NN) search.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query_text : str\n",
    "        The search query text\n",
    "    index_name : str\n",
    "        Name of the index to search\n",
    "    fields_to_search : list of str\n",
    "        Fields to search in (both keyword and semantic)\n",
    "        Default includes title for better semantic matching\n",
    "    k : int\n",
    "        Number of top results to return\n",
    "    model_id : str, optional\n",
    "        Model ID for embedding generation\n",
    "    keyword_boost : float\n",
    "        Boost factor for keyword search (default: 1.0)\n",
    "    semantic_boost : float\n",
    "        Boost factor for semantic search (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Search results with combined scores\n",
    "    \"\"\"\n",
    "    # Build keyword queries for each field\n",
    "    keyword_queries = []\n",
    "    for field in fields_to_search:\n",
    "        keyword_queries.append({\n",
    "            \"match\": {\n",
    "                field: {\n",
    "                    \"query\": query_text,\n",
    "                    \"boost\": keyword_boost\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Build semantic queries for each field\n",
    "    semantic_queries = []\n",
    "    for field in fields_to_search:\n",
    "        semantic_queries.append({\n",
    "            \"neural\": {\n",
    "                f\"{field}_embedding\": {\n",
    "                    \"query_text\": query_text,\n",
    "                    \"model_id\": model_id,\n",
    "                    \"k\": k * 2,  # Retrieve more candidates for better results\n",
    "                    \"boost\": semantic_boost\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Combine queries using bool should\n",
    "    search_body = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": keyword_queries + semantic_queries,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        },\n",
    "        # Return all fields except embeddings\n",
    "        \"_source\": {\n",
    "            \"excludes\": [\"*_embedding\"]\n",
    "        },\n",
    "        \"explain\": False  # Set to True to see score calculation details\n",
    "    }\n",
    "    \n",
    "    return os_client.search(index=index_name, body=search_body)\n",
    "\n",
    "\n",
    "# Example 1: Hybrid search with equal weights\n",
    "print(\"=\"*100)\n",
    "print(\"üîç HYBRID SEARCH EXAMPLE 1: Equal Keyword + Semantic Weights\")\n",
    "print(\"=\"*100)\n",
    "query = \"Which table has product details?\"\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Keyword Boost: 1.0, Semantic Boost: 1.0\")\n",
    "\n",
    "results = hybrid_search(\n",
    "    query_text=query,\n",
    "    index_name=\"adventure_works_meta_ai_ready\",\n",
    "    fields_to_search=[\"TABLE_NAME\"],\n",
    "    k=5,\n",
    "    model_id=model_id,\n",
    "    keyword_boost=1.0,\n",
    "    semantic_boost=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\n{'‚îÄ'*100}\")\n",
    "print(f\"Found {results['hits']['total']['value']} results\")\n",
    "print(f\"{'‚îÄ'*100}\")\n",
    "\n",
    "for i, hit in enumerate(results['hits']['hits'], 1):\n",
    "    score = hit['_score']\n",
    "    source = hit['_source']\n",
    "    \n",
    "    print(f\"\\nüìÑ Result {i} (Score: {score:.4f})\")\n",
    "    print(f\"   Table Schema: {source.get('TABLE_SCHEMA', 'N/A')}\")\n",
    "    print(f\"   Table Name: {source.get('TABLE_NAME', 'N/A')}\")\n",
    "    print(f\"   Column Name: {source.get('COLUMN_NAME', 'N/A')}\")\n",
    "    print(f\"   Data Type: {source.get('DATA_TYPE', 'N/A')}\")\n",
    "    print(f\"   Column Description: {source.get('INFERRED_COLUMN_DESCRIPTION', 'N/A')[:150]}...\")\n",
    "    print(f\"   Table Description: {source.get('INFERRED_TABLE_DESCRIPTION', 'N/A')[:150]}...\")\n",
    "    print(f\"   {'-'*96}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340cd044",
   "metadata": {},
   "source": [
    "## ü§ñ AI-Enhanced Metadata with DeepSeek API\n",
    "\n",
    "Generate intelligent column descriptions using DeepSeek API by analyzing sample data from each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85077a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI-Enhanced Metadata Configuration\n",
      "==================================================\n",
      "üîë DeepSeek API Key: ‚úÖ Configured\n",
      "üìä Sample size per column: 10\n",
      "üö´ Excluded schemas: information_schema, pg_catalog\n"
     ]
    }
   ],
   "source": [
    "# Configuration for AI-enhanced metadata\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "SAMPLING_COUNT = 10  # Number of sample values to analyze per column\n",
    "SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA = ['information_schema', 'pg_catalog']  # Skip system schemas\n",
    "\n",
    "print(\"ü§ñ AI-Enhanced Metadata Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üîë DeepSeek API Key: {'‚úÖ Configured' if DEEPSEEK_API_KEY else '‚ùå Missing'}\")\n",
    "print(f\"üìä Sample size per column: {SAMPLING_COUNT}\")\n",
    "print(f\"üö´ Excluded schemas: {', '.join(SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA)}\")\n",
    "\n",
    "# Define DeepSeek API Call Function\n",
    "def call_deepseek_api(sample_values, column_name, table_name, data_type):\n",
    "    \"\"\"\n",
    "    Call DeepSeek API to generate a description for the sampled data\n",
    "    \"\"\"\n",
    "    if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == \"your_deepseek_api_key_here\":\n",
    "        # Return a placeholder description if API key is not set\n",
    "        return f\"AI description needed for {column_name} column\"\n",
    "    \n",
    "    try:\n",
    "        # Prepare the prompt with sample values\n",
    "        sample_str = \", \".join([str(val) for val in sample_values[:10] if val is not None])\n",
    "        \n",
    "        if not sample_str:\n",
    "            return f\"{data_type} column in {table_name}\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on these sample values from the column '{column_name}' (data type: {data_type}) in the table '{table_name}':\n",
    "        {sample_str}\n",
    "        \n",
    "        Please provide a concise description of what this column contains in less than 40 words.\n",
    "        Focus on the type of data, its purpose, and any patterns you observe.\n",
    "        \"\"\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"deepseek-chat\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            description = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return description\n",
    "        else:\n",
    "            print(f\"  ‚ùå API Error {response.status_code}: {response.text}\")\n",
    "            return f\"API error for {column_name}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Exception calling API for {column_name}: {str(e)}\")\n",
    "        return f\"Error generating description for {column_name}\"\n",
    "\n",
    "\n",
    "def get_sample_values_from_db(db_connector, schema, table, column, sample_size=SAMPLING_COUNT):\n",
    "    \"\"\"\n",
    "    Get random sample values from the database for a specific column (PostgreSQL version)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PostgreSQL query to get random sample values\n",
    "        query = f\"\"\"\n",
    "        SELECT \"{column}\"\n",
    "        FROM \"{schema}\".\"{table}\"\n",
    "        WHERE \"{column}\" IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        result_df = db_connector.execute_query(query)\n",
    "        \n",
    "        if result_df is not None and not result_df.empty:\n",
    "            # Return list of values (column name is lowercase in PostgreSQL)\n",
    "            column_lower = column.lower()\n",
    "            if column_lower in result_df.columns:\n",
    "                return result_df[column_lower].tolist()\n",
    "            elif column in result_df.columns:\n",
    "                return result_df[column].tolist()\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Column {column} not found in result\")\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error sampling {schema}.{table}.{column}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def add_inferred_descriptions_to_metadata(metadata_df, db_connector, schemas_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Add INFERRED_COLUMN_DESCRIPTION column to metadata_df by querying the database\n",
    "    and using DeepSeek API to generate descriptions\n",
    "    \n",
    "    Parameters:\n",
    "    - metadata_df: DataFrame with metadata (must have TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE columns)\n",
    "    - db_connector: PostgreSQLConnector instance with active connection\n",
    "    - schemas_to_exclude: List of schema names to skip (won't call DeepSeek API for these)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df with new INFERRED_COLUMN_DESCRIPTION column\n",
    "    \"\"\"\n",
    "    if schemas_to_exclude is None:\n",
    "        schemas_to_exclude = []\n",
    "    \n",
    "    # Filter out excluded schemas\n",
    "    df_to_process = metadata_df[~metadata_df['TABLE_SCHEMA'].isin(schemas_to_exclude)].copy()\n",
    "    df_excluded = metadata_df[metadata_df['TABLE_SCHEMA'].isin(schemas_to_exclude)].copy()\n",
    "    \n",
    "    print(\"üöÄ Starting inference of column descriptions using DeepSeek API...\")\n",
    "    print(f\"üìä Total columns in metadata: {len(metadata_df)}\")\n",
    "    print(f\"üö´ Excluded schemas: {', '.join(schemas_to_exclude) if schemas_to_exclude else 'None'}\")\n",
    "    print(f\"‚öôÔ∏è  Columns to process (after exclusion): {len(df_to_process)}\")\n",
    "    print(f\"‚è≠Ô∏è  Columns skipped: {len(df_excluded)}\")\n",
    "    \n",
    "    # Add new column for inferred descriptions\n",
    "    df_to_process['INFERRED_COLUMN_DESCRIPTION'] = \"\"\n",
    "    df_excluded['INFERRED_COLUMN_DESCRIPTION'] = \"Excluded schema - not processed\"\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df_to_process.iterrows():\n",
    "        schema = row['TABLE_SCHEMA']\n",
    "        table = row['TABLE_NAME']\n",
    "        column = row['COLUMN_NAME']\n",
    "        data_type = row['DATA_TYPE']\n",
    "        \n",
    "        # Get the position in the filtered dataframe\n",
    "        position = list(df_to_process.index).index(idx) + 1\n",
    "        print(f\"  üîç [{position}/{len(df_to_process)}] Processing {schema}.{table}.{column}\")\n",
    "        \n",
    "        # Get sample values from database\n",
    "        sample_values = get_sample_values_from_db(db_connector, schema, table, column, sample_size=SAMPLING_COUNT)\n",
    "        \n",
    "        if sample_values:\n",
    "            print(f\"    üìà Retrieved {len(sample_values)} sample values\")\n",
    "            \n",
    "            # Generate description using DeepSeek API\n",
    "            description = call_deepseek_api(sample_values, column, f\"{schema}.{table}\", data_type)\n",
    "            df_to_process.at[idx, 'INFERRED_COLUMN_DESCRIPTION'] = description\n",
    "            print(f\"    ‚úÖ Generated: {description[:80]}...\")\n",
    "        else:\n",
    "            # Use a basic description if no sample data\n",
    "            df_to_process.at[idx, 'INFERRED_COLUMN_DESCRIPTION'] = f\"{data_type} column in {schema}.{table}\"\n",
    "            print(f\"    ‚ö†Ô∏è  No sample data available, using basic description\")\n",
    "    \n",
    "    # Combine processed and excluded dataframes\n",
    "    result_df = pd.concat([df_to_process, df_excluded], ignore_index=False).sort_index()\n",
    "    \n",
    "    print(f\"\\nüéâ Completed!\")\n",
    "    print(f\"‚úÖ Generated descriptions for {len(df_to_process)} columns\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped {len(df_excluded)} columns from excluded schemas\")\n",
    "    print(f\"üìä Total rows in result: {len(result_df)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def save_enhanced_metadata_to_excel(metadata_df_enhanced, db_connector):\n",
    "    \"\"\"\n",
    "    Save the enhanced metadata with inferred descriptions to a new sheet in the existing Excel file\n",
    "    \n",
    "    Parameters:\n",
    "    - metadata_df_enhanced: DataFrame with INFERRED_DESCRIPTION column\n",
    "    - db_connector: PostgreSQLConnector instance (to get database name)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean indicating success\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use temp directory if current directory has issues\n",
    "        try:\n",
    "            current_dir = os.getcwd()\n",
    "        except FileNotFoundError:\n",
    "            current_dir = tempfile.gettempdir()\n",
    "            \n",
    "        filename = f\"metadata_{db_connector.database}.xlsx\"\n",
    "        filepath = os.path.join(current_dir, filename)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ö†Ô∏è  File {filename} not found. Creating new file...\")\n",
    "            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "                metadata_df_enhanced.to_excel(writer, sheet_name='Metadata_Enhanced', index=False)\n",
    "            print(f\"‚úÖ New file created: {filepath}\")\n",
    "            return True\n",
    "        \n",
    "        # Read existing Excel file\n",
    "        print(f\"üìÇ Opening existing file: {filename}\")\n",
    "        \n",
    "        # Use openpyxl to preserve existing sheets\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            # Add or replace the Metadata_Enhanced sheet\n",
    "            metadata_df_enhanced.to_excel(writer, sheet_name='Metadata_Enhanced', index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Enhanced metadata saved to sheet 'Metadata_Enhanced' in: {filepath}\")\n",
    "        print(f\"üìä File size: {os.path.getsize(filepath)} bytes\")\n",
    "        print(f\"üìã Total rows saved: {len(metadata_df_enhanced)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving enhanced metadata: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0a742",
   "metadata": {},
   "source": [
    "# The below cell will produce metadata again\n",
    "- Don't execute if you already produced meta above and/or don't want further LLM calls costing you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2205daf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING AI-ENHANCED METADATA GENERATION\n",
      "============================================================\n",
      "‚úÖ Found metadata with 456 columns\n",
      "üß™ Testing DeepSeek API connection...\n",
      "üîç API Test Result: This column contains string values representing given names of users, likely for personal identification and addressing purposes.\n",
      "\n",
      "üéØ Starting enhanced metadata generation...\n",
      "üöÄ Starting inference of column descriptions using DeepSeek API...\n",
      "üìä Total columns in metadata: 456\n",
      "üö´ Excluded schemas: information_schema, pg_catalog\n",
      "‚öôÔ∏è  Columns to process (after exclusion): 456\n",
      "‚è≠Ô∏è  Columns skipped: 0\n",
      "  üîç [1/456] Processing humanresources.department.departmentid\n",
      "    üìà Retrieved 10 sample values\n",
      "üîç API Test Result: This column contains string values representing given names of users, likely for personal identification and addressing purposes.\n",
      "\n",
      "üéØ Starting enhanced metadata generation...\n",
      "üöÄ Starting inference of column descriptions using DeepSeek API...\n",
      "üìä Total columns in metadata: 456\n",
      "üö´ Excluded schemas: information_schema, pg_catalog\n",
      "‚öôÔ∏è  Columns to process (after exclusion): 456\n",
      "‚è≠Ô∏è  Columns skipped: 0\n",
      "  üîç [1/456] Processing humanresources.department.departmentid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This integer column contains unique department identifiers ranging from 3-16, li...\n",
      "  üîç [2/456] Processing humanresources.department.name\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This integer column contains unique department identifiers ranging from 3-16, li...\n",
      "  üîç [2/456] Processing humanresources.department.name\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names representing the organizational structure ...\n",
      "  üîç [3/456] Processing humanresources.department.groupname\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names representing the organizational structure ...\n",
      "  üîç [3/456] Processing humanresources.department.groupname\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names that categorize employees into functional ...\n",
      "  üîç [4/456] Processing humanresources.department.modifieddate\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains department names that categorize employees into functional ...\n",
      "  üîç [4/456] Processing humanresources.department.modifieddate\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This timestamp column tracks when department records were last updated, showing ...\n",
      "  üîç [5/456] Processing humanresources.employee.businessentityid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This timestamp column tracks when department records were last updated, showing ...\n",
      "  üîç [5/456] Processing humanresources.employee.businessentityid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique integer identifiers for each employee, serving as a ...\n",
      "  üîç [6/456] Processing humanresources.employee.nationalidnumber\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique integer identifiers for each employee, serving as a ...\n",
      "  üîç [6/456] Processing humanresources.employee.nationalidnumber\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique 9-digit numeric identifiers for employees, likely se...\n",
      "  üîç [7/456] Processing humanresources.employee.loginid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique 9-digit numeric identifiers for employees, likely se...\n",
      "  üîç [7/456] Processing humanresources.employee.loginid\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique usernames for employee login accounts, formatted as ...\n",
      "  üîç [8/456] Processing humanresources.employee.jobtitle\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains unique usernames for employee login accounts, formatted as ...\n",
      "  üîç [8/456] Processing humanresources.employee.jobtitle\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains job titles and roles, primarily for production and manageme...\n",
      "  üîç [9/456] Processing humanresources.employee.birthdate\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains job titles and roles, primarily for production and manageme...\n",
      "  üîç [9/456] Processing humanresources.employee.birthdate\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains employee birth dates, showing ages from mid-20s to late 50s...\n",
      "  üîç [10/456] Processing humanresources.employee.maritalstatus\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This column contains employee birth dates, showing ages from mid-20s to late 50s...\n",
      "  üîç [10/456] Processing humanresources.employee.maritalstatus\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This categorical column tracks marital status, containing 'M' (Married) and 'S' ...\n",
      "  üîç [11/456] Processing humanresources.employee.gender\n",
      "    üìà Retrieved 10 sample values\n",
      "    ‚úÖ Generated: This categorical column tracks marital status, containing 'M' (Married) and 'S' ...\n",
      "  üîç [11/456] Processing humanresources.employee.gender\n",
      "    üìà Retrieved 10 sample values\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ Starting enhanced metadata generation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Generate inferred descriptions with schema exclusions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m metadata_df_enhanced = \u001b[43madd_inferred_descriptions_to_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_connector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschemas_to_exclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Display sample results\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìã Sample results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36madd_inferred_descriptions_to_metadata\u001b[39m\u001b[34m(metadata_df, db_connector, schemas_to_exclude)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    üìà Retrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sample values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Generate description using DeepSeek API\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m description = \u001b[43mcall_deepseek_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m df_to_process.at[idx, \u001b[33m'\u001b[39m\u001b[33mINFERRED_COLUMN_DESCRIPTION\u001b[39m\u001b[33m'\u001b[39m] = description\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    ‚úÖ Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription[:\u001b[32m80\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mcall_deepseek_api\u001b[39m\u001b[34m(sample_values, column_name, table_name, data_type)\u001b[39m\n\u001b[32m     37\u001b[39m headers = {\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEEPSEEK_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m }\n\u001b[32m     42\u001b[39m data = {\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeepseek-chat\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m\n\u001b[32m     52\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEEPSEEK_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     57\u001b[39m     result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_sql_visualization_insights_agent/.venv/lib/python3.11/site-packages/urllib3/response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.5-linux-x86_64-gnu/lib/python3.11/ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execute the AI-enhanced metadata workflow\n",
    "print(\"üöÄ STARTING AI-ENHANCED METADATA GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have the required data\n",
    "if 'metadata_df' in locals() and metadata_df is not None:\n",
    "    print(f\"‚úÖ Found metadata with {len(metadata_df)} columns\")\n",
    "    \n",
    "    # Test API connection first\n",
    "    if DEEPSEEK_API_KEY:\n",
    "        print(\"üß™ Testing DeepSeek API connection...\")\n",
    "        test_description = call_deepseek_api(\n",
    "            [\"John\", \"Jane\", \"Michael\"], \n",
    "            \"first_name\", \n",
    "            \"users\", \n",
    "            \"varchar\"\n",
    "        )\n",
    "        print(f\"üîç API Test Result: {test_description}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No DeepSeek API key found. Will use placeholder descriptions.\")\n",
    "    \n",
    "    print(\"\\nüéØ Starting enhanced metadata generation...\")\n",
    "    \n",
    "    # Generate inferred descriptions with schema exclusions\n",
    "    metadata_df_enhanced = add_inferred_descriptions_to_metadata(\n",
    "        metadata_df, \n",
    "        db_connector,\n",
    "        schemas_to_exclude=SCHEMAS_TO_EXCLUDE_FOR_ENHANCED_METADATA\n",
    "    )\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nüìã Sample results:\")\n",
    "    sample_columns = ['TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'INFERRED_COLUMN_DESCRIPTION']\n",
    "    display(metadata_df_enhanced[sample_columns].head(10))\n",
    "    \n",
    "    # Save to Excel\n",
    "    print(\"\\nüíæ Saving enhanced metadata to Excel...\")\n",
    "    if save_enhanced_metadata_to_excel(metadata_df_enhanced, db_connector):\n",
    "        print(\"\\nüéâ SUCCESS! Enhanced metadata with AI-generated descriptions has been saved.\")\n",
    "        print(f\"üìÅ Check the 'Metadata_Enhanced' sheet in the Excel file\")\n",
    "        \n",
    "        # Show statistics\n",
    "        total_columns = len(metadata_df_enhanced)\n",
    "        ai_generated = len(metadata_df_enhanced[\n",
    "            ~metadata_df_enhanced['INFERRED_COLUMN_DESCRIPTION'].str.contains('Excluded schema', na=False)\n",
    "        ])\n",
    "        excluded_count = total_columns - ai_generated\n",
    "        \n",
    "        print(f\"\\nüìä Final Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Total columns processed: {total_columns}\")\n",
    "        print(f\"   ‚Ä¢ AI descriptions generated: {ai_generated}\")\n",
    "        print(f\"   ‚Ä¢ Excluded (system schemas): {excluded_count}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to save enhanced metadata.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  metadata_df not available. Please run the metadata extraction cells first.\")\n",
    "    print(\"üí° Go back and execute the metadata extraction steps.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-to-bi-insights (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
