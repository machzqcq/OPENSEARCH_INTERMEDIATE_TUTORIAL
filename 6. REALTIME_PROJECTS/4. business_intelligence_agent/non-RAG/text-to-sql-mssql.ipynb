{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d40fd10",
   "metadata": {},
   "source": [
    "# üöÄ MSSQL Text-to-SQL with Multi-Provider LLM Integration\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive text-to-SQL solution that:\n",
    "\n",
    "1. **Connects to MSSQL Database** - Establishes secure connection to SQL Server using pymssql\n",
    "2. **Extracts Database Metadata** - Retrieves table/column information and saves to Excel\n",
    "3. **Multi-Provider LLM Support** - OpenAI, Anthropic, Google Gemini, DeepSeek for natural language processing\n",
    "4. **Direct SQL Execution** - Runs generated queries directly on the MSSQL database\n",
    "5. **Interactive Interface** - User-friendly widgets for query input and execution\n",
    "\n",
    "### üîß Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "- **MSSQL Server** access with proper credentials\n",
    "- **API Keys** for at least one LLM provider\n",
    "- **Python packages**: pandas, sqlalchemy, pymssql, openpyxl, ipywidgets, requests, python-dotenv\n",
    "\n",
    "### üîë Environment Variables\n",
    "\n",
    "Set up your API keys and database connection:\n",
    "\n",
    "```env\n",
    "# Database Connection\n",
    "MSSQL_SERVER=your_server_name\n",
    "MSSQL_DATABASE=your_database_name\n",
    "MSSQL_USERNAME=your_username\n",
    "MSSQL_PASSWORD=your_password\n",
    "MSSQL_PORT=1433\n",
    "\n",
    "# LLM API Keys (at least one required)\n",
    "OPENAI_API_KEY=your_openai_key\n",
    "ANTHROPIC_API_KEY=your_anthropic_key\n",
    "GOOGLE_API_KEY=your_google_key\n",
    "DEEPSEEK_API_KEY=your_deepseek_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01beb5ed",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b84a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìä Pandas version: 2.3.1\n",
      "üóÑÔ∏è SQLAlchemy version: 2.0.42\n",
      "üîå pymssql version: 2.3.8\n",
      "‚úÖ Required database environment variables found\n",
      "‚úÖ OpenAI API key found\n",
      "‚úÖ Anthropic API key found\n",
      "‚úÖ Google API key found\n",
      "‚úÖ DeepSeek API key found\n",
      "\n",
      "ü§ñ Available LLM providers: OpenAI, Anthropic, Google, DeepSeek\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for data manipulation and database connectivity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connectivity\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text, MetaData, inspect\n",
    "import pymssql\n",
    "import urllib.parse\n",
    "\n",
    "# Excel file handling\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Environment variables and configuration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM API calls\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Set up plotting styles\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üóÑÔ∏è SQLAlchemy version: {sqlalchemy.__version__}\")\n",
    "print(f\"üîå pymssql version: {pymssql.__version__}\")\n",
    "\n",
    "# Check for required environment variables\n",
    "required_env_vars = ['MSSQL_SERVER', 'MSSQL_DATABASE']\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing required environment variables: {missing_vars}\")\n",
    "    print(\"Please set these in your .env file or environment\")\n",
    "else:\n",
    "    print(\"‚úÖ Required database environment variables found\")\n",
    "\n",
    "# Check for LLM API keys\n",
    "llm_providers = {\n",
    "    'OpenAI': 'OPENAI_API_KEY',\n",
    "    'Anthropic': 'ANTHROPIC_API_KEY', \n",
    "    'Google': 'GOOGLE_API_KEY',\n",
    "    'DeepSeek': 'DEEPSEEK_API_KEY'\n",
    "}\n",
    "\n",
    "available_providers = []\n",
    "for provider, env_var in llm_providers.items():\n",
    "    if os.getenv(env_var):\n",
    "        available_providers.append(provider)\n",
    "        print(f\"‚úÖ {provider} API key found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {provider} API key missing ({env_var})\")\n",
    "\n",
    "if not available_providers:\n",
    "    print(\"\\n‚ö†Ô∏è  No LLM API keys found! Please set at least one API key.\")\n",
    "else:\n",
    "    print(f\"\\nü§ñ Available LLM providers: {', '.join(available_providers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab8c93",
   "metadata": {},
   "source": [
    "## 2. Connect to MSSQL Database\n",
    "\n",
    "This section establishes a connection to the MSSQL Server database using SQLAlchemy with the pymssql driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to MSSQL database!\n",
      "üìä Server: localhost:1434\n",
      "üóÑÔ∏è Database: AdventureWorks2019\n",
      "\n",
      "üöÄ Database connector ready for use!\n"
     ]
    }
   ],
   "source": [
    "class MSSQLConnector:\n",
    "    \"\"\"\n",
    "    MSSQL Database Connector using SQLAlchemy and pymssql\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.server = os.getenv('MSSQL_SERVER')\n",
    "        self.database = os.getenv('MSSQL_DATABASE')\n",
    "        self.username = os.getenv('MSSQL_USERNAME')\n",
    "        self.password = os.getenv('MSSQL_PASSWORD')\n",
    "        self.port = int(os.getenv('MSSQL_PORT', os.getenv('MSSQL_PORT')))\n",
    "        self.use_windows_auth = os.getenv('MSSQL_USE_WINDOWS_AUTH', 'false').lower() == 'true'\n",
    "        \n",
    "        self.engine = None\n",
    "        self.connection_string = None\n",
    "        \n",
    "    def create_connection_string(self):\n",
    "        \"\"\"Create the connection string for MSSQL using pymssql\"\"\"\n",
    "        if self.use_windows_auth:\n",
    "            # Windows Authentication - pymssql doesn't support this directly\n",
    "            # You would need to use SSPI/Kerberos which is complex\n",
    "            raise ValueError(\n",
    "                \"Windows Authentication is not supported with pymssql. \"\n",
    "                \"Please use SQL Server authentication with username and password.\"\n",
    "            )\n",
    "        else:\n",
    "            # SQL Server Authentication\n",
    "            if not self.username or not self.password:\n",
    "                raise ValueError(\"Username and password required for SQL Server authentication\")\n",
    "            \n",
    "            # URL encode the password to handle special characters\n",
    "            encoded_password = urllib.parse.quote_plus(self.password)\n",
    "            encoded_username = urllib.parse.quote_plus(self.username)\n",
    "            \n",
    "            # pymssql connection string format\n",
    "            self.connection_string = (\n",
    "                f\"mssql+pymssql://{encoded_username}:{encoded_password}@\"\n",
    "                f\"{self.server}:{self.port}/{self.database}\"\n",
    "            )\n",
    "        \n",
    "        return self.connection_string\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection\"\"\"\n",
    "        try:\n",
    "            if not self.connection_string:\n",
    "                self.create_connection_string()\n",
    "            \n",
    "            self.engine = create_engine(self.connection_string)\n",
    "            \n",
    "            # Test the connection\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT 1 as test\")).fetchone()\n",
    "                if result[0] == 1:\n",
    "                    print(\"‚úÖ Successfully connected to MSSQL database!\")\n",
    "                    print(f\"üìä Server: {self.server}:{self.port}\")\n",
    "                    print(f\"üóÑÔ∏è Database: {self.database}\")\n",
    "                    return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to connect to database: {str(e)}\")\n",
    "            print(\"\\nüí° Troubleshooting tips:\")\n",
    "            print(\"1. Check if SQL Server is running\")\n",
    "            print(\"2. Verify server name and database name\")\n",
    "            print(\"3. Check credentials (username and password)\")\n",
    "            print(\"4. Ensure SQL Server is configured to allow TCP/IP connections\")\n",
    "            print(\"5. Check firewall settings and network connectivity\")\n",
    "            print(\"6. Verify the port number (default: 1433)\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Execute a SQL query and return results as DataFrame\"\"\"\n",
    "        try:\n",
    "            if not self.engine:\n",
    "                print(\"‚ùå No database connection. Call connect() first.\")\n",
    "                return None\n",
    "            \n",
    "            if params:\n",
    "                df = pd.read_sql_query(text(query), self.engine, params=params)\n",
    "            else:\n",
    "                df = pd.read_sql_query(text(query), self.engine)\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error executing query: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_tables(self):\n",
    "        \"\"\"Get list of all tables in the database\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            TABLE_SCHEMA,\n",
    "            TABLE_NAME,\n",
    "            TABLE_TYPE\n",
    "        FROM INFORMATION_SCHEMA.TABLES\n",
    "        WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "        ORDER BY TABLE_SCHEMA, TABLE_NAME\n",
    "        \"\"\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_columns(self, schema_name=None, table_name=None):\n",
    "        \"\"\"Get column information for tables\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            TABLE_SCHEMA,\n",
    "            TABLE_NAME,\n",
    "            COLUMN_NAME,\n",
    "            DATA_TYPE,\n",
    "            CHARACTER_MAXIMUM_LENGTH,\n",
    "            IS_NULLABLE,\n",
    "            COLUMN_DEFAULT,\n",
    "            ORDINAL_POSITION\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        \"\"\"\n",
    "        \n",
    "        conditions = []\n",
    "        params = {}\n",
    "        \n",
    "        if schema_name:\n",
    "            conditions.append(\"TABLE_SCHEMA = :schema_name\")\n",
    "            params['schema_name'] = schema_name\n",
    "            \n",
    "        if table_name:\n",
    "            conditions.append(\"TABLE_NAME = :table_name\")\n",
    "            params['table_name'] = table_name\n",
    "        \n",
    "        if conditions:\n",
    "            query += \" WHERE \" + \" AND \".join(conditions)\n",
    "        \n",
    "        query += \" ORDER BY TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION\"\n",
    "        \n",
    "        return self.execute_query(query, params if params else None)\n",
    "\n",
    "# Initialize the database connector\n",
    "db_connector = MSSQLConnector()\n",
    "\n",
    "# Test the connection\n",
    "if db_connector.connect():\n",
    "    print(\"\\nüöÄ Database connector ready for use!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Database connection failed. Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f0957",
   "metadata": {},
   "source": [
    "## 3. Extract Database Metadata and Save to Excel\n",
    "\n",
    "This section extracts comprehensive metadata from the MSSQL database including tables, columns, data types, and descriptions, then saves it to an Excel file for use in LLM prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8377c22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting database metadata...\n",
      "‚úÖ Successfully extracted metadata for 648 columns\n",
      "üìä Found 71 tables\n",
      "üè∑Ô∏è Schemas: dbo, HumanResources, Person, Production, Purchasing, Sales\n",
      "\n",
      "üìã Sample metadata:\n",
      "‚úÖ Successfully extracted metadata for 648 columns\n",
      "üìä Found 71 tables\n",
      "üè∑Ô∏è Schemas: dbo, HumanResources, Person, Production, Purchasing, Sales\n",
      "\n",
      "üìã Sample metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_SCHEMA</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>CHARACTER_MAXIMUM_LENGTH</th>\n",
       "      <th>NUMERIC_PRECISION</th>\n",
       "      <th>NUMERIC_SCALE</th>\n",
       "      <th>IS_NULLABLE</th>\n",
       "      <th>COLUMN_DEFAULT</th>\n",
       "      <th>ORDINAL_POSITION</th>\n",
       "      <th>COLUMN_DESCRIPTION</th>\n",
       "      <th>TABLE_TYPE</th>\n",
       "      <th>FULL_DATA_TYPE</th>\n",
       "      <th>INFERRED_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dbo</td>\n",
       "      <td>AWBuildVersion</td>\n",
       "      <td>SystemInformationID</td>\n",
       "      <td>tinyint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Primary key for AWBuildVersion records.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>tinyint</td>\n",
       "      <td>b'Primary key for AWBuildVersion records.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbo</td>\n",
       "      <td>AWBuildVersion</td>\n",
       "      <td>SystemInformationID</td>\n",
       "      <td>tinyint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Clustered index created by a primary key con...</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>tinyint</td>\n",
       "      <td>b'Clustered index created by a primary key con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbo</td>\n",
       "      <td>AWBuildVersion</td>\n",
       "      <td>Database Version</td>\n",
       "      <td>nvarchar</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>b'Version number of the database in 9.yy.mm.dd...</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>nvarchar(25)</td>\n",
       "      <td>b'Version number of the database in 9.yy.mm.dd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbo</td>\n",
       "      <td>AWBuildVersion</td>\n",
       "      <td>VersionDate</td>\n",
       "      <td>datetime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>b'Date and time the record was last updated.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>datetime</td>\n",
       "      <td>b'Date and time the record was last updated.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbo</td>\n",
       "      <td>AWBuildVersion</td>\n",
       "      <td>ModifiedDate</td>\n",
       "      <td>datetime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>(getdate())</td>\n",
       "      <td>4</td>\n",
       "      <td>b'Date and time the record was last updated.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>datetime</td>\n",
       "      <td>b'Date and time the record was last updated.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dbo</td>\n",
       "      <td>DatabaseLog</td>\n",
       "      <td>DatabaseLogID</td>\n",
       "      <td>int</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Primary key for DatabaseLog records.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>int</td>\n",
       "      <td>b'Primary key for DatabaseLog records.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dbo</td>\n",
       "      <td>DatabaseLog</td>\n",
       "      <td>PostTime</td>\n",
       "      <td>datetime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>b'The date and time the DDL change occurred.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>datetime</td>\n",
       "      <td>b'The date and time the DDL change occurred.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dbo</td>\n",
       "      <td>DatabaseLog</td>\n",
       "      <td>PostTime</td>\n",
       "      <td>datetime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>b'Nonclustered index created by a primary key ...</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>datetime</td>\n",
       "      <td>b'Nonclustered index created by a primary key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dbo</td>\n",
       "      <td>DatabaseLog</td>\n",
       "      <td>DatabaseUser</td>\n",
       "      <td>nvarchar</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>b'The user who implemented the DDL change.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>nvarchar(128)</td>\n",
       "      <td>b'The user who implemented the DDL change.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dbo</td>\n",
       "      <td>DatabaseLog</td>\n",
       "      <td>Event</td>\n",
       "      <td>nvarchar</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>b'The type of DDL statement that was executed.'</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>nvarchar(128)</td>\n",
       "      <td>b'The type of DDL statement that was executed.'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TABLE_SCHEMA      TABLE_NAME          COLUMN_NAME DATA_TYPE  \\\n",
       "0          dbo  AWBuildVersion  SystemInformationID   tinyint   \n",
       "1          dbo  AWBuildVersion  SystemInformationID   tinyint   \n",
       "2          dbo  AWBuildVersion     Database Version  nvarchar   \n",
       "3          dbo  AWBuildVersion          VersionDate  datetime   \n",
       "4          dbo  AWBuildVersion         ModifiedDate  datetime   \n",
       "5          dbo     DatabaseLog        DatabaseLogID       int   \n",
       "6          dbo     DatabaseLog             PostTime  datetime   \n",
       "7          dbo     DatabaseLog             PostTime  datetime   \n",
       "8          dbo     DatabaseLog         DatabaseUser  nvarchar   \n",
       "9          dbo     DatabaseLog                Event  nvarchar   \n",
       "\n",
       "   CHARACTER_MAXIMUM_LENGTH  NUMERIC_PRECISION  NUMERIC_SCALE IS_NULLABLE  \\\n",
       "0                       NaN                3.0            0.0          NO   \n",
       "1                       NaN                3.0            0.0          NO   \n",
       "2                      25.0                NaN            NaN          NO   \n",
       "3                       NaN                NaN            NaN          NO   \n",
       "4                       NaN                NaN            NaN          NO   \n",
       "5                       NaN               10.0            0.0          NO   \n",
       "6                       NaN                NaN            NaN          NO   \n",
       "7                       NaN                NaN            NaN          NO   \n",
       "8                     128.0                NaN            NaN          NO   \n",
       "9                     128.0                NaN            NaN          NO   \n",
       "\n",
       "  COLUMN_DEFAULT  ORDINAL_POSITION  \\\n",
       "0           None                 1   \n",
       "1           None                 1   \n",
       "2           None                 2   \n",
       "3           None                 3   \n",
       "4    (getdate())                 4   \n",
       "5           None                 1   \n",
       "6           None                 2   \n",
       "7           None                 2   \n",
       "8           None                 3   \n",
       "9           None                 4   \n",
       "\n",
       "                                  COLUMN_DESCRIPTION  TABLE_TYPE  \\\n",
       "0         b'Primary key for AWBuildVersion records.'  BASE TABLE   \n",
       "1  b'Clustered index created by a primary key con...  BASE TABLE   \n",
       "2  b'Version number of the database in 9.yy.mm.dd...  BASE TABLE   \n",
       "3      b'Date and time the record was last updated.'  BASE TABLE   \n",
       "4      b'Date and time the record was last updated.'  BASE TABLE   \n",
       "5            b'Primary key for DatabaseLog records.'  BASE TABLE   \n",
       "6      b'The date and time the DDL change occurred.'  BASE TABLE   \n",
       "7  b'Nonclustered index created by a primary key ...  BASE TABLE   \n",
       "8        b'The user who implemented the DDL change.'  BASE TABLE   \n",
       "9    b'The type of DDL statement that was executed.'  BASE TABLE   \n",
       "\n",
       "  FULL_DATA_TYPE                               INFERRED_DESCRIPTION  \n",
       "0        tinyint         b'Primary key for AWBuildVersion records.'  \n",
       "1        tinyint  b'Clustered index created by a primary key con...  \n",
       "2   nvarchar(25)  b'Version number of the database in 9.yy.mm.dd...  \n",
       "3       datetime      b'Date and time the record was last updated.'  \n",
       "4       datetime      b'Date and time the record was last updated.'  \n",
       "5            int            b'Primary key for DatabaseLog records.'  \n",
       "6       datetime      b'The date and time the DDL change occurred.'  \n",
       "7       datetime  b'Nonclustered index created by a primary key ...  \n",
       "8  nvarchar(128)        b'The user who implemented the DDL change.'  \n",
       "9  nvarchar(128)    b'The type of DDL statement that was executed.'  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata saved to: /home/ubuntu/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_bi/metadata_AdventureWorks2019.xlsx\n",
      "üìä File size: 53214 bytes\n",
      "\n",
      "üíæ Metadata successfully saved to Excel file!\n"
     ]
    }
   ],
   "source": [
    "class DatabaseMetadataExtractor:\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from MSSQL database\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connector):\n",
    "        self.db_connector = db_connector\n",
    "        self.metadata_df = None\n",
    "        \n",
    "    def extract_full_metadata(self):\n",
    "        \"\"\"Extract comprehensive metadata including extended properties\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            t.TABLE_SCHEMA,\n",
    "            t.TABLE_NAME,\n",
    "            c.COLUMN_NAME,\n",
    "            c.DATA_TYPE,\n",
    "            c.CHARACTER_MAXIMUM_LENGTH,\n",
    "            c.NUMERIC_PRECISION,\n",
    "            c.NUMERIC_SCALE,\n",
    "            c.IS_NULLABLE,\n",
    "            c.COLUMN_DEFAULT,\n",
    "            c.ORDINAL_POSITION,\n",
    "            -- Try to get column descriptions from extended properties\n",
    "            ISNULL(ep.value, '') as COLUMN_DESCRIPTION,\n",
    "            -- Additional table information\n",
    "            t.TABLE_TYPE,\n",
    "            -- Create a readable data type description\n",
    "            CASE \n",
    "                WHEN c.DATA_TYPE IN ('varchar', 'char', 'nvarchar', 'nchar') \n",
    "                    THEN c.DATA_TYPE + '(' + CAST(c.CHARACTER_MAXIMUM_LENGTH as varchar(10)) + ')'\n",
    "                WHEN c.DATA_TYPE IN ('decimal', 'numeric') \n",
    "                    THEN c.DATA_TYPE + '(' + CAST(c.NUMERIC_PRECISION as varchar(10)) + ',' + CAST(c.NUMERIC_SCALE as varchar(10)) + ')'\n",
    "                ELSE c.DATA_TYPE\n",
    "            END as FULL_DATA_TYPE\n",
    "        FROM INFORMATION_SCHEMA.TABLES t\n",
    "        INNER JOIN INFORMATION_SCHEMA.COLUMNS c \n",
    "            ON t.TABLE_SCHEMA = c.TABLE_SCHEMA \n",
    "            AND t.TABLE_NAME = c.TABLE_NAME\n",
    "        LEFT JOIN sys.tables st \n",
    "            ON st.name = t.TABLE_NAME \n",
    "            AND st.schema_id = SCHEMA_ID(t.TABLE_SCHEMA)\n",
    "        LEFT JOIN sys.columns sc \n",
    "            ON sc.object_id = st.object_id \n",
    "            AND sc.name = c.COLUMN_NAME\n",
    "        LEFT JOIN sys.extended_properties ep \n",
    "            ON ep.major_id = sc.object_id \n",
    "            AND ep.minor_id = sc.column_id \n",
    "            AND ep.name = 'MS_Description'\n",
    "        WHERE t.TABLE_TYPE = 'BASE TABLE'\n",
    "        ORDER BY t.TABLE_SCHEMA, t.TABLE_NAME, c.ORDINAL_POSITION\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîç Extracting database metadata...\")\n",
    "        self.metadata_df = self.db_connector.execute_query(query)\n",
    "        \n",
    "        if self.metadata_df is not None:\n",
    "            print(f\"‚úÖ Successfully extracted metadata for {len(self.metadata_df)} columns\")\n",
    "            print(f\"üìä Found {self.metadata_df['TABLE_NAME'].nunique()} tables\")\n",
    "            print(f\"üè∑Ô∏è Schemas: {', '.join(self.metadata_df['TABLE_SCHEMA'].unique())}\")\n",
    "            \n",
    "            # Add inferred descriptions for columns without descriptions\n",
    "            self.metadata_df['INFERRED_DESCRIPTION'] = self.metadata_df.apply(\n",
    "                self._infer_column_description, axis=1\n",
    "            )\n",
    "            \n",
    "            return self.metadata_df\n",
    "        else:\n",
    "            print(\"‚ùå Failed to extract metadata\")\n",
    "            return None\n",
    "    \n",
    "    def _infer_column_description(self, row):\n",
    "        \"\"\"Infer description based on column name and data type\"\"\"\n",
    "        column_name = row['COLUMN_NAME'].lower()\n",
    "        data_type = row['DATA_TYPE'].lower()\n",
    "        \n",
    "        # Use existing description if available\n",
    "        if row['COLUMN_DESCRIPTION'] and row['COLUMN_DESCRIPTION'].strip():\n",
    "            return row['COLUMN_DESCRIPTION']\n",
    "        \n",
    "        # Common patterns for column descriptions\n",
    "        if 'id' in column_name:\n",
    "            if column_name.endswith('_id') or column_name == 'id':\n",
    "                return \"Unique identifier\"\n",
    "            else:\n",
    "                return f\"Identifier for {column_name.replace('_id', '').replace('id', '')}\"\n",
    "        \n",
    "        if 'name' in column_name:\n",
    "            return f\"Name of the {column_name.replace('_name', '').replace('name', '')}\"\n",
    "        \n",
    "        if 'date' in column_name or 'time' in column_name:\n",
    "            return f\"Date/time value for {column_name}\"\n",
    "        \n",
    "        if 'email' in column_name:\n",
    "            return \"Email address\"\n",
    "        \n",
    "        if 'phone' in column_name:\n",
    "            return \"Phone number\"\n",
    "        \n",
    "        if 'address' in column_name:\n",
    "            return \"Address information\"\n",
    "        \n",
    "        if 'status' in column_name:\n",
    "            return \"Status indicator\"\n",
    "        \n",
    "        if 'flag' in column_name:\n",
    "            return \"Boolean flag\"\n",
    "        \n",
    "        if 'count' in column_name or 'qty' in column_name:\n",
    "            return \"Quantity or count value\"\n",
    "        \n",
    "        if 'amount' in column_name or 'price' in column_name:\n",
    "            return \"Monetary amount\"\n",
    "        \n",
    "        # Default based on data type\n",
    "        if data_type in ['varchar', 'nvarchar', 'char', 'nchar', 'text']:\n",
    "            return f\"Text field: {column_name}\"\n",
    "        elif data_type in ['int', 'bigint', 'smallint', 'tinyint']:\n",
    "            return f\"Integer value: {column_name}\"\n",
    "        elif data_type in ['decimal', 'numeric', 'float', 'real', 'money']:\n",
    "            return f\"Numeric value: {column_name}\"\n",
    "        elif data_type in ['datetime', 'datetime2', 'date', 'time']:\n",
    "            return f\"Date/time value: {column_name}\"\n",
    "        elif data_type in ['bit']:\n",
    "            return f\"Boolean value: {column_name}\"\n",
    "        else:\n",
    "            return f\"Data field: {column_name}\"\n",
    "    \n",
    "    def save_to_excel(self, filename=\"database_metadata.xlsx\"):\n",
    "        \"\"\"Save metadata to Excel file\"\"\"\n",
    "        if self.metadata_df is None:\n",
    "            print(\"‚ùå No metadata to save. Run extract_full_metadata() first.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            filepath = os.path.join(os.getcwd(), filename)\n",
    "            \n",
    "            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "                # Main metadata sheet\n",
    "                self.metadata_df.to_excel(writer, sheet_name='Metadata', index=False)\n",
    "                \n",
    "                # Summary sheet\n",
    "                summary_data = {\n",
    "                    'Metric': [\n",
    "                        'Total Tables',\n",
    "                        'Total Columns', \n",
    "                        'Schemas',\n",
    "                        'Data Types Used',\n",
    "                        'Tables with Descriptions',\n",
    "                        'Columns with Descriptions'\n",
    "                    ],\n",
    "                    'Value': [\n",
    "                        self.metadata_df['TABLE_NAME'].nunique(),\n",
    "                        len(self.metadata_df),\n",
    "                        ', '.join(self.metadata_df['TABLE_SCHEMA'].unique()),\n",
    "                        ', '.join(self.metadata_df['DATA_TYPE'].unique()),\n",
    "                        len(self.metadata_df[self.metadata_df['COLUMN_DESCRIPTION'].notna()]['TABLE_NAME'].unique()),\n",
    "                        len(self.metadata_df[self.metadata_df['COLUMN_DESCRIPTION'].notna()])\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "                \n",
    "                # Table list sheet\n",
    "                table_summary = self.metadata_df.groupby(['TABLE_SCHEMA', 'TABLE_NAME']).agg({\n",
    "                    'COLUMN_NAME': 'count',\n",
    "                    'COLUMN_DESCRIPTION': lambda x: x.notna().sum()\n",
    "                }).rename(columns={\n",
    "                    'COLUMN_NAME': 'Column_Count',\n",
    "                    'COLUMN_DESCRIPTION': 'Described_Columns'\n",
    "                }).reset_index()\n",
    "                \n",
    "                table_summary.to_excel(writer, sheet_name='Tables', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Metadata saved to: {filepath}\")\n",
    "            print(f\"üìä File size: {os.path.getsize(filepath)} bytes\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving to Excel: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Initialize metadata extractor\n",
    "metadata_extractor = DatabaseMetadataExtractor(db_connector)\n",
    "\n",
    "# Extract metadata if database connection is available\n",
    "if db_connector.engine:\n",
    "    metadata_df = metadata_extractor.extract_full_metadata()\n",
    "    \n",
    "    if metadata_df is not None:\n",
    "        print(\"\\nüìã Sample metadata:\")\n",
    "        display(metadata_df.head(10))\n",
    "        \n",
    "        # Save to Excel\n",
    "        if metadata_extractor.save_to_excel(f\"metadata_{db_connector.database}.xlsx\"):\n",
    "            print(\"\\nüíæ Metadata successfully saved to Excel file!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to extract metadata\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No database connection available. Skipping metadata extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf63863",
   "metadata": {},
   "source": [
    "## 4. Load Metadata into DataFrame\n",
    "\n",
    "Load the saved Excel metadata file into a pandas DataFrame for use in LLM prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60f7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Metadata file not found: /home/ubuntu/git-projects/personal/github.com/elasticsearch_opensearch/opensearch/my_tutorial/scripts/5. REALTIME_PROJECTS/4. text_to_bi/mssql_metadata.xlsx\n",
      "üí° Run the metadata extraction section first to create the file.\n",
      "\n",
      "üîÑ Using recently extracted metadata instead...\n",
      "‚úÖ Using in-memory metadata: 648 rows\n",
      "\n",
      "üöÄ Metadata ready for LLM queries!\n",
      "üìù Use 'df_meta' variable for text-to-SQL generation\n"
     ]
    }
   ],
   "source": [
    "def load_metadata_from_excel(filename=f\"metadata_{db_connector.database}.xlsx\"):\n",
    "    \"\"\"\n",
    "    Load metadata from Excel file into DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filepath = os.path.join(os.getcwd(), filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ùå Metadata file not found: {filepath}\")\n",
    "            print(\"üí° Run the metadata extraction section first to create the file.\")\n",
    "            return None\n",
    "        \n",
    "        # Load the main metadata sheet\n",
    "        df_meta = pd.read_excel(filepath, sheet_name='Metadata')\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded metadata from: {filepath}\")\n",
    "        print(f\"üìä Loaded {len(df_meta)} rows of metadata\")\n",
    "        print(f\"üè∑Ô∏è Tables: {df_meta['TABLE_NAME'].nunique()}\")\n",
    "        print(f\"üìã Schemas: {', '.join(df_meta['TABLE_SCHEMA'].unique())}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nüìã Sample metadata structure:\")\n",
    "        display(df_meta.head())\n",
    "        \n",
    "        # Show data types distribution\n",
    "        print(f\"\\nüìä Data types in database:\")\n",
    "        data_type_counts = df_meta['DATA_TYPE'].value_counts()\n",
    "        for dtype, count in data_type_counts.head(10).items():\n",
    "            print(f\"  {dtype}: {count} columns\")\n",
    "        \n",
    "        return df_meta\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading metadata: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load metadata into df_meta variable\n",
    "df_meta = load_metadata_from_excel(\"mssql_metadata.xlsx\")\n",
    "\n",
    "# If loading from file fails, try to use the extracted metadata\n",
    "if df_meta is None and 'metadata_df' in locals() and metadata_df is not None:\n",
    "    print(\"\\nüîÑ Using recently extracted metadata instead...\")\n",
    "    df_meta = metadata_df.copy()\n",
    "    print(f\"‚úÖ Using in-memory metadata: {len(df_meta)} rows\")\n",
    "\n",
    "if df_meta is not None:\n",
    "    # Prepare metadata for LLM prompts by creating a simplified format\n",
    "    # Create a table-focused view for better LLM understanding\n",
    "    df_meta['TABLE_FULL_NAME'] = df_meta['TABLE_SCHEMA'] + '.' + df_meta['TABLE_NAME']\n",
    "    df_meta['COLUMN_INFO'] = (\n",
    "        df_meta['COLUMN_NAME'] + ' (' + \n",
    "        df_meta['FULL_DATA_TYPE'] + \n",
    "        ', ' + df_meta['IS_NULLABLE'].map({'YES': 'nullable', 'NO': 'not null'}) + ')'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüöÄ Metadata ready for LLM queries!\")\n",
    "    print(f\"üìù Use 'df_meta' variable for text-to-SQL generation\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No metadata available. Please check database connection and run metadata extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e392cb4",
   "metadata": {},
   "source": [
    "## 5. Text-to-SQL with LLM (Universal Provider Support)\n",
    "\n",
    "This section implements universal LLM integration supporting multiple providers:\n",
    "- **OpenAI** (GPT-4o, GPT-4o-mini, GPT-4-turbo, GPT-3.5-turbo)\n",
    "- **Anthropic** (Claude-3.5-Sonnet, Claude-3.5-Haiku, Claude-3-Opus)  \n",
    "- **Google** (Gemini-1.5-Pro, Gemini-1.5-Flash, Gemini-1.0-Pro)\n",
    "- **DeepSeek** (DeepSeek-Chat, DeepSeek-Coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5111ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SQL Generator ready!\n",
      "üìä Loaded metadata for 71 tables\n"
     ]
    }
   ],
   "source": [
    "class UniversalLLMProvider:\n",
    "    \"\"\"\n",
    "    Universal LLM provider supporting multiple APIs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.providers_config = {\n",
    "            \"openai\": {\n",
    "                \"env_var\": \"OPENAI_API_KEY\",\n",
    "                \"models\": [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"],\n",
    "                \"default_model\": \"gpt-4o-mini\"\n",
    "            },\n",
    "            \"anthropic\": {\n",
    "                \"env_var\": \"ANTHROPIC_API_KEY\",\n",
    "                \"models\": [\"claude-3-5-sonnet-20241022\", \"claude-3-5-haiku-20241022\", \"claude-3-opus-20240229\"],\n",
    "                \"default_model\": \"claude-3-5-sonnet-20241022\"\n",
    "            },\n",
    "            \"google\": {\n",
    "                \"env_var\": \"GOOGLE_API_KEY\", \n",
    "                \"models\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\"],\n",
    "                \"default_model\": \"gemini-1.5-flash\"\n",
    "            },\n",
    "            \"deepseek\": {\n",
    "                \"env_var\": \"DEEPSEEK_API_KEY\",\n",
    "                \"models\": [\"deepseek-chat\", \"deepseek-coder\"],\n",
    "                \"default_model\": \"deepseek-chat\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def query_openai(self, prompt, api_key, model=\"gpt-4o-mini\"):\n",
    "        \"\"\"Query OpenAI GPT models\"\"\"\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['choices'][0]['message']['content']\n",
    "    \n",
    "    def query_anthropic(self, prompt, api_key, model=\"claude-3-5-sonnet-20241022\"):\n",
    "        \"\"\"Query Anthropic Claude models\"\"\"\n",
    "        url = \"https://api.anthropic.com/v1/messages\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"x-api-key\": api_key,\n",
    "            \"anthropic-version\": \"2023-06-01\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0.1,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['content'][0]['text']\n",
    "    \n",
    "    def query_google_gemini(self, prompt, api_key, model=\"gemini-1.5-flash\"):\n",
    "        \"\"\"Query Google Gemini models\"\"\"\n",
    "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        data = {\n",
    "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "            \"generationConfig\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"maxOutputTokens\": 2000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['candidates'][0]['content']['parts'][0]['text']\n",
    "    \n",
    "    def query_deepseek(self, prompt, api_key, model=\"deepseek-chat\"):\n",
    "        \"\"\"Query DeepSeek models\"\"\"\n",
    "        url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['choices'][0]['message']['content']\n",
    "    \n",
    "    def query_llm(self, prompt, provider, model=None):\n",
    "        \"\"\"Universal method to query any LLM provider\"\"\"\n",
    "        if provider not in self.providers_config:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "        \n",
    "        config = self.providers_config[provider]\n",
    "        api_key = os.getenv(config[\"env_var\"])\n",
    "        \n",
    "        if not api_key:\n",
    "            raise ValueError(f\"API key not found for {provider}. Set {config['env_var']} environment variable.\")\n",
    "        \n",
    "        selected_model = model or config[\"default_model\"]\n",
    "        \n",
    "        try:\n",
    "            if provider == \"openai\":\n",
    "                return self.query_openai(prompt, api_key, selected_model)\n",
    "            elif provider == \"anthropic\":\n",
    "                return self.query_anthropic(prompt, api_key, selected_model)\n",
    "            elif provider == \"google\":\n",
    "                return self.query_google_gemini(prompt, api_key, selected_model)\n",
    "            elif provider == \"deepseek\":\n",
    "                return self.query_deepseek(prompt, api_key, selected_model)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error querying {provider}: {str(e)}\")\n",
    "\n",
    "class SQLGenerator:\n",
    "    \"\"\"\n",
    "    Generate SQL queries from natural language using LLMs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_df, llm_provider):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.llm_provider = llm_provider\n",
    "    \n",
    "    def analyze_query_requirements(self, user_query, provider, model=None):\n",
    "        \"\"\"Analyze user query to identify relevant tables and columns\"\"\"\n",
    "        if self.metadata_df is None:\n",
    "            raise ValueError(\"No metadata available. Load metadata first.\")\n",
    "        \n",
    "        # Prepare metadata context for LLM\n",
    "        metadata_context = self._prepare_metadata_context()\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Given the following database metadata for MSSQL server database and user query, identify the most relevant tables and columns:\n",
    "\n",
    "        DATABASE METADATA:\n",
    "        {metadata_context}\n",
    "\n",
    "        USER QUERY: {user_query}\n",
    "\n",
    "        Please respond with:\n",
    "        1. Relevant tables: List table names (include schema if needed)\n",
    "        2. Relevant columns from those tables  \n",
    "        3. Brief explanation of why these are relevant\n",
    "        4. Any join conditions that might be needed\n",
    "\n",
    "        Format your response as structured text.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_provider.query_llm(prompt, provider, model)\n",
    "        return response\n",
    "    \n",
    "    def generate_sql_query(self, user_query, relevant_tables_columns, provider, model=None):\n",
    "        \"\"\"Generate SQL query based on analysis\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following information, generate a Microsoft SQL Server query to answer the user's question:\n",
    "\n",
    "        USER QUERY: {user_query}\n",
    "\n",
    "        RELEVANT TABLES AND COLUMNS:\n",
    "        {relevant_tables_columns}\n",
    "\n",
    "        Please generate a well-formatted SQL Server query that:\n",
    "        1. Uses proper Microsoft SQL Server compatible syntax\n",
    "        2. Includes appropriate JOINs if multiple tables are needed\n",
    "        3. Uses proper WHERE clauses for filtering\n",
    "        4. Uses appropriate aggregate functions if needed\n",
    "        5. Is optimized for performance\n",
    "        6. Includes proper schema names (schema.table format)\n",
    "        7. Uses TOP instead of LIMIT for SQL Server\n",
    "        8. Self-check for accurate syntax for partition, Ranking and other advanced sql functions for e.g. partition should have over clause\n",
    "        9. Self-check that every column name, table and schema produced in final SQL exists in the database metadata provided. Check for exact spelling\n",
    "\n",
    "        Return ONLY the SQL query without any additional explanation or markdown formatting.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_provider.query_llm(prompt, provider, model)\n",
    "        \n",
    "        # Clean up the response to extract just the SQL\n",
    "        sql_query = self._clean_sql_response(response)\n",
    "        return sql_query\n",
    "    \n",
    "    def _prepare_metadata_context(self, max_tables=50):\n",
    "        \"\"\"Prepare metadata context for LLM prompts\"\"\"\n",
    "        if self.metadata_df is None:\n",
    "            return \"No metadata available\"\n",
    "        \n",
    "        # Group by table to create table-centric view\n",
    "        table_info = []\n",
    "        \n",
    "        for table_name, group in self.metadata_df.groupby(['TABLE_SCHEMA', 'TABLE_NAME']):\n",
    "            schema, table = table_name\n",
    "            columns_info = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                col_desc = row.get('INFERRED_DESCRIPTION', '')\n",
    "                col_info = f\"  {row['COLUMN_NAME']} ({row['FULL_DATA_TYPE']}) - {col_desc}\"\n",
    "                columns_info.append(col_info)\n",
    "            \n",
    "            table_desc = f\"Table: {schema}.{table}\\n\" + \"\\n\".join(columns_info[:20])  # Limit columns per table\n",
    "            table_info.append(table_desc)\n",
    "            \n",
    "            if len(table_info) >= max_tables:\n",
    "                break\n",
    "        \n",
    "        return \"\\n\\n\".join(table_info)\n",
    "    \n",
    "    def _clean_sql_response(self, response):\n",
    "        \"\"\"Clean LLM response to extract SQL query\"\"\"\n",
    "        # Remove markdown code blocks\n",
    "        import re\n",
    "        response = re.sub(r'```sql\\s*', '', response, flags=re.IGNORECASE)\n",
    "        response = re.sub(r'```\\s*', '', response)\n",
    "        \n",
    "        # Remove common prefixes\n",
    "        response = re.sub(r'^(sql\\s*:?\\s*)', '', response, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        response = response.strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def process_nl_query(self, user_query, provider, model=None):\n",
    "        \"\"\"Complete process: analyze query -> generate SQL\"\"\"\n",
    "        print(f\"ü§ñ Processing with {provider.title()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Step 1: Analyze requirements\n",
    "        print(\"Step 1: Analyzing query requirements...\")\n",
    "        analysis = self.analyze_query_requirements(user_query, provider, model)\n",
    "        print(\"Analysis:\")\n",
    "        print(analysis)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        \n",
    "        # Step 2: Generate SQL\n",
    "        print(\"Step 2: Generating SQL query...\")\n",
    "        sql_query = self.generate_sql_query(user_query, analysis, provider, model)\n",
    "        print(\"Generated SQL:\")\n",
    "        print(sql_query)\n",
    "        \n",
    "        return analysis, sql_query\n",
    "\n",
    "# Initialize the LLM provider and SQL generator\n",
    "llm_provider = UniversalLLMProvider()\n",
    "\n",
    "if df_meta is not None:\n",
    "    sql_generator = SQLGenerator(df_meta, llm_provider)\n",
    "    print(\"üöÄ SQL Generator ready!\")\n",
    "    print(f\"üìä Loaded metadata for {df_meta['TABLE_NAME'].nunique()} tables\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize SQL Generator - no metadata available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab4d5f",
   "metadata": {},
   "source": [
    "## 6. Direct SQL Execution on MSSQL Database\n",
    "\n",
    "Execute generated SQL queries directly on the MSSQL database and return results as pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f4acfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SQL Executor ready!\n",
      "‚ö° Ready to execute queries on MSSQL database\n"
     ]
    }
   ],
   "source": [
    "class SQLExecutor:\n",
    "    \"\"\"\n",
    "    Execute SQL queries on MSSQL database with safety checks and monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connector):\n",
    "        self.db_connector = db_connector\n",
    "        self.execution_history = []\n",
    "    \n",
    "    def execute_sql_query(self, sql_query, timeout=30, max_rows=10000):\n",
    "        \"\"\"\n",
    "        Execute SQL query with safety checks and monitoring\n",
    "        \n",
    "        Parameters:\n",
    "        - sql_query: SQL query to execute\n",
    "        - timeout: Query timeout in seconds\n",
    "        - max_rows: Maximum rows to return (safety limit)\n",
    "        \"\"\"\n",
    "        if not self.db_connector.engine:\n",
    "            raise Exception(\"No database connection available\")\n",
    "        \n",
    "        # Basic safety checks\n",
    "        if not self._is_safe_query(sql_query):\n",
    "            raise Exception(\"Query contains potentially unsafe operations\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîÑ Executing SQL query...\")\n",
    "            print(\"-\" * 50)\n",
    "            print(sql_query)\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Execute query with timeout\n",
    "            with self.db_connector.engine.connect() as connection:\n",
    "                # Set query timeout\n",
    "                connection = connection.execution_options(autocommit=True)\n",
    "                \n",
    "                result_df = pd.read_sql_query(\n",
    "                    text(sql_query), \n",
    "                    connection,\n",
    "                    chunksize=None\n",
    "                )\n",
    "                \n",
    "                # Apply row limit for safety\n",
    "                if len(result_df) > max_rows:\n",
    "                    print(f\"‚ö†Ô∏è  Result truncated to {max_rows} rows (original: {len(result_df)} rows)\")\n",
    "                    result_df = result_df.head(max_rows)\n",
    "                \n",
    "                execution_time = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                print(f\"‚úÖ Query executed successfully!\")\n",
    "                print(f\"‚è±Ô∏è  Execution time: {execution_time:.2f} seconds\")\n",
    "                print(f\"üìä Rows returned: {len(result_df)}\")\n",
    "                print(f\"üìã Columns: {len(result_df.columns)}\")\n",
    "                \n",
    "                # Log execution\n",
    "                self._log_execution(sql_query, len(result_df), execution_time, True, None)\n",
    "                \n",
    "                return result_df\n",
    "                \n",
    "        except Exception as e:\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            print(f\"‚ùå Query execution failed!\")\n",
    "            print(f\"‚è±Ô∏è  Time before failure: {execution_time:.2f} seconds\")\n",
    "            print(f\"üîç Error: {error_msg}\")\n",
    "            \n",
    "            # Log failed execution\n",
    "            self._log_execution(sql_query, 0, execution_time, False, error_msg)\n",
    "            \n",
    "            # Provide helpful error suggestions\n",
    "            self._suggest_error_fixes(error_msg)\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def _is_safe_query(self, sql_query):\n",
    "        \"\"\"Basic safety checks for SQL queries\"\"\"\n",
    "        sql_upper = sql_query.upper().strip()\n",
    "        \n",
    "        # Check for dangerous operations\n",
    "        dangerous_keywords = [\n",
    "            'DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE', 'INSERT', 'UPDATE',\n",
    "            'EXEC', 'EXECUTE', 'SP_', 'XP_', 'BULK', 'OPENROWSET', 'OPENDATASOURCE'\n",
    "        ]\n",
    "        \n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in sql_upper:\n",
    "                print(f\"‚ùå Unsafe operation detected: {keyword}\")\n",
    "                return False\n",
    "        \n",
    "        # Must start with SELECT (allow WITH for CTEs)\n",
    "        if not (sql_upper.startswith('SELECT') or sql_upper.startswith('WITH')):\n",
    "            print(f\"‚ùå Only SELECT queries are allowed\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _suggest_error_fixes(self, error_msg):\n",
    "        \"\"\"Suggest fixes for common SQL errors\"\"\"\n",
    "        error_lower = error_msg.lower()\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        if \"invalid object name\" in error_lower:\n",
    "            suggestions.append(\"‚Ä¢ Check table/column names and ensure they exist\")\n",
    "            suggestions.append(\"‚Ä¢ Verify schema names are correct (use schema.table format)\")\n",
    "            suggestions.append(\"‚Ä¢ Check for typos in table or column names\")\n",
    "        \n",
    "        if \"syntax error\" in error_lower or \"incorrect syntax\" in error_lower:\n",
    "            suggestions.append(\"‚Ä¢ Check SQL syntax - ensure proper SQL Server T-SQL format\")\n",
    "            suggestions.append(\"‚Ä¢ Use TOP instead of LIMIT for SQL Server\")\n",
    "            suggestions.append(\"‚Ä¢ Check for missing commas, parentheses, or quotes\")\n",
    "        \n",
    "        if \"permission\" in error_lower or \"access\" in error_lower:\n",
    "            suggestions.append(\"‚Ä¢ Check database permissions for your account\")\n",
    "            suggestions.append(\"‚Ä¢ Verify you have SELECT permissions on the tables\")\n",
    "        \n",
    "        if \"timeout\" in error_lower:\n",
    "            suggestions.append(\"‚Ä¢ Query took too long - try adding WHERE clauses to limit data\")\n",
    "            suggestions.append(\"‚Ä¢ Consider using TOP to limit results\")\n",
    "            suggestions.append(\"‚Ä¢ Add appropriate indexes if you have admin access\")\n",
    "        \n",
    "        if suggestions:\n",
    "            print(\"\\nüí° Suggestions to fix the error:\")\n",
    "            for suggestion in suggestions:\n",
    "                print(suggestion)\n",
    "    \n",
    "    def _log_execution(self, query, rows_returned, execution_time, success, error_msg):\n",
    "        \"\"\"Log query execution for analysis\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'query': query[:200] + \"...\" if len(query) > 200 else query,\n",
    "            'rows_returned': rows_returned,\n",
    "            'execution_time': execution_time,\n",
    "            'success': success,\n",
    "            'error_msg': error_msg\n",
    "        }\n",
    "        self.execution_history.append(log_entry)\n",
    "    \n",
    "    def get_execution_history(self):\n",
    "        \"\"\"Get execution history as DataFrame\"\"\"\n",
    "        if not self.execution_history:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.execution_history)\n",
    "    \n",
    "    def get_query_statistics(self):\n",
    "        \"\"\"Get execution statistics\"\"\"\n",
    "        if not self.execution_history:\n",
    "            print(\"No execution history available\")\n",
    "            return\n",
    "        \n",
    "        df_history = self.get_execution_history()\n",
    "        \n",
    "        print(\"üìä Query Execution Statistics:\")\n",
    "        print(f\"  Total queries executed: {len(df_history)}\")\n",
    "        print(f\"  Successful queries: {df_history['success'].sum()}\")\n",
    "        print(f\"  Failed queries: {(~df_history['success']).sum()}\")\n",
    "        print(f\"  Average execution time: {df_history['execution_time'].mean():.2f} seconds\")\n",
    "        print(f\"  Total rows returned: {df_history['rows_returned'].sum()}\")\n",
    "\n",
    "# Initialize SQL executor\n",
    "if db_connector.engine:\n",
    "    sql_executor = SQLExecutor(db_connector)\n",
    "    print(\"üöÄ SQL Executor ready!\")\n",
    "    print(\"‚ö° Ready to execute queries on MSSQL database\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize SQL Executor - no database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feeee15",
   "metadata": {},
   "source": [
    "## 7. Interactive Widgets for NL Query to SQL and Execution\n",
    "\n",
    "Complete interactive interface for natural language to SQL conversion and execution on MSSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8fefac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Interactive Text-to-SQL Interface Ready!\n",
      "\n",
      "üí° Usage Instructions:\n",
      "1. Select your preferred LLM provider and model\n",
      "2. Enter your question in natural language\n",
      "3. Click 'Generate SQL' to create the query\n",
      "4. Review the generated SQL\n",
      "5. Click 'Execute SQL' to run it on the database\n",
      "6. Click 'Visualize' to create charts and graphs\n",
      "7. Click 'Insights' to generate business recommendations\n",
      "8. View the results, visualizations, and insights below\n",
      "\n",
      "üìù Example Questions:\n",
      "  1. Show me all customers from California\n",
      "  2. Find the top 10 products by sales\n",
      "  3. List employees hired in the last year\n",
      "  4. What are the most recent orders?\n",
      "  5. Show revenue by month for this year\n",
      "  6. Segment customers based on their order values and frequency into high, medium, and low value segments.\n",
      "\n",
      "üìä Visualization-Friendly Queries:\n",
      "  1. Sales trends by month and product category\n",
      "  2. Customer segmentation by purchase behavior\n",
      "  3. Regional performance comparison\n",
      "  4. Employee productivity metrics by department\n",
      "  5. Inventory levels and turnover rates\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5b1b9bb039467a872cc22598550fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>üöÄ MSSQL Text-to-SQL Interface</h2>'), HTML(value='\\n        <div style=\"backgro‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataVisualizationAndInsights:\n",
    "    \"\"\"\n",
    "    Generate visualizations and business insights from DataFrame results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider):\n",
    "        self.llm_provider = llm_provider\n",
    "        \n",
    "    def generate_insights(self, df, original_query, provider=\"openai\", model=None):\n",
    "        \"\"\"Generate business insights using LLM\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            return \"No data available for insights generation.\"\n",
    "        \n",
    "        # Prepare data summary for LLM\n",
    "        data_summary = self._prepare_data_summary(df)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on the following SQL query results and data summary, provide business insights and recommendations:\n",
    "\n",
    "        ORIGINAL QUERY: {original_query}\n",
    "\n",
    "        DATA SUMMARY:\n",
    "        {data_summary}\n",
    "\n",
    "        Please provide:\n",
    "        1. Key findings from the data\n",
    "        2. Business insights and trends\n",
    "        3. Potential recommendations or actions\n",
    "        4. Any anomalies or interesting patterns\n",
    "        5. Suggested follow-up questions for deeper analysis\n",
    "\n",
    "        Format your response in clear, business-friendly language.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            insights = self.llm_provider.query_llm(prompt, provider, model)\n",
    "            return insights\n",
    "        except Exception as e:\n",
    "            return f\"Error generating insights: {str(e)}\"\n",
    "    \n",
    "    def _prepare_data_summary(self, df):\n",
    "        \"\"\"Prepare a comprehensive summary of the DataFrame\"\"\"\n",
    "        summary = []\n",
    "        \n",
    "        # Basic info\n",
    "        summary.append(f\"Dataset Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        summary.append(f\"Columns: {', '.join(df.columns.tolist())}\")\n",
    "        \n",
    "        # Data types and basic stats\n",
    "        summary.append(\"\\nColumn Information:\")\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            non_null = df[col].notna().sum()\n",
    "            null_count = df[col].isna().sum()\n",
    "            \n",
    "            col_info = f\"  {col}: {dtype}, {non_null} non-null, {null_count} null\"\n",
    "            \n",
    "            # Add statistics for numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                try:\n",
    "                    stats = df[col].describe()\n",
    "                    col_info += f\" (min: {stats['min']:.2f}, max: {stats['max']:.2f}, mean: {stats['mean']:.2f})\"\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Add info for categorical columns\n",
    "            elif pd.api.types.is_object_dtype(df[col]):\n",
    "                unique_count = df[col].nunique()\n",
    "                col_info += f\" ({unique_count} unique values)\"\n",
    "                if unique_count <= 10:\n",
    "                    top_values = df[col].value_counts().head(5)\n",
    "                    col_info += f\", top values: {dict(top_values)}\"\n",
    "            \n",
    "            summary.append(col_info)\n",
    "        \n",
    "        # Sample data\n",
    "        summary.append(f\"\\nSample Data (first 3 rows):\")\n",
    "        sample_data = df.head(3).to_string()\n",
    "        summary.append(sample_data)\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "    \n",
    "    def create_visualizations(self, df, original_query=\"\"):\n",
    "        \"\"\"Create appropriate visualizations based on DataFrame content\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            print(\"No data available for visualization.\")\n",
    "            return\n",
    "        \n",
    "        print(\"üìä Generating visualizations...\")\n",
    "        \n",
    "        # Determine appropriate visualizations based on data types\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "        \n",
    "        visualizations_created = 0\n",
    "        \n",
    "        # 1. Distribution plots for numeric columns\n",
    "        if numeric_cols:\n",
    "            self._create_distribution_plots(df, numeric_cols[:4])  # Limit to 4 columns\n",
    "            visualizations_created += 1\n",
    "        \n",
    "        # 2. Bar charts for categorical data\n",
    "        if categorical_cols:\n",
    "            self._create_categorical_plots(df, categorical_cols[:3])  # Limit to 3 columns\n",
    "            visualizations_created += 1\n",
    "        \n",
    "        # 3. Correlation heatmap if multiple numeric columns\n",
    "        if len(numeric_cols) > 1:\n",
    "            self._create_correlation_heatmap(df, numeric_cols)\n",
    "            visualizations_created += 1\n",
    "        \n",
    "        # 4. Time series if datetime columns exist\n",
    "        if datetime_cols and numeric_cols:\n",
    "            self._create_time_series_plots(df, datetime_cols[0], numeric_cols[:2])\n",
    "            visualizations_created += 1\n",
    "        \n",
    "        # 5. Top N analysis for categorical + numeric combinations\n",
    "        if categorical_cols and numeric_cols:\n",
    "            self._create_top_n_analysis(df, categorical_cols[0], numeric_cols[0])\n",
    "            visualizations_created += 1\n",
    "        \n",
    "        # 6. Interactive plotly visualization\n",
    "        self._create_interactive_plot(df, numeric_cols, categorical_cols)\n",
    "        visualizations_created += 1\n",
    "        \n",
    "        if visualizations_created == 0:\n",
    "            print(\"‚ö†Ô∏è No suitable visualizations could be created for this data.\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Created {visualizations_created} visualizations\")\n",
    "    \n",
    "    def _create_distribution_plots(self, df, numeric_cols):\n",
    "        \"\"\"Create distribution plots for numeric columns\"\"\"\n",
    "        n_cols = min(len(numeric_cols), 2)\n",
    "        n_rows = (len(numeric_cols) + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))\n",
    "        fig.suptitle('üìä Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:4]):\n",
    "            ax = axes[i] if len(axes) > 1 else axes[0]\n",
    "            \n",
    "            # Histogram with KDE\n",
    "            sns.histplot(data=df, x=col, kde=True, ax=ax)\n",
    "            ax.set_title(f'Distribution of {col}')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(numeric_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_categorical_plots(self, df, categorical_cols):\n",
    "        \"\"\"Create bar charts for categorical columns\"\"\"\n",
    "        n_cols = min(len(categorical_cols), 3)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_cols, figsize=(6*n_cols, 6))\n",
    "        fig.suptitle('üìà Categorical Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(categorical_cols[:3]):\n",
    "            ax = axes[i] if n_cols > 1 else axes[0]\n",
    "            \n",
    "            # Get top 10 categories to avoid overcrowding\n",
    "            top_categories = df[col].value_counts().head(10)\n",
    "            \n",
    "            # Bar plot\n",
    "            sns.barplot(x=top_categories.values, y=top_categories.index, ax=ax)\n",
    "            ax.set_title(f'Top Categories: {col}')\n",
    "            ax.set_xlabel('Count')\n",
    "            ax.set_ylabel(col)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_correlation_heatmap(self, df, numeric_cols):\n",
    "        \"\"\"Create correlation heatmap for numeric columns\"\"\"\n",
    "        if len(numeric_cols) < 2:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        sns.heatmap(correlation_matrix, \n",
    "                   annot=True, \n",
    "                   cmap='coolwarm', \n",
    "                   center=0,\n",
    "                   square=True,\n",
    "                   fmt='.2f')\n",
    "        \n",
    "        plt.title('üî• Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_time_series_plots(self, df, date_col, numeric_cols):\n",
    "        \"\"\"Create time series plots\"\"\"\n",
    "        fig, axes = plt.subplots(len(numeric_cols), 1, figsize=(12, 4*len(numeric_cols)))\n",
    "        fig.suptitle('üìÖ Time Series Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if len(numeric_cols) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            ax = axes[i] if len(numeric_cols) > 1 else axes[0]\n",
    "            \n",
    "            # Sort by date and plot\n",
    "            df_sorted = df.sort_values(date_col)\n",
    "            ax.plot(df_sorted[date_col], df_sorted[col], marker='o', linewidth=2)\n",
    "            ax.set_title(f'{col} over Time')\n",
    "            ax.set_xlabel(date_col)\n",
    "            ax.set_ylabel(col)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_top_n_analysis(self, df, categorical_col, numeric_col):\n",
    "        \"\"\"Create top N analysis combining categorical and numeric data\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Group by categorical column and aggregate numeric column\n",
    "        grouped = df.groupby(categorical_col)[numeric_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "        grouped = grouped.sort_values('sum', ascending=False).head(15)\n",
    "        \n",
    "        # Create subplot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'üèÜ Top Analysis: {categorical_col} vs {numeric_col}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Top by sum\n",
    "        sns.barplot(data=grouped.head(10), x='sum', y=categorical_col, ax=ax1)\n",
    "        ax1.set_title(f'Top 10 by Total {numeric_col}')\n",
    "        ax1.set_xlabel(f'Total {numeric_col}')\n",
    "        \n",
    "        # Top by average\n",
    "        grouped_avg = grouped.sort_values('mean', ascending=False).head(10)\n",
    "        sns.barplot(data=grouped_avg, x='mean', y=categorical_col, ax=ax2)\n",
    "        ax2.set_title(f'Top 10 by Average {numeric_col}')\n",
    "        ax2.set_xlabel(f'Average {numeric_col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_interactive_plot(self, df, numeric_cols, categorical_cols):\n",
    "        \"\"\"Create interactive Plotly visualization\"\"\"\n",
    "        if not numeric_cols:\n",
    "            return\n",
    "        \n",
    "        print(\"üéØ Interactive Visualization:\")\n",
    "        \n",
    "        if len(numeric_cols) >= 2:\n",
    "            # Scatter plot if we have at least 2 numeric columns\n",
    "            color_col = categorical_cols[0] if categorical_cols else None\n",
    "            \n",
    "            fig = px.scatter(df, \n",
    "                           x=numeric_cols[0], \n",
    "                           y=numeric_cols[1],\n",
    "                           color=color_col,\n",
    "                           title=f\"Interactive Scatter Plot: {numeric_cols[0]} vs {numeric_cols[1]}\",\n",
    "                           hover_data=df.columns.tolist())\n",
    "        \n",
    "        elif categorical_cols:\n",
    "            # Bar chart if we have categorical and numeric data\n",
    "            # Group data for better visualization\n",
    "            grouped = df.groupby(categorical_cols[0])[numeric_cols[0]].sum().reset_index()\n",
    "            grouped = grouped.sort_values(numeric_cols[0], ascending=False).head(20)\n",
    "            \n",
    "            fig = px.bar(grouped, \n",
    "                        x=categorical_cols[0], \n",
    "                        y=numeric_cols[0],\n",
    "                        title=f\"Interactive Bar Chart: {categorical_cols[0]} vs {numeric_cols[0]}\")\n",
    "            fig.update_xaxes(tickangle=45)\n",
    "        \n",
    "        else:\n",
    "            # Histogram for single numeric column\n",
    "            fig = px.histogram(df, \n",
    "                             x=numeric_cols[0],\n",
    "                             title=f\"Interactive Histogram: {numeric_cols[0]}\")\n",
    "        \n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "\n",
    "class InteractiveTextToSQL:\n",
    "    \"\"\"\n",
    "    Interactive interface for text-to-SQL with MSSQL execution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sql_generator, sql_executor, llm_provider):\n",
    "        self.sql_generator = sql_generator\n",
    "        self.sql_executor = sql_executor\n",
    "        self.llm_provider = llm_provider\n",
    "        self.viz_insights = DataVisualizationAndInsights(llm_provider)\n",
    "        self.last_result_df = None  # Store last query result for visualization\n",
    "        self.setup_widgets()\n",
    "    \n",
    "    def setup_widgets(self):\n",
    "        \"\"\"Create all the interactive widgets\"\"\"\n",
    "        \n",
    "        # Check available providers\n",
    "        available_providers = []\n",
    "        providers_info = self.llm_provider.providers_config\n",
    "        \n",
    "        for provider, config in providers_info.items():\n",
    "            if os.getenv(config[\"env_var\"]):\n",
    "                available_providers.append((provider.title(), provider))\n",
    "        \n",
    "        if not available_providers:\n",
    "            available_providers = [(\"No API Keys\", \"none\")]\n",
    "        \n",
    "        # Provider selection\n",
    "        self.provider_selector = widgets.Dropdown(\n",
    "            options=available_providers,\n",
    "            value=available_providers[0][1],\n",
    "            description='LLM Provider:',\n",
    "            style={'description_width': '120px'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Model selection\n",
    "        self.model_selector = widgets.Dropdown(\n",
    "            options=[],\n",
    "            description='Model:',\n",
    "            style={'description_width': '120px'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Natural language query input\n",
    "        self.nl_query_input = widgets.Textarea(\n",
    "            value='Show revenue by month for this year.',\n",
    "            placeholder='Enter your question in natural language...',\n",
    "            description='Question:',\n",
    "            layout=widgets.Layout(width='100%', height='120px'),\n",
    "            style={'description_width': '120px'}\n",
    "        )\n",
    "        \n",
    "        # Generated SQL display\n",
    "        self.generated_sql_display = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Generated SQL query will appear here...',\n",
    "            description='Generated SQL:',\n",
    "            layout=widgets.Layout(width='100%', height='200px'),\n",
    "            style={'description_width': '120px'}\n",
    "        )\n",
    "        \n",
    "        # Buttons\n",
    "        self.generate_button = widgets.Button(\n",
    "            description='üß† Generate SQL',\n",
    "            button_style='primary',\n",
    "            tooltip='Generate SQL from natural language',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        self.execute_button = widgets.Button(\n",
    "            description='‚ñ∂Ô∏è Execute SQL',\n",
    "            button_style='success',\n",
    "            tooltip='Execute the generated SQL on database',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='üóëÔ∏è Clear',\n",
    "            button_style='warning',\n",
    "            tooltip='Clear all fields',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        self.visualize_button = widgets.Button(\n",
    "            description='üìä Visualize',\n",
    "            button_style='info',\n",
    "            tooltip='Create visualizations from results',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        self.insights_button = widgets.Button(\n",
    "            description='üí° Insights',\n",
    "            button_style='success',\n",
    "            tooltip='Generate business insights',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        # Output areas\n",
    "        self.generation_output = widgets.Output()\n",
    "        self.execution_output = widgets.Output()\n",
    "        self.visualization_output = widgets.Output()\n",
    "        self.insights_output = widgets.Output()\n",
    "        \n",
    "        # Setup event handlers\n",
    "        self.provider_selector.observe(self.update_model_options, names='value')\n",
    "        self.generate_button.on_click(self.generate_sql)\n",
    "        self.execute_button.on_click(self.execute_sql)\n",
    "        self.clear_button.on_click(self.clear_all)\n",
    "        self.visualize_button.on_click(self.create_visualizations)\n",
    "        self.insights_button.on_click(self.generate_insights)\n",
    "        \n",
    "        # Initial model update\n",
    "        self.update_model_options()\n",
    "    \n",
    "    def update_model_options(self, change=None):\n",
    "        \"\"\"Update model options based on selected provider\"\"\"\n",
    "        provider = self.provider_selector.value\n",
    "        \n",
    "        if provider == \"none\":\n",
    "            self.model_selector.options = [(\"No Models\", \"none\")]\n",
    "            return\n",
    "        \n",
    "        if provider in self.llm_provider.providers_config:\n",
    "            models = self.llm_provider.providers_config[provider][\"models\"]\n",
    "            model_options = [(model, model) for model in models]\n",
    "            self.model_selector.options = model_options\n",
    "            if model_options:\n",
    "                self.model_selector.value = model_options[0][1]\n",
    "    \n",
    "    def generate_sql(self, button):\n",
    "        \"\"\"Generate SQL from natural language query\"\"\"\n",
    "        with self.generation_output:\n",
    "            clear_output()\n",
    "            \n",
    "            nl_query = self.nl_query_input.value.strip()\n",
    "            provider = self.provider_selector.value\n",
    "            model = self.model_selector.value\n",
    "            \n",
    "            if not nl_query:\n",
    "                print(\"‚ùå Please enter a natural language question\")\n",
    "                return\n",
    "            \n",
    "            if provider == \"none\":\n",
    "                print(\"‚ùå No LLM provider available. Please set API keys.\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                print(f\"ü§ñ Generating SQL using {provider.title()} ({model})\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                # Generate SQL using the SQL generator\n",
    "                analysis, sql_query = self.sql_generator.process_nl_query(\n",
    "                    nl_query, provider, model\n",
    "                )\n",
    "                \n",
    "                # Update the SQL display\n",
    "                self.generated_sql_display.value = sql_query\n",
    "                \n",
    "                print(\"\\n‚úÖ SQL generation completed!\")\n",
    "                print(\"üëÜ Review the generated SQL above and click 'Execute SQL' to run it.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error generating SQL: {str(e)}\")\n",
    "    \n",
    "    def execute_sql(self, button):\n",
    "        \"\"\"Execute the generated SQL query\"\"\"\n",
    "        with self.execution_output:\n",
    "            clear_output()\n",
    "            \n",
    "            sql_query = self.generated_sql_display.value.strip()\n",
    "            \n",
    "            if not sql_query:\n",
    "                print(\"‚ùå No SQL query to execute. Generate one first.\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Execute the SQL query\n",
    "                result_df = self.sql_executor.execute_sql_query(sql_query)\n",
    "                \n",
    "                if result_df is not None:\n",
    "                    # Store the result for visualization and insights\n",
    "                    self.last_result_df = result_df\n",
    "                    \n",
    "                    print(f\"\\nüìä Query Results ({len(result_df)} rows, {len(result_df.columns)} columns):\")\n",
    "                    \n",
    "                    # Display results\n",
    "                    if len(result_df) > 0:\n",
    "                        display(result_df)\n",
    "                        \n",
    "                        # Show column info\n",
    "                        print(f\"\\nüìã Column Information:\")\n",
    "                        for col in result_df.columns:\n",
    "                            dtype = result_df[col].dtype\n",
    "                            non_null = result_df[col].notna().sum()\n",
    "                            print(f\"  {col}: {dtype} ({non_null}/{len(result_df)} non-null)\")\n",
    "                        \n",
    "                        print(f\"\\nüéØ Data ready for visualization and insights!\")\n",
    "                        print(\"Click 'Visualize' or 'Insights' buttons to explore the data further.\")\n",
    "                    else:\n",
    "                        print(\"No rows returned by the query.\")\n",
    "                        self.last_result_df = None\n",
    "                else:\n",
    "                    print(\"‚ùå Query execution failed. Check the error message above.\")\n",
    "                    self.last_result_df = None\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Execution error: {str(e)}\")\n",
    "                self.last_result_df = None\n",
    "    \n",
    "    def create_visualizations(self, button):\n",
    "        \"\"\"Create visualizations from the last query result\"\"\"\n",
    "        with self.visualization_output:\n",
    "            clear_output()\n",
    "            \n",
    "            if self.last_result_df is None or len(self.last_result_df) == 0:\n",
    "                print(\"‚ùå No data available for visualization.\")\n",
    "                print(\"Execute a SQL query first to generate data.\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                print(\"üé® Creating visualizations from query results...\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                # Create visualizations\n",
    "                self.viz_insights.create_visualizations(\n",
    "                    self.last_result_df, \n",
    "                    self.nl_query_input.value\n",
    "                )\n",
    "                \n",
    "                print(\"\\n‚úÖ Visualizations created successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error creating visualizations: {str(e)}\")\n",
    "    \n",
    "    def generate_insights(self, button):\n",
    "        \"\"\"Generate business insights from the last query result\"\"\"\n",
    "        with self.insights_output:\n",
    "            clear_output()\n",
    "            \n",
    "            if self.last_result_df is None or len(self.last_result_df) == 0:\n",
    "                print(\"‚ùå No data available for insights generation.\")\n",
    "                print(\"Execute a SQL query first to generate data.\")\n",
    "                return\n",
    "            \n",
    "            provider = self.provider_selector.value\n",
    "            model = self.model_selector.value\n",
    "            \n",
    "            if provider == \"none\":\n",
    "                print(\"‚ùå No LLM provider available for insights generation.\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                print(f\"üß† Generating business insights using {provider.title()}...\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                # Generate insights\n",
    "                insights = self.viz_insights.generate_insights(\n",
    "                    self.last_result_df,\n",
    "                    self.nl_query_input.value,\n",
    "                    provider,\n",
    "                    model\n",
    "                )\n",
    "                \n",
    "                print(\"üí° Business Insights & Recommendations:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(insights)\n",
    "                \n",
    "                print(\"\\n‚úÖ Insights generated successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error generating insights: {str(e)}\")\n",
    "    \n",
    "    def clear_all(self, button):\n",
    "        \"\"\"Clear all input and output fields\"\"\"\n",
    "        self.nl_query_input.value = ''\n",
    "        self.generated_sql_display.value = ''\n",
    "        self.last_result_df = None\n",
    "        \n",
    "        with self.generation_output:\n",
    "            clear_output()\n",
    "        \n",
    "        with self.execution_output:\n",
    "            clear_output()\n",
    "            \n",
    "        with self.visualization_output:\n",
    "            clear_output()\n",
    "            \n",
    "        with self.insights_output:\n",
    "            clear_output()\n",
    "        \n",
    "        print(\"üóëÔ∏è All fields cleared!\")\n",
    "    \n",
    "    def display_interface(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        \n",
    "        # Status display\n",
    "        status_html = self.get_status_html()\n",
    "        \n",
    "        # Create the layout\n",
    "        interface = widgets.VBox([\n",
    "            widgets.HTML(\"<h2>üöÄ MSSQL Text-to-SQL Interface</h2>\"),\n",
    "            widgets.HTML(status_html),\n",
    "            \n",
    "            # Provider and model selection\n",
    "            widgets.HTML(\"<h4>ü§ñ LLM Configuration</h4>\"),\n",
    "            widgets.HBox([self.provider_selector, self.model_selector]),\n",
    "            \n",
    "            # Query input\n",
    "            widgets.HTML(\"<h4>üí¨ Natural Language Query</h4>\"),\n",
    "            self.nl_query_input,\n",
    "            \n",
    "            # Action buttons\n",
    "            widgets.HTML(\"<h4>‚ö° Actions</h4>\"),\n",
    "            widgets.HBox([\n",
    "                self.generate_button, \n",
    "                self.execute_button, \n",
    "                self.visualize_button,\n",
    "                self.insights_button,\n",
    "                self.clear_button\n",
    "            ]),\n",
    "            \n",
    "            # Generated SQL\n",
    "            widgets.HTML(\"<h4>üìù Generated SQL</h4>\"),\n",
    "            self.generated_sql_display,\n",
    "            \n",
    "            # Outputs\n",
    "            widgets.HTML(\"<h4>üîç Generation Analysis</h4>\"),\n",
    "            self.generation_output,\n",
    "            \n",
    "            widgets.HTML(\"<h4>üìä Execution Results</h4>\"),\n",
    "            self.execution_output,\n",
    "            \n",
    "            widgets.HTML(\"<h4>üìà Data Visualizations</h4>\"),\n",
    "            self.visualization_output,\n",
    "            \n",
    "            widgets.HTML(\"<h4>üí° Business Insights</h4>\"),\n",
    "            self.insights_output\n",
    "        ])\n",
    "        \n",
    "        return interface\n",
    "    \n",
    "    def get_status_html(self):\n",
    "        \"\"\"Get HTML status display\"\"\"\n",
    "        # Check database connection\n",
    "        db_status = \"üü¢ Connected\" if self.sql_executor.db_connector.engine else \"üî¥ Disconnected\"\n",
    "        \n",
    "        # Check metadata\n",
    "        metadata_status = \"üü¢ Loaded\" if self.sql_generator.metadata_df is not None else \"üî¥ Not Available\"\n",
    "        \n",
    "        # Check API keys\n",
    "        available_providers = []\n",
    "        for provider, config in self.llm_provider.providers_config.items():\n",
    "            if os.getenv(config[\"env_var\"]):\n",
    "                available_providers.append(provider.title())\n",
    "        \n",
    "        llm_status = f\"üü¢ {', '.join(available_providers)}\" if available_providers else \"üî¥ No API Keys\"\n",
    "        \n",
    "        html = f\"\"\"\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "            <b>System Status:</b><br>\n",
    "            üìä Database: {db_status}<br>\n",
    "            üìã Metadata: {metadata_status}<br>\n",
    "            ü§ñ LLM Providers: {llm_status}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "\n",
    "# Initialize and display the interface\n",
    "if 'sql_generator' in locals() and 'sql_executor' in locals():\n",
    "    interface = InteractiveTextToSQL(sql_generator, sql_executor, llm_provider)\n",
    "    \n",
    "    print(\"üéâ Interactive Text-to-SQL Interface Ready!\")\n",
    "    print(\"\\nüí° Usage Instructions:\")\n",
    "    print(\"1. Select your preferred LLM provider and model\")\n",
    "    print(\"2. Enter your question in natural language\")\n",
    "    print(\"3. Click 'Generate SQL' to create the query\")\n",
    "    print(\"4. Review the generated SQL\")\n",
    "    print(\"5. Click 'Execute SQL' to run it on the database\")\n",
    "    print(\"6. Click 'Visualize' to create charts and graphs\")\n",
    "    print(\"7. Click 'Insights' to generate business recommendations\")\n",
    "    print(\"8. View the results, visualizations, and insights below\")\n",
    "    \n",
    "    print(\"\\nüìù Example Questions:\")\n",
    "    examples = [\n",
    "        \"Show me all customers from California\",\n",
    "        \"Find the top 10 products by sales\",\n",
    "        \"List employees hired in the last year\",\n",
    "        \"What are the most recent orders?\",\n",
    "        \"Show revenue by month for all years in the data\",\n",
    "        \"Segment customers based on their order values and frequency into high, medium, and low value segments.\"\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"  {i}. {example}\")\n",
    "    \n",
    "    print(\"\\nüìä Visualization-Friendly Queries:\")\n",
    "    viz_examples = [\n",
    "        \"Sales trends by month and product category\",\n",
    "        \"Customer segmentation by purchase behavior\", \n",
    "        \"Regional performance comparison\",\n",
    "        \"Employee productivity metrics by department\",\n",
    "        \"Inventory levels and turnover rates\"\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(viz_examples, 1):\n",
    "        print(f\"  {i}. {example}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Display the interface\n",
    "    display(interface.display_interface())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot create interface - missing required components\")\n",
    "    print(\"Please ensure database connection and metadata are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406e328",
   "metadata": {},
   "source": [
    "## üéØ Summary and Next Steps\n",
    "\n",
    "### ‚úÖ What This Notebook Provides\n",
    "\n",
    "1. **MSSQL Database Integration** - Secure connection to SQL Server using pymssql with comprehensive error handling\n",
    "2. **Automated Metadata Extraction** - Extracts table/column information and saves to Excel for reuse\n",
    "3. **Multi-Provider LLM Support** - Works with OpenAI, Anthropic, Google Gemini, and DeepSeek\n",
    "4. **Intelligent SQL Generation** - Converts natural language to optimized T-SQL queries\n",
    "5. **Direct Database Execution** - Runs queries safely on MSSQL with monitoring and limits\n",
    "6. **Interactive Interface** - User-friendly widgets for the complete workflow\n",
    "\n",
    "### üîß Configuration Checklist\n",
    "\n",
    "Before using this notebook:\n",
    "\n",
    "- [ ] Set up MSSQL connection environment variables (server, database, username, password, port)\n",
    "- [ ] Configure at least one LLM API key\n",
    "- [ ] Install required Python packages (pymssql instead of pyodbc)\n",
    "- [ ] Test database connectivity\n",
    "- [ ] Run metadata extraction\n",
    "- [ ] Verify the interactive interface loads\n",
    "\n",
    "### üöÄ Usage Workflow\n",
    "\n",
    "1. **Setup** - Configure environment variables and run initial cells\n",
    "2. **Connect** - Establish database connection using pymssql\n",
    "3. **Extract** - Generate and save metadata to Excel\n",
    "4. **Query** - Use the interactive interface to ask questions in natural language\n",
    "5. **Execute** - Run generated SQL queries on your database\n",
    "6. **Analyze** - Review results and refine queries as needed\n",
    "\n",
    "### üí° Pro Tips\n",
    "\n",
    "- **Be Specific**: More detailed questions produce better SQL queries\n",
    "- **Use Examples**: Reference specific table/column names when known\n",
    "- **Set Limits**: Always include row limits for large datasets\n",
    "- **Review SQL**: Check generated queries before execution\n",
    "- **Save Results**: Export important query results to files\n",
    "\n",
    "### üîê Security Notes\n",
    "\n",
    "- Only SELECT queries are allowed for safety\n",
    "- Row limits prevent overwhelming results\n",
    "- Query timeout prevents long-running operations\n",
    "- All database operations are logged for monitoring\n",
    "- SQL Server authentication required (Windows Auth not supported with pymssql)\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- Extend with custom functions for your specific domain\n",
    "- Add query result caching for frequently used queries\n",
    "- Implement query optimization suggestions\n",
    "- Create saved query templates for common patterns\n",
    "- Add data visualization capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ You're ready to start querying your MSSQL database with natural language!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582c47e",
   "metadata": {},
   "source": [
    "## üé® Enhanced Features: Data Visualization & Business Insights\n",
    "\n",
    "### üìä Automatic Data Visualization\n",
    "\n",
    "When you execute a SQL query, the system now provides intelligent visualization capabilities:\n",
    "\n",
    "#### **Smart Chart Selection**\n",
    "- **Distribution Plots**: Histograms and KDE plots for numeric data\n",
    "- **Bar Charts**: Top categories for categorical data\n",
    "- **Correlation Heatmaps**: Relationships between numeric variables\n",
    "- **Time Series**: Trends over time for datetime columns\n",
    "- **Top N Analysis**: Ranking analysis for categorical + numeric combinations\n",
    "- **Interactive Plots**: Plotly visualizations for exploration\n",
    "\n",
    "#### **Visualization Types Created**\n",
    "1. **Statistical Distributions** - Understand data spread and patterns\n",
    "2. **Categorical Analysis** - See frequency and proportions\n",
    "3. **Correlation Analysis** - Identify relationships between variables\n",
    "4. **Time-based Trends** - Track changes over time\n",
    "5. **Ranking & Comparisons** - Top performers and outliers\n",
    "6. **Interactive Exploration** - Drill-down capabilities\n",
    "\n",
    "### üí° AI-Powered Business Insights\n",
    "\n",
    "The system uses your selected LLM to generate business insights:\n",
    "\n",
    "#### **Insight Categories**\n",
    "- **Key Findings**: Main takeaways from the data\n",
    "- **Business Trends**: Patterns and movements\n",
    "- **Recommendations**: Actionable suggestions\n",
    "- **Anomalies**: Unusual patterns requiring attention\n",
    "- **Follow-up Questions**: Suggestions for deeper analysis\n",
    "\n",
    "#### **Insight Generation Process**\n",
    "1. **Data Analysis**: Statistical summary of results\n",
    "2. **Pattern Recognition**: Identifies trends and outliers\n",
    "3. **Business Context**: Applies domain knowledge\n",
    "4. **Actionable Recommendations**: Provides next steps\n",
    "\n",
    "### üîÑ Workflow Enhancement\n",
    "\n",
    "The enhanced workflow now includes:\n",
    "\n",
    "```\n",
    "Natural Language Query ‚Üí SQL Generation ‚Üí Execution ‚Üí Visualization ‚Üí Insights\n",
    "```\n",
    "\n",
    "Each step builds upon the previous, providing a complete data analysis pipeline from question to actionable insights.\n",
    "\n",
    "### üìà Use Cases\n",
    "\n",
    "Perfect for:\n",
    "- **Executive Dashboards**: Quick visual summaries\n",
    "- **Trend Analysis**: Understanding patterns over time\n",
    "- **Performance Monitoring**: KPI tracking and comparison\n",
    "- **Data Exploration**: Discovery of hidden patterns\n",
    "- **Report Generation**: Automated insights for stakeholders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-to-bi-insights (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
