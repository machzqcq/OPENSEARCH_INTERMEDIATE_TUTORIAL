{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb00de6",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Comprehensive OpenSearch Analysis: Analyzers, BM25, Neural & Hybrid Search\n",
    "\n",
    "This notebook performs a comprehensive comparison of different text analysis and search strategies using the SQUAD dataset:\n",
    "\n",
    "## ğŸ“Š Workflow Overview\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#4A90E2','primaryTextColor':'#fff','primaryBorderColor':'#357ABD','lineColor':'#F28C28','secondaryColor':'#50E3C2','tertiaryColor':'#F5A623','background':'#ffffff','mainBkg':'#4A90E2','secondBkg':'#50E3C2','tertiaryBkg':'#F5A623'}}}%%\n",
    "\n",
    "graph TB\n",
    "    Start([ğŸ“¦ SQUAD Dataset<br/>~87k documents]) --> Load[ğŸ”„ Data Preprocessing]\n",
    "    \n",
    "    Load --> Phase1{Phase 1:<br/>Text Analyzers}\n",
    "    \n",
    "    Phase1 --> A1[ğŸ“ Standard Analyzer<br/>General purpose]\n",
    "    Phase1 --> A2[ğŸ‡¬ğŸ‡§ English Analyzer<br/>Stemming + stopwords]\n",
    "    Phase1 --> A3[ğŸ”‘ Keyword Analyzer<br/>Exact matching]\n",
    "    Phase1 --> A4[âš¡ Whitespace Analyzer<br/>Space splitting]\n",
    "    Phase1 --> A5[ğŸ”¤ Simple Analyzer<br/>Lowercase only]\n",
    "    \n",
    "    A1 --> Ingest1[â±ï¸ Bulk Ingest + Metrics]\n",
    "    A2 --> Ingest1\n",
    "    A3 --> Ingest1\n",
    "    A4 --> Ingest1\n",
    "    A5 --> Ingest1\n",
    "    \n",
    "    Ingest1 --> Compare1[ğŸ“Š Table 1:<br/>Ingestion Performance<br/>Time, Throughput, Size]\n",
    "    \n",
    "    Compare1 --> Query1[ğŸ” BM25 Queries<br/>match, match_phrase, multi_match]\n",
    "    \n",
    "    Query1 --> Compare2[ğŸ“Š Table 2:<br/>Query Performance<br/>Latency, Relevance, Results]\n",
    "    \n",
    "    Compare2 --> Phase2{Phase 2:<br/>Neural Search}\n",
    "    \n",
    "    Phase2 --> Model[ğŸ¤– Deploy Model<br/>msmarco-distilbert-base-tas-b<br/>HuggingFace]\n",
    "    \n",
    "    Model --> Embed[ğŸ§¬ Generate Vectors<br/>title, context, question, answers]\n",
    "    \n",
    "    Embed --> IngestVec[ğŸ“¥ Ingest with k-NN<br/>Vector embeddings]\n",
    "    \n",
    "    IngestVec --> Phase3{Phase 3:<br/>Search Comparison}\n",
    "    \n",
    "    Phase3 --> BM25[ğŸ¯ BM25 Search<br/>Lexical matching]\n",
    "    Phase3 --> Neural[ğŸ§  Neural Search<br/>k-NN vector similarity]\n",
    "    Phase3 --> Hybrid[âš¡ Hybrid Search<br/>BM25 + Neural combined]\n",
    "    \n",
    "    BM25 --> Final[ğŸ“Š Table 3:<br/>Final Comparison<br/>BM25 vs Neural vs Hybrid<br/>Latency, Precision, Use Cases]\n",
    "    Neural --> Final\n",
    "    Hybrid --> Final\n",
    "    \n",
    "    Final --> End([âœ… Complete Analysis<br/>with Recommendations])\n",
    "    \n",
    "    style Start fill:#4A90E2,stroke:#357ABD,stroke-width:3px,color:#fff\n",
    "    style Phase1 fill:#F5A623,stroke:#D68910,stroke-width:3px,color:#fff\n",
    "    style Phase2 fill:#F5A623,stroke:#D68910,stroke-width:3px,color:#fff\n",
    "    style Phase3 fill:#F5A623,stroke:#D68910,stroke-width:3px,color:#fff\n",
    "    style Compare1 fill:#50E3C2,stroke:#2ECC71,stroke-width:2px,color:#000\n",
    "    style Compare2 fill:#50E3C2,stroke:#2ECC71,stroke-width:2px,color:#000\n",
    "    style Final fill:#50E3C2,stroke:#2ECC71,stroke-width:2px,color:#000\n",
    "    style Model fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff\n",
    "    style End fill:#2ECC71,stroke:#27AE60,stroke-width:3px,color:#fff\n",
    "    style BM25 fill:#9B59B6,stroke:#8E44AD,stroke-width:2px,color:#fff\n",
    "    style Neural fill:#9B59B6,stroke:#8E44AD,stroke-width:2px,color:#fff\n",
    "    style Hybrid fill:#9B59B6,stroke:#8E44AD,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "## ğŸ¯ Analysis Goals\n",
    "\n",
    "1. **Analyzer Comparison**: Test multiple text analyzers and measure ingestion performance\n",
    "2. **Query Strategy**: Compare search effectiveness across different analyzers\n",
    "3. **Neural Search**: Implement vector-based semantic search using sentence transformers\n",
    "4. **Hybrid Approach**: Combine lexical (BM25) and semantic (Neural) search for optimal results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66680bd7",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b5d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(current_dir, '../../0. DATA'))\n",
    "\n",
    "# Construct the path to the directory levels up\n",
    "module_paths = [os.path.abspath(os.path.join(current_dir, '../../')),] \n",
    "\n",
    "# Add the module path to sys.path if it's not already there\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    import helpers as hp\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Error importing modules: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb3d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to OpenSearch cluster\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "import time\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch, helpers\n",
    "\n",
    "IS_AUTH = True # Set to False if security is disabled\n",
    "HOST = 'localhost'  # Replace with your OpenSearch host, if running everything locally use 'localhost'\n",
    "\n",
    "if IS_AUTH:\n",
    "    # Initialize the OpenSearch client\n",
    "    os_client = OpenSearch(\n",
    "        hosts=[{'host': HOST, 'port': 9200}],\n",
    "        http_auth=('admin', 'Developer@123'),  # Replace with your credentials\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "else:\n",
    "    # Initialize the OpenSearch client without authentication\n",
    "    os_client = OpenSearch(\n",
    "        hosts=[{'host': HOST, 'port': 9200}],\n",
    "        use_ssl=False,\n",
    "        verify_certs=False,\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "# Initialize ML Commons client\n",
    "ml_client = MLCommonClient(os_client)\n",
    "\n",
    "# Check if cluster is up\n",
    "if (os_client.ping()):\n",
    "    print(\"Connected to OpenSearch cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c1186",
   "metadata": {},
   "source": [
    "## Read SQUAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b4077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id is unique i.e. primary key\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ad078301-002a-4cfb-b15a-7612ffc562e4",
       "rows": [
        [
         "Index",
         "0.000125885009765625"
        ],
        [
         "id",
         "6.09848690032959"
        ],
        [
         "title",
         "5.291620254516602"
        ],
        [
         "context",
         "83.03140926361084"
        ],
        [
         "question",
         "9.089622497558594"
        ],
        [
         "answers",
         "16.03985595703125"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/plain": [
       "Index        0.000126\n",
       "id           6.098487\n",
       "title        5.291620\n",
       "context     83.031409\n",
       "question     9.089622\n",
       "answers     16.039856\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total memory usage: 119.55 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.output_area pre {white-space: pre-wrap; word-break: break-word;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the SQuAD training dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "context",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answers",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "6c9a8ca8-1f53-4b0a-befb-6e0c61a30d49",
       "rows": [
        [
         "0",
         "5733be284776f41900661182",
         "University_of_Notre_Dame",
         "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
         "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?",
         "{'text': array(['Saint Bernadette Soubirous'], dtype=object), 'answer_start': array([515], dtype=int32)}"
        ],
        [
         "1",
         "5733be284776f4190066117f",
         "University_of_Notre_Dame",
         "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
         "What is in front of the Notre Dame Main Building?",
         "{'text': array(['a copper statue of Christ'], dtype=object), 'answer_start': array([188], dtype=int32)}"
        ],
        [
         "2",
         "5733be284776f41900661180",
         "University_of_Notre_Dame",
         "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
         "The Basilica of the Sacred heart at Notre Dame is beside to which structure?",
         "{'text': array(['the Main Building'], dtype=object), 'answer_start': array([279], dtype=int32)}"
        ],
        [
         "3",
         "5733be284776f41900661181",
         "University_of_Notre_Dame",
         "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
         "What is the Grotto at Notre Dame?",
         "{'text': array(['a Marian place of prayer and reflection'], dtype=object), 'answer_start': array([381], dtype=int32)}"
        ],
        [
         "4",
         "5733be284776f4190066117e",
         "University_of_Notre_Dame",
         "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
         "What sits on top of the Main Building at Notre Dame?",
         "{'text': array(['a golden statue of the Virgin Mary'], dtype=object), 'answer_start': array([92], dtype=int32)}"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One row as dictionary:\n",
      "{\n",
      "    \"id\": \"5733be284776f41900661182\",\n",
      "    \"title\": \"University_of_Notre_Dame\",\n",
      "    \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\",\n",
      "    \"question\": \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
      "    \"answers\": {\n",
      "        \"text\": [\n",
      "            \"Saint Bernadette Soubirous\"\n",
      "        ],\n",
      "        \"answer_start\": [\n",
      "            515\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import json\n",
    "df_squad_train = pd.read_parquet(f\"{DATA_DIR}/SQUAD-train.parquet\")\n",
    "\n",
    "# Check id is unique i.e. count of rows is same as count of unique ids\n",
    "if {len(df_squad_train)} == {df_squad_train['id'].nunique()}:\n",
    "    print(\"id is unique i.e. primary key\")\n",
    "else:\n",
    "    print(\"id is not unique\")\n",
    "\n",
    "# Print pandas memory usage in MB\n",
    "memory_usage = df_squad_train.memory_usage(deep=True)\n",
    "memory_usage_mb = memory_usage / (1024 * 1024)\n",
    "display(memory_usage_mb)\n",
    "print(f\"\\nTotal memory usage: {memory_usage_mb.sum():.2f} MB\")\n",
    "\n",
    "# Enable word wrap for better readability in Jupyter Notebook\n",
    "display(HTML(\"<style>.output_area pre {white-space: pre-wrap; word-break: break-word;}</style>\")) \n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First few rows of the SQuAD training dataset:\")\n",
    "display(df_squad_train.head())\n",
    "\n",
    "# Print one row as dictionary pretty print\n",
    "print(\"One row as dictionary:\")\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "row_dict = df_squad_train.iloc[0].to_dict()\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(json.dumps(convert_to_serializable(row_dict), indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba4ae5",
   "metadata": {},
   "source": [
    "# ğŸ“Š Phase 1: Text Analyzer Comparison\n",
    "\n",
    "In this phase, we'll create multiple indices with different text analyzers and compare their ingestion performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d31b3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Analyzer Configurations:\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ STANDARD: Default analyzer with grammar based tokenization and lowercase filter\n",
      "\n",
      "ğŸ”¹ ENGLISH: English analyzer with stemming and stopwords removal\n",
      "\n",
      "ğŸ”¹ KEYWORD: Keyword analyzer for exact matching (no tokenization)\n",
      "\n",
      "ğŸ”¹ WHITESPACE: Whitespace analyzer splits on whitespace only\n",
      "\n",
      "ğŸ”¹ SIMPLE: Simple analyzer divides text at non-letters and lowercase\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define different analyzer configurations\n",
    "analyzer_configs = {\n",
    "    \"standard\": {\n",
    "        \"description\": \"Default analyzer with grammar based tokenization and lowercase filter\",\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"standard\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"context\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"question\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"answers\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                        \"answer_start\": {\"type\": \"integer\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"english\": {\n",
    "        \"description\": \"English analyzer with stemming and stopwords removal\",\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"english\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                \"context\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                \"question\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                \"answers\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                        \"answer_start\": {\"type\": \"integer\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"keyword\": {\n",
    "        \"description\": \"Keyword analyzer for exact matching (no tokenization)\",\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"keyword\"},\n",
    "                \"context\": {\"type\": \"text\"},  # Keep text for context as it's too long\n",
    "                \"question\": {\"type\": \"text\"},  # Keep text for question\n",
    "                \"answers\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"text\"},\n",
    "                        \"answer_start\": {\"type\": \"integer\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"whitespace\": {\n",
    "        \"description\": \"Whitespace analyzer splits on whitespace only\",\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"whitespace\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"text\", \"analyzer\": \"whitespace\"},\n",
    "                \"context\": {\"type\": \"text\", \"analyzer\": \"whitespace\"},\n",
    "                \"question\": {\"type\": \"text\", \"analyzer\": \"whitespace\"},\n",
    "                \"answers\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"text\", \"analyzer\": \"whitespace\"},\n",
    "                        \"answer_start\": {\"type\": \"integer\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"simple\": {\n",
    "        \"description\": \"Simple analyzer divides text at non-letters and lowercase\",\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"simple\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"text\", \"analyzer\": \"simple\"},\n",
    "                \"context\": {\"type\": \"text\", \"analyzer\": \"simple\"},\n",
    "                \"question\": {\"type\": \"text\", \"analyzer\": \"simple\"},\n",
    "                \"answers\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"text\", \"analyzer\": \"simple\"},\n",
    "                        \"answer_start\": {\"type\": \"integer\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ Analyzer Configurations:\")\n",
    "print(\"=\" * 80)\n",
    "for name, config in analyzer_configs.items():\n",
    "    print(f\"\\nğŸ”¹ {name.upper()}: {config['description']}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2c69bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Function to prepare documents for bulk ingestion\n",
    "def prepare_bulk_data(df, index_name):\n",
    "    \"\"\"\n",
    "    Prepare data for bulk ingestion\n",
    "    \"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        doc = {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row['id'],\n",
    "            \"_source\": {\n",
    "                \"id\": row['id'],\n",
    "                \"title\": row['title'],\n",
    "                \"context\": row['context'],\n",
    "                \"question\": row['question'],\n",
    "                \"answers\": row['answers']\n",
    "            }\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "# Function to perform bulk ingestion with metrics\n",
    "def bulk_ingest_with_metrics(os_client, df, index_name, analyzer_name):\n",
    "    \"\"\"\n",
    "    Perform bulk ingestion and measure performance metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸš€ Starting bulk ingestion for '{analyzer_name}' analyzer...\")\n",
    "    print(f\"   Index: {index_name}\")\n",
    "    print(f\"   Documents: {len(df):,}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Perform bulk ingestion with progress tracking\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        # Use helpers.bulk for efficient ingestion\n",
    "        # Using streaming API with stats_only=False to get detailed responses\n",
    "        result_gen = helpers.bulk(\n",
    "            os_client, \n",
    "            prepare_bulk_data(df, index_name),\n",
    "            chunk_size=500,\n",
    "            request_timeout=60,\n",
    "            raise_on_error=False,\n",
    "            stats_only=False\n",
    "        )\n",
    "        \n",
    "        for item in result_gen:\n",
    "            # Each item is a tuple: (success_boolean, response_dict)\n",
    "            if isinstance(item, tuple) and len(item) == 2:\n",
    "                ok, response = item\n",
    "                if ok:\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "            else:\n",
    "                # Fallback if response format is unexpected\n",
    "                success_count += 1\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Refresh index to make documents searchable\n",
    "        os_client.indices.refresh(index=index_name)\n",
    "        \n",
    "        # Get index stats\n",
    "        stats = os_client.indices.stats(index=index_name)\n",
    "        index_size_bytes = stats['indices'][index_name]['total']['store']['size_in_bytes']\n",
    "        index_size_mb = index_size_bytes / (1024 * 1024)\n",
    "        doc_count = stats['indices'][index_name]['total']['docs']['count']\n",
    "        \n",
    "        docs_per_second = len(df) / elapsed_time if elapsed_time > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"analyzer\": analyzer_name,\n",
    "            \"index\": index_name,\n",
    "            \"total_docs\": len(df),\n",
    "            \"success\": success_count,\n",
    "            \"errors\": error_count,\n",
    "            \"time_seconds\": round(elapsed_time, 2),\n",
    "            \"docs_per_second\": round(docs_per_second, 2),\n",
    "            \"index_size_mb\": round(index_size_mb, 2),\n",
    "            \"indexed_docs\": doc_count,\n",
    "            \"status\": \"âœ… Success\" if error_count == 0 else f\"âš ï¸ {error_count} errors\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return {\n",
    "            \"analyzer\": analyzer_name,\n",
    "            \"index\": index_name,\n",
    "            \"total_docs\": len(df),\n",
    "            \"success\": 0,\n",
    "            \"errors\": len(df),\n",
    "            \"time_seconds\": round(elapsed_time, 2),\n",
    "            \"docs_per_second\": 0,\n",
    "            \"index_size_mb\": 0,\n",
    "            \"indexed_docs\": 0,\n",
    "            \"status\": f\"âŒ Failed: {str(e)[:50]}\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c159a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸  Deleted existing index: squad_standard_analyzer\n",
      "âœ¨ Created index: squad_standard_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'standard' analyzer...\n",
      "   Index: squad_standard_analyzer\n",
      "   Documents: 87,599\n",
      "   â±ï¸  Time: 9.54s | ğŸ“Š Throughput: 9178.4 docs/s\n",
      "   ğŸ’¾ Index Size: 104.04 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_english_analyzer\n",
      "   â±ï¸  Time: 9.54s | ğŸ“Š Throughput: 9178.4 docs/s\n",
      "   ğŸ’¾ Index Size: 104.04 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_english_analyzer\n",
      "âœ¨ Created index: squad_english_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'english' analyzer...\n",
      "   Index: squad_english_analyzer\n",
      "   Documents: 87,599\n",
      "âœ¨ Created index: squad_english_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'english' analyzer...\n",
      "   Index: squad_english_analyzer\n",
      "   Documents: 87,599\n",
      "   â±ï¸  Time: 10.34s | ğŸ“Š Throughput: 8470.54 docs/s\n",
      "   ğŸ’¾ Index Size: 98.38 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_keyword_analyzer\n",
      "   â±ï¸  Time: 10.34s | ğŸ“Š Throughput: 8470.54 docs/s\n",
      "   ğŸ’¾ Index Size: 98.38 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_keyword_analyzer\n",
      "âœ¨ Created index: squad_keyword_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'keyword' analyzer...\n",
      "   Index: squad_keyword_analyzer\n",
      "   Documents: 87,599\n",
      "âœ¨ Created index: squad_keyword_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'keyword' analyzer...\n",
      "   Index: squad_keyword_analyzer\n",
      "   Documents: 87,599\n",
      "   â±ï¸  Time: 6.51s | ğŸ“Š Throughput: 13451.89 docs/s\n",
      "   ğŸ’¾ Index Size: 84.6 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_whitespace_analyzer\n",
      "   â±ï¸  Time: 6.51s | ğŸ“Š Throughput: 13451.89 docs/s\n",
      "   ğŸ’¾ Index Size: 84.6 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_whitespace_analyzer\n",
      "âœ¨ Created index: squad_whitespace_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'whitespace' analyzer...\n",
      "   Index: squad_whitespace_analyzer\n",
      "   Documents: 87,599\n",
      "âœ¨ Created index: squad_whitespace_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'whitespace' analyzer...\n",
      "   Index: squad_whitespace_analyzer\n",
      "   Documents: 87,599\n",
      "   â±ï¸  Time: 9.09s | ğŸ“Š Throughput: 9639.35 docs/s\n",
      "   ğŸ’¾ Index Size: 109.64 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_simple_analyzer\n",
      "âœ¨ Created index: squad_simple_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'simple' analyzer...\n",
      "   Index: squad_simple_analyzer\n",
      "   Documents: 87,599\n",
      "   â±ï¸  Time: 9.09s | ğŸ“Š Throughput: 9639.35 docs/s\n",
      "   ğŸ’¾ Index Size: 109.64 MB | âœ… Success\n",
      "ğŸ—‘ï¸  Deleted existing index: squad_simple_analyzer\n",
      "âœ¨ Created index: squad_simple_analyzer\n",
      "\n",
      "ğŸš€ Starting bulk ingestion for 'simple' analyzer...\n",
      "   Index: squad_simple_analyzer\n",
      "   Documents: 87,599\n",
      "   â±ï¸  Time: 9.03s | ğŸ“Š Throughput: 9698.72 docs/s\n",
      "   ğŸ’¾ Index Size: 36.58 MB | âœ… Success\n",
      "\n",
      "================================================================================\n",
      "âœ… All indices created and data ingested!\n",
      "================================================================================\n",
      "   â±ï¸  Time: 9.03s | ğŸ“Š Throughput: 9698.72 docs/s\n",
      "   ğŸ’¾ Index Size: 36.58 MB | âœ… Success\n",
      "\n",
      "================================================================================\n",
      "âœ… All indices created and data ingested!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create indices and perform bulk ingestion for each analyzer\n",
    "ingestion_results = []\n",
    "\n",
    "for analyzer_name, config in analyzer_configs.items():\n",
    "    index_name = f\"squad_{analyzer_name}_analyzer\"\n",
    "    \n",
    "    # Delete index if exists\n",
    "    if os_client.indices.exists(index=index_name):\n",
    "        os_client.indices.delete(index=index_name)\n",
    "        print(f\"ğŸ—‘ï¸  Deleted existing index: {index_name}\")\n",
    "    \n",
    "    # Create index with specific analyzer settings\n",
    "    os_client.indices.create(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"settings\": config[\"settings\"],\n",
    "            \"mappings\": config[\"mappings\"]\n",
    "        }\n",
    "    )\n",
    "    print(f\"âœ¨ Created index: {index_name}\")\n",
    "    \n",
    "    # Perform bulk ingestion with metrics\n",
    "    result = bulk_ingest_with_metrics(os_client, df_squad_train, index_name, analyzer_name)\n",
    "    ingestion_results.append(result)\n",
    "    \n",
    "    print(f\"   â±ï¸  Time: {result['time_seconds']}s | ğŸ“Š Throughput: {result['docs_per_second']} docs/s\")\n",
    "    print(f\"   ğŸ’¾ Index Size: {result['index_size_mb']} MB | {result['status']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… All indices created and data ingested!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef284e",
   "metadata": {},
   "source": [
    "## ğŸ“Š Table 1: Ingestion Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "754fee7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .ingestion-table {\n",
       "        width: 100%;\n",
       "        border-collapse: collapse;\n",
       "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "        box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "        margin: 20px 0;\n",
       "    }\n",
       "    .ingestion-table th {\n",
       "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "        color: white;\n",
       "        padding: 12px;\n",
       "        text-align: left;\n",
       "        font-weight: 600;\n",
       "        font-size: 14px;\n",
       "    }\n",
       "    .ingestion-table td {\n",
       "        padding: 10px 12px;\n",
       "        border-bottom: 1px solid #e0e0e0;\n",
       "        font-size: 13px;\n",
       "    }\n",
       "    .ingestion-table tr:hover {\n",
       "        background-color: #f5f5f5;\n",
       "    }\n",
       "    .ingestion-table tr:nth-child(even) {\n",
       "        background-color: #fafafa;\n",
       "    }\n",
       "</style>\n",
       "<div class=\"ingestion-table\">\n",
       "    <table class=\"ingestion-table\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ğŸ† Rank</th>\n",
       "      <th>ğŸ”§ Analyzer</th>\n",
       "      <th>ğŸ“„ Total Docs</th>\n",
       "      <th>âœ… Success</th>\n",
       "      <th>âŒ Errors</th>\n",
       "      <th>â±ï¸ Time (s)</th>\n",
       "      <th>ğŸ“Š Throughput (docs/s)</th>\n",
       "      <th>ğŸ’¾ Size (MB)</th>\n",
       "      <th>ğŸ“Œ Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keyword</td>\n",
       "      <td>87599</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.51</td>\n",
       "      <td>13451.89</td>\n",
       "      <td>84.60</td>\n",
       "      <td>âœ… Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>simple</td>\n",
       "      <td>87599</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9698.72</td>\n",
       "      <td>36.58</td>\n",
       "      <td>âœ… Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>whitespace</td>\n",
       "      <td>87599</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.09</td>\n",
       "      <td>9639.35</td>\n",
       "      <td>109.64</td>\n",
       "      <td>âœ… Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>standard</td>\n",
       "      <td>87599</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.54</td>\n",
       "      <td>9178.40</td>\n",
       "      <td>104.04</td>\n",
       "      <td>âœ… Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>english</td>\n",
       "      <td>87599</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10.34</td>\n",
       "      <td>8470.54</td>\n",
       "      <td>98.38</td>\n",
       "      <td>âœ… Success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ INGESTION PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "ğŸ† Fastest: KEYWORD (13451.89 docs/s)\n",
      "ğŸŒ Slowest: ENGLISH (8470.54 docs/s)\n",
      "ğŸ“Š Average Throughput: 10087.78 docs/s\n",
      "ğŸ’¾ Average Index Size: 86.65 MB\n",
      "â±ï¸ Total Ingestion Time: 44.51s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive comparison table for ingestion\n",
    "df_ingestion = pd.DataFrame(ingestion_results)\n",
    "\n",
    "# Sort by throughput (docs per second) descending\n",
    "df_ingestion_sorted = df_ingestion.sort_values('docs_per_second', ascending=False)\n",
    "\n",
    "# Create a styled display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add ranking\n",
    "df_ingestion_sorted['rank'] = range(1, len(df_ingestion_sorted) + 1)\n",
    "\n",
    "# Reorder columns for better presentation\n",
    "df_display = df_ingestion_sorted[[\n",
    "    'rank', 'analyzer', 'total_docs', 'success', 'errors',\n",
    "    'time_seconds', 'docs_per_second', 'index_size_mb', 'status'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for better readability\n",
    "df_display.columns = [\n",
    "    'ğŸ† Rank', 'ğŸ”§ Analyzer', 'ğŸ“„ Total Docs', 'âœ… Success', 'âŒ Errors',\n",
    "    'â±ï¸ Time (s)', 'ğŸ“Š Throughput (docs/s)', 'ğŸ’¾ Size (MB)', 'ğŸ“Œ Status'\n",
    "]\n",
    "\n",
    "# Create HTML table with styling\n",
    "html_table = df_display.to_html(index=False, escape=False, border=0)\n",
    "\n",
    "# Add custom CSS styling\n",
    "styled_html = f\"\"\"\n",
    "<style>\n",
    "    .ingestion-table {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "        margin: 20px 0;\n",
    "    }}\n",
    "    .ingestion-table th {{\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        padding: 12px;\n",
    "        text-align: left;\n",
    "        font-weight: 600;\n",
    "        font-size: 14px;\n",
    "    }}\n",
    "    .ingestion-table td {{\n",
    "        padding: 10px 12px;\n",
    "        border-bottom: 1px solid #e0e0e0;\n",
    "        font-size: 13px;\n",
    "    }}\n",
    "    .ingestion-table tr:hover {{\n",
    "        background-color: #f5f5f5;\n",
    "    }}\n",
    "    .ingestion-table tr:nth-child(even) {{\n",
    "        background-color: #fafafa;\n",
    "    }}\n",
    "</style>\n",
    "<div class=\"ingestion-table\">\n",
    "    {html_table.replace('<table', '<table class=\"ingestion-table\"')}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(styled_html))\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ INGESTION PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ğŸ† Fastest: {df_ingestion_sorted.iloc[0]['analyzer'].upper()} \"\n",
    "      f\"({df_ingestion_sorted.iloc[0]['docs_per_second']} docs/s)\")\n",
    "print(f\"ğŸŒ Slowest: {df_ingestion_sorted.iloc[-1]['analyzer'].upper()} \"\n",
    "      f\"({df_ingestion_sorted.iloc[-1]['docs_per_second']} docs/s)\")\n",
    "print(f\"ğŸ“Š Average Throughput: {df_ingestion_sorted['docs_per_second'].mean():.2f} docs/s\")\n",
    "print(f\"ğŸ’¾ Average Index Size: {df_ingestion_sorted['index_size_mb'].mean():.2f} MB\")\n",
    "print(f\"â±ï¸ Total Ingestion Time: {df_ingestion_sorted['time_seconds'].sum():.2f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a20b86",
   "metadata": {},
   "source": [
    "# ğŸ” Phase 2: Query Performance Comparison\n",
    "\n",
    "Now let's test how different analyzers perform for searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a9f773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Query execution functions defined\n",
      "\n",
      "ğŸ“ Sample Queries:\n",
      "  1. Simple Match Query: 'What is the capital of France?'  [match]\n",
      "  2. Multi-field Search: 'machine learning algorithms'  [multi_match]\n",
      "  3. Phrase Match: 'neural network'  [match_phrase]\n",
      "  4. Context Search: 'Super Bowl'  [match]\n"
     ]
    }
   ],
   "source": [
    "# Define sample search queries\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"name\": \"Simple Match Query\",\n",
    "        \"type\": \"match\",\n",
    "        \"query_text\": \"What is the capital of France?\",\n",
    "        \"field\": \"question\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Multi-field Search\",\n",
    "        \"type\": \"multi_match\",\n",
    "        \"query_text\": \"machine learning algorithms\",\n",
    "        \"fields\": [\"title\", \"context\", \"question\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Phrase Match\",\n",
    "        \"type\": \"match_phrase\",\n",
    "        \"query_text\": \"neural network\",\n",
    "        \"field\": \"context\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Context Search\",\n",
    "        \"type\": \"match\",\n",
    "        \"query_text\": \"Super Bowl\",\n",
    "        \"field\": \"context\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to execute query and measure performance\n",
    "def execute_search_with_metrics(os_client, index_name, query_config):\n",
    "    \"\"\"\n",
    "    Execute a search query and measure performance\n",
    "    \"\"\"\n",
    "    # Build query based on type\n",
    "    if query_config[\"type\"] == \"match\":\n",
    "        query_body = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    query_config[\"field\"]: query_config[\"query_text\"]\n",
    "                }\n",
    "            },\n",
    "            \"size\": 10\n",
    "        }\n",
    "    elif query_config[\"type\"] == \"multi_match\":\n",
    "        query_body = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query_config[\"query_text\"],\n",
    "                    \"fields\": query_config[\"fields\"]\n",
    "                }\n",
    "            },\n",
    "            \"size\": 10\n",
    "        }\n",
    "    elif query_config[\"type\"] == \"match_phrase\":\n",
    "        query_body = {\n",
    "            \"query\": {\n",
    "                \"match_phrase\": {\n",
    "                    query_config[\"field\"]: query_config[\"query_text\"]\n",
    "                }\n",
    "            },\n",
    "            \"size\": 10\n",
    "        }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = os_client.search(index=index_name, body=query_body)\n",
    "        elapsed_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "        \n",
    "        hits = response['hits']['hits']\n",
    "        total_hits = response['hits']['total']['value']\n",
    "        max_score = response['hits']['max_score'] if hits else 0\n",
    "        \n",
    "        return {\n",
    "            \"latency_ms\": round(elapsed_time, 2),\n",
    "            \"total_hits\": total_hits,\n",
    "            \"returned_hits\": len(hits),\n",
    "            \"max_score\": round(max_score, 4) if max_score else 0,\n",
    "            \"status\": \"âœ… Success\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        elapsed_time = (time.time() - start_time) * 1000\n",
    "        return {\n",
    "            \"latency_ms\": round(elapsed_time, 2),\n",
    "            \"total_hits\": 0,\n",
    "            \"returned_hits\": 0,\n",
    "            \"max_score\": 0,\n",
    "            \"status\": f\"âŒ Error: {str(e)[:30]}\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Query execution functions defined\")\n",
    "print(\"\\nğŸ“ Sample Queries:\")\n",
    "for i, q in enumerate(sample_queries, 1):\n",
    "    print(f\"  {i}. {q['name']}: '{q['query_text']}'  [{q['type']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29ae9678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” Executing: Simple Match Query\n",
      "   Query: 'What is the capital of France?'\n",
      "================================================================================\n",
      "  ğŸ“Š standard     | â±ï¸  59.06ms | ğŸ“„ 10000 hits | â­ 6.2091 score\n",
      "  ğŸ“Š english      | â±ï¸  18.39ms | ğŸ“„ 10000 hits | â­ 5.7593 score\n",
      "  ğŸ“Š keyword      | â±ï¸   3.64ms | ğŸ“„     0 hits | â­ 0.0000 score\n",
      "  ğŸ“Š whitespace   | â±ï¸  13.09ms | ğŸ“„ 10000 hits | â­ 5.6961 score\n",
      "  ğŸ“Š simple       | â±ï¸   6.88ms | ğŸ“„ 10000 hits | â­ 6.1711 score\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Executing: Multi-field Search\n",
      "   Query: 'machine learning algorithms'\n",
      "================================================================================\n",
      "  ğŸ“Š standard     | â±ï¸  20.66ms | ğŸ“„  1489 hits | â­ 7.5463 score\n",
      "  ğŸ“Š english      | â±ï¸   7.47ms | ğŸ“„  2504 hits | â­ 7.5365 score\n",
      "  ğŸ“Š keyword      | â±ï¸   5.66ms | ğŸ“„     0 hits | â­ 0.0000 score\n",
      "  ğŸ“Š whitespace   | â±ï¸   5.52ms | ğŸ“„   955 hits | â­ 6.1530 score\n",
      "  ğŸ“Š simple       | â±ï¸   5.85ms | ğŸ“„  1528 hits | â­ 7.5252 score\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Executing: Phrase Match\n",
      "   Query: 'neural network'\n",
      "================================================================================\n",
      "  ğŸ“Š standard     | â±ï¸  15.41ms | ğŸ“„     4 hits | â­ 4.5246 score\n",
      "  ğŸ“Š english      | â±ï¸   3.92ms | ğŸ“„    14 hits | â­ 5.6011 score\n",
      "  ğŸ“Š keyword      | â±ï¸   2.29ms | ğŸ“„     0 hits | â­ 0.0000 score\n",
      "  ğŸ“Š whitespace   | â±ï¸   3.51ms | ğŸ“„     4 hits | â­ 4.6800 score\n",
      "  ğŸ“Š simple       | â±ï¸   4.15ms | ğŸ“„     4 hits | â­ 4.4820 score\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Executing: Context Search\n",
      "   Query: 'Super Bowl'\n",
      "================================================================================\n",
      "  ğŸ“Š standard     | â±ï¸   6.63ms | ğŸ“„   642 hits | â­ 8.4817 score\n",
      "  ğŸ“Š english      | â±ï¸   5.92ms | ğŸ“„   668 hits | â­ 8.2940 score\n",
      "  ğŸ“Š keyword      | â±ï¸   3.69ms | ğŸ“„     0 hits | â­ 0.0000 score\n",
      "  ğŸ“Š whitespace   | â±ï¸   5.22ms | ğŸ“„   441 hits | â­ 9.0142 score\n",
      "  ğŸ“Š simple       | â±ï¸   4.13ms | ğŸ“„   642 hits | â­ 8.6555 score\n",
      "\n",
      "================================================================================\n",
      "âœ… All queries executed!\n",
      "================================================================================\n",
      "  ğŸ“Š simple       | â±ï¸   4.13ms | ğŸ“„   642 hits | â­ 8.6555 score\n",
      "\n",
      "================================================================================\n",
      "âœ… All queries executed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute queries across all analyzers\n",
    "query_results = []\n",
    "\n",
    "for query_config in sample_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ” Executing: {query_config['name']}\")\n",
    "    print(f\"   Query: '{query_config['query_text']}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for analyzer_name in analyzer_configs.keys():\n",
    "        index_name = f\"squad_{analyzer_name}_analyzer\"\n",
    "        \n",
    "        result = execute_search_with_metrics(os_client, index_name, query_config)\n",
    "        \n",
    "        query_results.append({\n",
    "            \"query_name\": query_config['name'],\n",
    "            \"query_text\": query_config['query_text'],\n",
    "            \"query_type\": query_config['type'],\n",
    "            \"analyzer\": analyzer_name,\n",
    "            \"index\": index_name,\n",
    "            **result\n",
    "        })\n",
    "        \n",
    "        print(f\"  ğŸ“Š {analyzer_name:12s} | \"\n",
    "              f\"â±ï¸ {result['latency_ms']:6.2f}ms | \"\n",
    "              f\"ğŸ“„ {result['total_hits']:5d} hits | \"\n",
    "              f\"â­ {result['max_score']:.4f} score\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… All queries executed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df50879",
   "metadata": {},
   "source": [
    "## ğŸ“Š Table 2: Query Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "082add4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "ğŸ“Š QUERY PERFORMANCE BY ANALYZER\n",
      "========================================================================================================================\n",
      "\n",
      "ğŸ” Simple Match Query\n",
      "   Query: 'What is the capital of France?'\n",
      "   Type: match\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " ğŸ†   Analyzer  Latency (ms)  Total Hits  Max Score    Status\n",
      " 1    keyword          3.64           0     0.0000 âœ… Success\n",
      " 2     simple          6.88       10000     6.1711 âœ… Success\n",
      " 3 whitespace         13.09       10000     5.6961 âœ… Success\n",
      " 4    english         18.39       10000     5.7593 âœ… Success\n",
      " 5   standard         59.06       10000     6.2091 âœ… Success\n",
      "\n",
      "   ğŸ† Winner: KEYWORD - 3.64ms latency\n",
      "\n",
      "\n",
      "ğŸ” Multi-field Search\n",
      "   Query: 'machine learning algorithms'\n",
      "   Type: multi_match\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " ğŸ†   Analyzer  Latency (ms)  Total Hits  Max Score    Status\n",
      " 1 whitespace          5.52         955     6.1530 âœ… Success\n",
      " 2    keyword          5.66           0     0.0000 âœ… Success\n",
      " 3     simple          5.85        1528     7.5252 âœ… Success\n",
      " 4    english          7.47        2504     7.5365 âœ… Success\n",
      " 5   standard         20.66        1489     7.5463 âœ… Success\n",
      "\n",
      "   ğŸ† Winner: WHITESPACE - 5.52ms latency\n",
      "\n",
      "\n",
      "ğŸ” Phrase Match\n",
      "   Query: 'neural network'\n",
      "   Type: match_phrase\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " ğŸ†   Analyzer  Latency (ms)  Total Hits  Max Score    Status\n",
      " 1    keyword          2.29           0     0.0000 âœ… Success\n",
      " 2 whitespace          3.51           4     4.6800 âœ… Success\n",
      " 3    english          3.92          14     5.6011 âœ… Success\n",
      " 4     simple          4.15           4     4.4820 âœ… Success\n",
      " 5   standard         15.41           4     4.5246 âœ… Success\n",
      "\n",
      "   ğŸ† Winner: KEYWORD - 2.29ms latency\n",
      "\n",
      "\n",
      "ğŸ” Context Search\n",
      "   Query: 'Super Bowl'\n",
      "   Type: match\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " ğŸ†   Analyzer  Latency (ms)  Total Hits  Max Score    Status\n",
      " 1    keyword          3.69           0     0.0000 âœ… Success\n",
      " 2     simple          4.13         642     8.6555 âœ… Success\n",
      " 3 whitespace          5.22         441     9.0142 âœ… Success\n",
      " 4    english          5.92         668     8.2940 âœ… Success\n",
      " 5   standard          6.63         642     8.4817 âœ… Success\n",
      "\n",
      "   ğŸ† Winner: KEYWORD - 3.69ms latency\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "ğŸ“ˆ SUMMARY STATISTICS BY ANALYZER\n",
      "========================================================================================================================\n",
      "            rank  Avg Latency  Min Latency  Max Latency  Std Dev  Avg Hits  Avg Score\n",
      "analyzer                                                                             \n",
      "keyword        1         3.82         2.29         5.66     1.39      0.00       0.00\n",
      "simple         2         5.25         4.13         6.88     1.35   3043.50       6.71\n",
      "whitespace     3         6.84         3.51        13.09     4.26   2850.00       6.39\n",
      "english        4         8.93         3.92        18.39     6.48   3296.50       6.80\n",
      "standard       5        25.44         6.63        59.06    23.15   3033.75       6.69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .query-table {\n",
       "        width: 100%;\n",
       "        border-collapse: collapse;\n",
       "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "        box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "        margin: 20px 0;\n",
       "    }\n",
       "    .query-table th {\n",
       "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
       "        color: white;\n",
       "        padding: 12px;\n",
       "        text-align: left;\n",
       "        font-weight: 600;\n",
       "        font-size: 14px;\n",
       "    }\n",
       "    .query-table td {\n",
       "        padding: 10px 12px;\n",
       "        border-bottom: 1px solid #e0e0e0;\n",
       "        font-size: 13px;\n",
       "    }\n",
       "    .query-table tr:hover {\n",
       "        background-color: #fff5f5;\n",
       "    }\n",
       "    .query-table tr:nth-child(even) {\n",
       "        background-color: #fef5f5;\n",
       "    }\n",
       "</style>\n",
       "<div class=\"query-table\">\n",
       "    <h3>ğŸ† Query Performance Leaderboard</h3>\n",
       "    <table class=\"query-table\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>Min Latency</th>\n",
       "      <th>Max Latency</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Avg Hits</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyzer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>1</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.66</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>2</td>\n",
       "      <td>5.25</td>\n",
       "      <td>4.13</td>\n",
       "      <td>6.88</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3043.50</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitespace</th>\n",
       "      <td>3</td>\n",
       "      <td>6.84</td>\n",
       "      <td>3.51</td>\n",
       "      <td>13.09</td>\n",
       "      <td>4.26</td>\n",
       "      <td>2850.00</td>\n",
       "      <td>6.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english</th>\n",
       "      <td>4</td>\n",
       "      <td>8.93</td>\n",
       "      <td>3.92</td>\n",
       "      <td>18.39</td>\n",
       "      <td>6.48</td>\n",
       "      <td>3296.50</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>5</td>\n",
       "      <td>25.44</td>\n",
       "      <td>6.63</td>\n",
       "      <td>59.06</td>\n",
       "      <td>23.15</td>\n",
       "      <td>3033.75</td>\n",
       "      <td>6.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "ğŸ† Overall Fastest Analyzer: KEYWORD (Avg: 3.82ms)\n",
      "â­ Best Relevance Scores: ENGLISH (Avg Score: 6.8000)\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive query comparison table\n",
    "df_queries = pd.DataFrame(query_results)\n",
    "\n",
    "# Create pivot table for better visualization\n",
    "df_query_pivot = df_queries.pivot_table(\n",
    "    index=['query_name', 'query_text'],\n",
    "    columns='analyzer',\n",
    "    values=['latency_ms', 'total_hits', 'max_score'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Display detailed results grouped by query\n",
    "print(\"=\"*120)\n",
    "print(\"ğŸ“Š QUERY PERFORMANCE BY ANALYZER\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "for query_name in df_queries['query_name'].unique():\n",
    "    query_data = df_queries[df_queries['query_name'] == query_name]\n",
    "    \n",
    "    print(f\"\\nğŸ” {query_name}\")\n",
    "    print(f\"   Query: '{query_data.iloc[0]['query_text']}'\")\n",
    "    print(f\"   Type: {query_data.iloc[0]['query_type']}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Create display dataframe\n",
    "    display_df = query_data[['analyzer', 'latency_ms', 'total_hits', 'max_score', 'status']].copy()\n",
    "    display_df = display_df.sort_values('latency_ms')\n",
    "    display_df['rank'] = range(1, len(display_df) + 1)\n",
    "    \n",
    "    display_df = display_df[['rank', 'analyzer', 'latency_ms', 'total_hits', 'max_score', 'status']]\n",
    "    display_df.columns = ['ğŸ†', 'Analyzer', 'Latency (ms)', 'Total Hits', 'Max Score', 'Status']\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Print winner\n",
    "    fastest = display_df.iloc[0]\n",
    "    print(f\"\\n   ğŸ† Winner: {fastest['Analyzer'].upper()} - {fastest['Latency (ms)']}ms latency\")\n",
    "    print()\n",
    "\n",
    "# Create summary statistics table\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ“ˆ SUMMARY STATISTICS BY ANALYZER\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "summary_stats = df_queries.groupby('analyzer').agg({\n",
    "    'latency_ms': ['mean', 'min', 'max', 'std'],\n",
    "    'total_hits': 'mean',\n",
    "    'max_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "summary_stats.columns = ['Avg Latency', 'Min Latency', 'Max Latency', 'Std Dev', 'Avg Hits', 'Avg Score']\n",
    "summary_stats = summary_stats.sort_values('Avg Latency')\n",
    "summary_stats['rank'] = range(1, len(summary_stats) + 1)\n",
    "summary_stats = summary_stats[['rank', 'Avg Latency', 'Min Latency', 'Max Latency', 'Std Dev', 'Avg Hits', 'Avg Score']]\n",
    "\n",
    "print(summary_stats.to_string())\n",
    "\n",
    "# Create styled HTML table\n",
    "html_summary = f\"\"\"\n",
    "<style>\n",
    "    .query-table {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "        margin: 20px 0;\n",
    "    }}\n",
    "    .query-table th {{\n",
    "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
    "        color: white;\n",
    "        padding: 12px;\n",
    "        text-align: left;\n",
    "        font-weight: 600;\n",
    "        font-size: 14px;\n",
    "    }}\n",
    "    .query-table td {{\n",
    "        padding: 10px 12px;\n",
    "        border-bottom: 1px solid #e0e0e0;\n",
    "        font-size: 13px;\n",
    "    }}\n",
    "    .query-table tr:hover {{\n",
    "        background-color: #fff5f5;\n",
    "    }}\n",
    "    .query-table tr:nth-child(even) {{\n",
    "        background-color: #fef5f5;\n",
    "    }}\n",
    "</style>\n",
    "<div class=\"query-table\">\n",
    "    <h3>ğŸ† Query Performance Leaderboard</h3>\n",
    "    {summary_stats.to_html(index=True, escape=False).replace('<table', '<table class=\"query-table\"')}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_summary))\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"ğŸ† Overall Fastest Analyzer: {summary_stats.index[0].upper()} \"\n",
    "      f\"(Avg: {summary_stats.iloc[0]['Avg Latency']:.2f}ms)\")\n",
    "print(f\"â­ Best Relevance Scores: {summary_stats['Avg Score'].idxmax().upper()} \"\n",
    "      f\"(Avg Score: {summary_stats['Avg Score'].max():.4f})\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fec9c",
   "metadata": {},
   "source": [
    "# ğŸ§  Phase 3: Neural Search with Sentence Transformers\n",
    "\n",
    "Now we'll deploy a sentence transformer model and create vector embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed56b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¤– Registering and deploying ML model...\n",
      "================================================================================\n",
      "Model was registered successfully. Model Id:  nD4n_5kBHWwV4qjRGNg4\n",
      "nD4n_5kBHWwV4qjRGNg4\n",
      "Task ID: rtYn_5kBDLL71ClipzUm\n",
      "Model was registered successfully. Model Id:  nD4n_5kBHWwV4qjRGNg4\n",
      "nD4n_5kBHWwV4qjRGNg4\n",
      "Task ID: rtYn_5kBDLL71ClipzUm\n",
      "Model deployed successfully\n",
      "âœ… Model registered successfully!\n",
      "   Model ID: nD4n_5kBHWwV4qjRGNg4\n",
      "\n",
      "â³ Waiting for model deployment...\n",
      "   Current model state: DEPLOYED\n",
      "âœ… Model deployed successfully!\n",
      "\n",
      "================================================================================\n",
      "âœ… Model is ready for use\n",
      "   Model ID: nD4n_5kBHWwV4qjRGNg4\n",
      "================================================================================\n",
      "Model deployed successfully\n",
      "âœ… Model registered successfully!\n",
      "   Model ID: nD4n_5kBHWwV4qjRGNg4\n",
      "\n",
      "â³ Waiting for model deployment...\n",
      "   Current model state: DEPLOYED\n",
      "âœ… Model deployed successfully!\n",
      "\n",
      "================================================================================\n",
      "âœ… Model is ready for use\n",
      "   Model ID: nD4n_5kBHWwV4qjRGNg4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Register and deploy the sentence transformer model\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¤– Registering and deploying ML model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    model_response = ml_client.register_pretrained_model(\n",
    "        model_name=\"huggingface/sentence-transformers/msmarco-distilbert-base-tas-b\",\n",
    "        model_version=\"1.0.1\",\n",
    "        model_format=\"TORCH_SCRIPT\",\n",
    "        deploy_model=True,\n",
    "        wait_until_deployed=True\n",
    "    )\n",
    "    model_id = model_response\n",
    "    print(f\"âœ… Model registered successfully!\")\n",
    "    print(f\"   Model ID: {model_id}\")\n",
    "    \n",
    "    # Wait for model to be fully deployed\n",
    "    print(\"\\nâ³ Waiting for model deployment...\")\n",
    "    max_wait_time = 300  # 5 minutes max wait\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        model_info = ml_client.get_model_info(model_id)\n",
    "        model_state = model_info.get('model_state', 'UNKNOWN')\n",
    "        print(f\"   Current model state: {model_state}\")\n",
    "        \n",
    "        if model_state == 'DEPLOYED':\n",
    "            print(\"âœ… Model deployed successfully!\")\n",
    "            break\n",
    "        \n",
    "        if time.time() - start_time > max_wait_time:\n",
    "            print(\"âš ï¸  Warning: Model deployment timeout. Proceeding anyway...\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… Model is ready for use\")\n",
    "    print(f\"   Model ID: {model_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error with model setup: {e}\")\n",
    "    print(\"âš ï¸  You may need to manually register/deploy the model\")\n",
    "    print(\"   Continuing with a placeholder model_id...\")\n",
    "    model_id = \"PLACEHOLDER_MODEL_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee58b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¬ Generating vector embeddings for 1,000 documents\n",
      "   Fields to vectorize: title, context, question, answers.text\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Verifying model deployment...\n",
      "   Model ID: nD4n_5kBHWwV4qjRGNg4\n",
      "   Model state: DEPLOYED\n",
      "âœ… Model is deployed and ready!\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ Generating embeddings...\n",
      "   Progress: 0/1000 (0.0%) - 0.0 docs/s\n",
      "   Progress: 100/1000 (10.0%) - 7.3 docs/s\n",
      "   Progress: 100/1000 (10.0%) - 7.3 docs/s\n",
      "   Progress: 200/1000 (20.0%) - 7.3 docs/s\n",
      "   Progress: 200/1000 (20.0%) - 7.3 docs/s\n",
      "   Progress: 300/1000 (30.0%) - 7.3 docs/s\n",
      "   Progress: 300/1000 (30.0%) - 7.3 docs/s\n",
      "   Progress: 400/1000 (40.0%) - 7.4 docs/s\n",
      "   Progress: 400/1000 (40.0%) - 7.4 docs/s\n",
      "   Progress: 500/1000 (50.0%) - 7.5 docs/s\n",
      "   Progress: 500/1000 (50.0%) - 7.5 docs/s\n",
      "   Progress: 600/1000 (60.0%) - 7.5 docs/s\n",
      "   Progress: 600/1000 (60.0%) - 7.5 docs/s\n",
      "   Progress: 700/1000 (70.0%) - 7.5 docs/s\n",
      "   Progress: 700/1000 (70.0%) - 7.5 docs/s\n",
      "   Progress: 800/1000 (80.0%) - 7.6 docs/s\n",
      "   Progress: 800/1000 (80.0%) - 7.6 docs/s\n",
      "   Progress: 900/1000 (90.0%) - 7.6 docs/s\n",
      "   Progress: 900/1000 (90.0%) - 7.6 docs/s\n",
      "\n",
      "================================================================================\n",
      "âœ… Embedding generation complete!\n",
      "   Total time: 131.71s\n",
      "   Rate: 7.59 docs/s\n",
      "   Success: 1000 | Failed: 0\n",
      "   Vector dimension: 768\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "âœ… Embedding generation complete!\n",
      "   Total time: 131.71s\n",
      "   Rate: 7.59 docs/s\n",
      "   Success: 1000 | Failed: 0\n",
      "   Vector dimension: 768\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for a sample of data first (for performance)\n",
    "# Using a smaller sample for demonstration - adjust as needed\n",
    "SAMPLE_SIZE = 1000  # Adjust based on your needs\n",
    "df_sample = df_squad_train.head(SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"ğŸ§¬ Generating vector embeddings for {len(df_sample):,} documents\")\n",
    "print(f\"   Fields to vectorize: title, context, question, answers.text\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify model is ready before generating embeddings\n",
    "print(f\"\\nğŸ” Verifying model deployment...\")\n",
    "print(f\"   Model ID: {model_id}\")\n",
    "\n",
    "if model_id == \"PLACEHOLDER_MODEL_ID\":\n",
    "    print(\"âŒ Model deployment failed in previous step!\")\n",
    "    print(\"   Please re-run the model registration cell (cell 19) and ensure it completes successfully.\")\n",
    "    print(\"   The model state should be 'DEPLOYED' before continuing.\")\n",
    "    raise Exception(\"Invalid model_id. Cannot generate embeddings without a deployed model.\")\n",
    "\n",
    "try:\n",
    "    # Check model status\n",
    "    model_info = ml_client.get_model_info(model_id)\n",
    "    model_state = model_info.get('model_state', 'UNKNOWN')\n",
    "    print(f\"   Model state: {model_state}\")\n",
    "    \n",
    "    if model_state != 'DEPLOYED':\n",
    "        print(f\"âš ï¸  Warning: Model state is '{model_state}', not 'DEPLOYED'\")\n",
    "        print(\"   Embedding generation may fail. Consider re-deploying the model.\")\n",
    "    else:\n",
    "        print(\"âœ… Model is deployed and ready!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Warning: Could not verify model status: {e}\")\n",
    "    print(\"   Proceeding anyway, but embedding generation may fail.\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Function to generate embeddings using ML Commons\n",
    "def generate_embedding(text, model_id):\n",
    "    \"\"\"Generate embedding for given text using deployed model\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Check if model_id is valid\n",
    "    if model_id == \"PLACEHOLDER_MODEL_ID\" or not model_id:\n",
    "        print(f\"   âš ï¸  Invalid model_id: {model_id}. Model deployment may have failed.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Truncate text if too long (model has token limits)\n",
    "        max_chars = 500\n",
    "        text = text[:max_chars] if len(text) > max_chars else text\n",
    "        \n",
    "        # Use direct API call for more reliable embedding generation\n",
    "        request_body = {\n",
    "            \"text_docs\": [text],\n",
    "            \"return_number\": True,\n",
    "            \"target_response\": [\"sentence_embedding\"]\n",
    "        }\n",
    "        \n",
    "        response = os_client.transport.perform_request(\n",
    "            'POST',\n",
    "            f'/_plugins/_ml/models/{model_id}/_predict',\n",
    "            body=request_body\n",
    "        )\n",
    "        \n",
    "        if 'inference_results' in response:\n",
    "            embeddings = response['inference_results'][0]['output'][0]['data']\n",
    "            return embeddings\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Embedding error: {str(e)[:150]}\")\n",
    "        return None\n",
    "\n",
    "# Generate embeddings for all fields\n",
    "print(\"\\nğŸ”„ Generating embeddings...\")\n",
    "embedding_start = time.time()\n",
    "\n",
    "# Create lists to store embeddings\n",
    "title_embeddings = []\n",
    "context_embeddings = []\n",
    "question_embeddings = []\n",
    "answer_embeddings = []\n",
    "\n",
    "successful_embeddings = 0\n",
    "failed_embeddings = 0\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    if idx % 100 == 0:\n",
    "        elapsed = time.time() - embedding_start\n",
    "        rate = idx / elapsed if elapsed > 0 else 0\n",
    "        print(f\"   Progress: {idx}/{len(df_sample)} ({(idx/len(df_sample)*100):.1f}%) - {rate:.1f} docs/s\")\n",
    "    \n",
    "    # Generate embeddings for each field\n",
    "    title_emb = generate_embedding(str(row['title']), model_id)\n",
    "    context_emb = generate_embedding(str(row['context']), model_id)\n",
    "    question_emb = generate_embedding(str(row['question']), model_id)\n",
    "    \n",
    "    # For answers, use the first answer text\n",
    "    answer_text = \"\"\n",
    "    if isinstance(row['answers'], dict) and 'text' in row['answers']:\n",
    "        if isinstance(row['answers']['text'], list) and len(row['answers']['text']) > 0:\n",
    "            answer_text = str(row['answers']['text'][0])\n",
    "    answer_emb = generate_embedding(answer_text, model_id)\n",
    "    \n",
    "    if title_emb and context_emb and question_emb:\n",
    "        successful_embeddings += 1\n",
    "    else:\n",
    "        failed_embeddings += 1\n",
    "    \n",
    "    title_embeddings.append(title_emb)\n",
    "    context_embeddings.append(context_emb)\n",
    "    question_embeddings.append(question_emb)\n",
    "    answer_embeddings.append(answer_emb)\n",
    "\n",
    "# Add embeddings to dataframe\n",
    "df_sample['title_vector'] = title_embeddings\n",
    "df_sample['context_vector'] = context_embeddings\n",
    "df_sample['question_vector'] = question_embeddings\n",
    "df_sample['answer_vector'] = answer_embeddings\n",
    "\n",
    "embedding_time = time.time() - embedding_start\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Embedding generation complete!\")\n",
    "print(f\"   Total time: {embedding_time:.2f}s\")\n",
    "print(f\"   Rate: {len(df_sample)/embedding_time:.2f} docs/s\")\n",
    "print(f\"   Success: {successful_embeddings} | Failed: {failed_embeddings}\")\n",
    "print(f\"   Vector dimension: {len(title_embeddings[0]) if title_embeddings[0] else 'N/A'}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b453375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸  Deleted existing index: squad_with_vectors\n",
      "âœ¨ Created k-NN index: squad_with_vectors\n",
      "   Vector dimension: 768\n",
      "\n",
      "ğŸš€ Ingesting 1,000 documents with vector embeddings...\n",
      "\n",
      "================================================================================\n",
      "âœ… Vector ingestion complete!\n",
      "   Time: 1.21s\n",
      "   Rate: 829.36 docs/s\n",
      "   Success: 1000 | Errors: 0\n",
      "   Index size: 9.53 MB\n",
      "   Documents in index: 2,000\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "âœ… Vector ingestion complete!\n",
      "   Time: 1.21s\n",
      "   Rate: 829.36 docs/s\n",
      "   Success: 1000 | Errors: 0\n",
      "   Index size: 9.53 MB\n",
      "   Documents in index: 2,000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create index with k-NN enabled for vector search\n",
    "knn_index_name = \"squad_with_vectors\"\n",
    "\n",
    "# Delete if exists\n",
    "if os_client.indices.exists(index=knn_index_name):\n",
    "    os_client.indices.delete(index=knn_index_name)\n",
    "    print(f\"ğŸ—‘ï¸  Deleted existing index: {knn_index_name}\")\n",
    "\n",
    "# Get vector dimension from first embedding\n",
    "vector_dim = len(title_embeddings[0]) if title_embeddings[0] else 768\n",
    "\n",
    "# Create index with k-NN mappings\n",
    "# Note: Using Lucene engine (nmslib is deprecated)\n",
    "# Lucene HNSW parameters:\n",
    "#   - ef_construction: Controls index build quality/speed tradeoff (default: 100, range: 2-âˆ)\n",
    "#   - m: Max number of connections per node (default: 16, recommended: 16-32)\n",
    "knn_settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 100\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"context\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"answers\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"text\"},\n",
    "                    \"answer_start\": {\"type\": \"integer\"}\n",
    "                }\n",
    "            },\n",
    "            \"title_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": vector_dim,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"lucene\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 128,\n",
    "                        \"m\": 16\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"context_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": vector_dim,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"lucene\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 128,\n",
    "                        \"m\": 16\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": vector_dim,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"lucene\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 128,\n",
    "                        \"m\": 16\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"answer_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": vector_dim,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"lucene\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 128,\n",
    "                        \"m\": 16\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "os_client.indices.create(index=knn_index_name, body=knn_settings)\n",
    "print(f\"âœ¨ Created k-NN index: {knn_index_name}\")\n",
    "print(f\"   Vector dimension: {vector_dim}\")\n",
    "\n",
    "# Prepare bulk data with vectors\n",
    "def prepare_bulk_data_with_vectors(df, index_name):\n",
    "    \"\"\"Prepare data with vector embeddings for bulk ingestion\"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if vectors are valid (not None and not empty)\n",
    "        title_vec = row['title_vector']\n",
    "        context_vec = row['context_vector']\n",
    "        question_vec = row['question_vector']\n",
    "        answer_vec = row['answer_vector']\n",
    "        \n",
    "        # Skip if any required vector is None or empty\n",
    "        if title_vec is None or context_vec is None or question_vec is None:\n",
    "            continue\n",
    "        \n",
    "        # Additional check for list type and non-empty\n",
    "        if not isinstance(title_vec, list) or not isinstance(context_vec, list) or not isinstance(question_vec, list):\n",
    "            continue\n",
    "            \n",
    "        if len(title_vec) == 0 or len(context_vec) == 0 or len(question_vec) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build the source document\n",
    "        source_doc = {\n",
    "            \"id\": row['id'],\n",
    "            \"title\": row['title'],\n",
    "            \"context\": row['context'],\n",
    "            \"question\": row['question'],\n",
    "            \"answers\": row['answers'],\n",
    "            \"title_vector\": title_vec,\n",
    "            \"context_vector\": context_vec,\n",
    "            \"question_vector\": question_vec\n",
    "        }\n",
    "        \n",
    "        # Only include answer_vector if it's valid and non-empty\n",
    "        # OpenSearch will reject knn_vector fields with empty arrays or null values\n",
    "        if answer_vec and isinstance(answer_vec, list) and len(answer_vec) > 0:\n",
    "            source_doc[\"answer_vector\"] = answer_vec\n",
    "            \n",
    "        doc = {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row['id'],\n",
    "            \"_source\": source_doc\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "# Ingest data with vectors\n",
    "print(f\"\\nğŸš€ Ingesting {len(df_sample):,} documents with vector embeddings...\")\n",
    "\n",
    "ingest_start = time.time()\n",
    "\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "# Use helpers.bulk for efficient ingestion\n",
    "result_gen = helpers.bulk(\n",
    "    os_client,\n",
    "    prepare_bulk_data_with_vectors(df_sample, knn_index_name),\n",
    "    chunk_size=100,  # Smaller chunk size for vector data\n",
    "    request_timeout=120,\n",
    "    raise_on_error=False,\n",
    "    stats_only=True  # Use stats_only for cleaner output\n",
    ")\n",
    "\n",
    "# Result is a tuple: (success_count, errors_list or error_count)\n",
    "success_count, errors = result_gen\n",
    "if isinstance(errors, list):\n",
    "    error_count = len(errors)\n",
    "    # Print first few errors if any\n",
    "    for i, error in enumerate(errors[:5], 1):\n",
    "        print(f\"   âš ï¸  Error {i}: {error}\")\n",
    "else:\n",
    "    error_count = errors\n",
    "\n",
    "ingest_time = time.time() - ingest_start\n",
    "\n",
    "# Refresh index\n",
    "os_client.indices.refresh(index=knn_index_name)\n",
    "\n",
    "# Get stats\n",
    "stats = os_client.indices.stats(index=knn_index_name)\n",
    "index_size_bytes = stats['indices'][knn_index_name]['total']['store']['size_in_bytes']\n",
    "index_size_mb = index_size_bytes / (1024 * 1024)\n",
    "doc_count = stats['indices'][knn_index_name]['total']['docs']['count']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Vector ingestion complete!\")\n",
    "print(f\"   Time: {ingest_time:.2f}s\")\n",
    "print(f\"   Rate: {success_count/ingest_time:.2f} docs/s\")\n",
    "print(f\"   Success: {success_count} | Errors: {error_count}\")\n",
    "print(f\"   Index size: {index_size_mb:.2f} MB\")\n",
    "print(f\"   Documents in index: {doc_count:,}\")\n",
    "\n",
    "if error_count > 0:\n",
    "    print(f\"\\nâš ï¸  {error_count} errors occurred during ingestion\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c332d",
   "metadata": {},
   "source": [
    "# âš¡ Phase 4: BM25 vs Neural vs Hybrid Search Comparison\n",
    "\n",
    "Compare traditional BM25, neural k-NN search, and hybrid approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a755c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Search functions defined\n",
      "\n",
      "ğŸ“ Test queries:\n",
      "  1. Factual Question: 'What is machine learning?'\n",
      "  2. Semantic Concept: 'artificial intelligence and deep learning'\n",
      "  3. Specific Entity: 'Super Bowl 50'\n",
      "  4. Abstract Query: 'how does training work in neural networks'\n"
     ]
    }
   ],
   "source": [
    "# Define test queries for comparison\n",
    "test_search_queries = [\n",
    "    {\n",
    "        \"name\": \"Factual Question\",\n",
    "        \"text\": \"What is machine learning?\",\n",
    "        \"field\": \"question\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Semantic Concept\",\n",
    "        \"text\": \"artificial intelligence and deep learning\",\n",
    "        \"field\": \"context\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Specific Entity\",\n",
    "        \"text\": \"Super Bowl 50\",\n",
    "        \"field\": \"context\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Abstract Query\",\n",
    "        \"text\": \"how does training work in neural networks\",\n",
    "        \"field\": \"question\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to perform BM25 search\n",
    "def bm25_search(os_client, index_name, query_text, field):\n",
    "    \"\"\"Traditional BM25 text search\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = os_client.search(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        field: query_text\n",
    "                    }\n",
    "                },\n",
    "                \"size\": 10\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        hits = response['hits']['hits']\n",
    "        \n",
    "        return {\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"total_hits\": response['hits']['total']['value'],\n",
    "            \"returned_docs\": len(hits),\n",
    "            \"max_score\": round(hits[0]['_score'], 4) if hits else 0,\n",
    "            \"avg_score\": round(sum(h['_score'] for h in hits) / len(hits), 4) if hits else 0,\n",
    "            \"top_doc_id\": hits[0]['_id'] if hits else None,\n",
    "            \"status\": \"âœ…\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"latency_ms\": round((time.time() - start_time) * 1000, 2),\n",
    "            \"total_hits\": 0,\n",
    "            \"returned_docs\": 0,\n",
    "            \"max_score\": 0,\n",
    "            \"avg_score\": 0,\n",
    "            \"top_doc_id\": None,\n",
    "            \"status\": f\"âŒ {str(e)[:30]}\"\n",
    "        }\n",
    "\n",
    "# Function to perform Neural k-NN search\n",
    "def neural_search(os_client, model_id, index_name, query_text, vector_field):\n",
    "    \"\"\"Neural search using k-NN vector similarity\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = generate_embedding(query_text, model_id)\n",
    "        \n",
    "        if not query_embedding:\n",
    "            raise Exception(\"Failed to generate query embedding\")\n",
    "        \n",
    "        response = os_client.search(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"knn\": {\n",
    "                        vector_field: {\n",
    "                            \"vector\": query_embedding,\n",
    "                            \"k\": 10\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"size\": 10\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        hits = response['hits']['hits']\n",
    "        \n",
    "        return {\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"total_hits\": response['hits']['total']['value'],\n",
    "            \"returned_docs\": len(hits),\n",
    "            \"max_score\": round(hits[0]['_score'], 4) if hits else 0,\n",
    "            \"avg_score\": round(sum(h['_score'] for h in hits) / len(hits), 4) if hits else 0,\n",
    "            \"top_doc_id\": hits[0]['_id'] if hits else None,\n",
    "            \"status\": \"âœ…\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"latency_ms\": round((time.time() - start_time) * 1000, 2),\n",
    "            \"total_hits\": 0,\n",
    "            \"returned_docs\": 0,\n",
    "            \"max_score\": 0,\n",
    "            \"avg_score\": 0,\n",
    "            \"top_doc_id\": None,\n",
    "            \"status\": f\"âŒ {str(e)[:30]}\"\n",
    "        }\n",
    "\n",
    "# Function to perform Hybrid search\n",
    "def hybrid_search(os_client, model_id, index_name, query_text, text_field, vector_field):\n",
    "    \"\"\"Hybrid search combining BM25 and neural search\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = generate_embedding(query_text, model_id)\n",
    "        \n",
    "        if not query_embedding:\n",
    "            raise Exception(\"Failed to generate query embedding\")\n",
    "        \n",
    "        # Hybrid query with normalization\n",
    "        response = os_client.search(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"hybrid\": {\n",
    "                        \"queries\": [\n",
    "                            {\n",
    "                                \"match\": {\n",
    "                                    text_field: query_text\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"knn\": {\n",
    "                                    vector_field: {\n",
    "                                        \"vector\": query_embedding,\n",
    "                                        \"k\": 10\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"size\": 10\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        hits = response['hits']['hits']\n",
    "        \n",
    "        return {\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"total_hits\": response['hits']['total']['value'],\n",
    "            \"returned_docs\": len(hits),\n",
    "            \"max_score\": round(hits[0]['_score'], 4) if hits else 0,\n",
    "            \"avg_score\": round(sum(h['_score'] for h in hits) / len(hits), 4) if hits else 0,\n",
    "            \"top_doc_id\": hits[0]['_id'] if hits else None,\n",
    "            \"status\": \"âœ…\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback: if hybrid not supported, use manual combination\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        return {\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"total_hits\": 0,\n",
    "            \"returned_docs\": 0,\n",
    "            \"max_score\": 0,\n",
    "            \"avg_score\": 0,\n",
    "            \"top_doc_id\": None,\n",
    "            \"status\": f\"âš ï¸ Hybrid not supported\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Search functions defined\")\n",
    "print(\"\\nğŸ“ Test queries:\")\n",
    "for i, q in enumerate(test_search_queries, 1):\n",
    "    print(f\"  {i}. {q['name']}: '{q['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61e7860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” EXECUTING SEARCH STRATEGY COMPARISONS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Factual Question\n",
      "   Text: 'What is machine learning?'\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ BM25 Search... âœ… - 24.85ms\n",
      "  ğŸ§  Neural Search... âœ… - 80.63ms\n",
      "  âš¡ Hybrid Search... âœ… - 81.02ms\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Semantic Concept\n",
      "   Text: 'artificial intelligence and deep learning'\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ BM25 Search... âœ… - 23.66ms\n",
      "  ğŸ§  Neural Search... âœ… - 41.9ms\n",
      "  âš¡ Hybrid Search... âœ… - 65.7ms\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Specific Entity\n",
      "   Text: 'Super Bowl 50'\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ BM25 Search... âœ… - 9.88ms\n",
      "  ğŸ§  Neural Search... âœ… - 27.45ms\n",
      "  âš¡ Hybrid Search... âœ… - 38.43ms\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Abstract Query\n",
      "   Text: 'how does training work in neural networks'\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ BM25 Search... âœ… - 16.74ms\n",
      "  ğŸ§  Neural Search... âœ… - 23.66ms\n",
      "  ğŸ§  Neural Search... âœ… - 41.9ms\n",
      "  âš¡ Hybrid Search... âœ… - 65.7ms\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Specific Entity\n",
      "   Text: 'Super Bowl 50'\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ BM25 Search... âœ… - 9.88ms\n",
      "  ğŸ§  Neural Search... âœ… - 27.45ms\n",
      "  âš¡ Hybrid Search... âœ… - 38.43ms\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ Query: Abstract Query\n",
      "   Text: 'how does training work in neural networks'\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ BM25 Search... âœ… - 16.74ms\n",
      "  ğŸ§  Neural Search... âœ… - 37.02ms\n",
      "  âš¡ Hybrid Search... âœ… - 46.23ms\n",
      "\n",
      "================================================================================\n",
      "âœ… All search strategy comparisons complete!\n",
      "================================================================================\n",
      "âœ… - 37.02ms\n",
      "  âš¡ Hybrid Search... âœ… - 46.23ms\n",
      "\n",
      "================================================================================\n",
      "âœ… All search strategy comparisons complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute all three search strategies for comparison\n",
    "comparison_results = []\n",
    "\n",
    "# Use the standard analyzer index for BM25 comparison\n",
    "bm25_index = \"squad_standard_analyzer\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ” EXECUTING SEARCH STRATEGY COMPARISONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_search_queries:\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(f\"ğŸ“ Query: {query['name']}\")\n",
    "    print(f\"   Text: '{query['text']}'\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    # Determine vector field based on text field\n",
    "    vector_field_map = {\n",
    "        \"question\": \"question_vector\",\n",
    "        \"context\": \"context_vector\",\n",
    "        \"title\": \"title_vector\"\n",
    "    }\n",
    "    vector_field = vector_field_map.get(query['field'], \"question_vector\")\n",
    "    \n",
    "    # BM25 Search\n",
    "    print(\"  ğŸ¯ BM25 Search...\", end=\" \")\n",
    "    bm25_result = bm25_search(os_client, bm25_index, query['text'], query['field'])\n",
    "    print(f\"{bm25_result['status']} - {bm25_result['latency_ms']}ms\")\n",
    "    \n",
    "    # Neural Search\n",
    "    print(\"  ğŸ§  Neural Search...\", end=\" \")\n",
    "    neural_result = neural_search(os_client, model_id, knn_index_name, query['text'], vector_field)\n",
    "    print(f\"{neural_result['status']} - {neural_result['latency_ms']}ms\")\n",
    "    \n",
    "    # Hybrid Search\n",
    "    print(\"  âš¡ Hybrid Search...\", end=\" \")\n",
    "    hybrid_result = hybrid_search(os_client, model_id, knn_index_name, query['text'], query['field'], vector_field)\n",
    "    print(f\"{hybrid_result['status']} - {hybrid_result['latency_ms']}ms\")\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results.append({\n",
    "        \"query_name\": query['name'],\n",
    "        \"query_text\": query['text'],\n",
    "        \"strategy\": \"BM25\",\n",
    "        **bm25_result\n",
    "    })\n",
    "    \n",
    "    comparison_results.append({\n",
    "        \"query_name\": query['name'],\n",
    "        \"query_text\": query['text'],\n",
    "        \"strategy\": \"Neural\",\n",
    "        **neural_result\n",
    "    })\n",
    "    \n",
    "    comparison_results.append({\n",
    "        \"query_name\": query['name'],\n",
    "        \"query_text\": query['text'],\n",
    "        \"strategy\": \"Hybrid\",\n",
    "        **hybrid_result\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… All search strategy comparisons complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72638a3",
   "metadata": {},
   "source": [
    "## ğŸ“Š Table 3: BM25 vs Neural vs Hybrid Search - Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bb4b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "ğŸ“Š DETAILED COMPARISON BY QUERY\n",
      "========================================================================================================================\n",
      "\n",
      "ğŸ” Factual Question\n",
      "   Query: 'What is machine learning?'\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Strategy  Latency (ms)  Total Hits  Docs     Max Score     Avg Score Status\n",
      "    BM25         24.85       10000    10  8.711500e+00  4.927000e+00      âœ…\n",
      "  Neural         80.63          10    10  8.630000e-01  8.580000e-01      âœ…\n",
      "  Hybrid         81.02         463    10 -9.549512e+09 -1.397195e+09      âœ…\n",
      "\n",
      "   âš¡ Fastest: BM25 (24.85ms)\n",
      "   â­ Best Score: BM25 (8.7115)\n",
      "\n",
      "\n",
      "ğŸ” Semantic Concept\n",
      "   Query: 'artificial intelligence and deep learning'\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Strategy  Latency (ms)  Total Hits  Docs     Max Score     Avg Score Status\n",
      "    BM25         23.66       10000    10  9.550200e+00  6.513300e+00      âœ…\n",
      "  Neural         41.90          10    10  8.428000e-01  8.428000e-01      âœ…\n",
      "  Hybrid         65.70         974    10 -9.549512e+09 -1.397195e+09      âœ…\n",
      "\n",
      "   âš¡ Fastest: BM25 (23.66ms)\n",
      "   â­ Best Score: BM25 (9.5502)\n",
      "\n",
      "\n",
      "ğŸ” Specific Entity\n",
      "   Query: 'Super Bowl 50'\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Strategy  Latency (ms)  Total Hits  Docs     Max Score     Avg Score Status\n",
      "    BM25          9.88        2122    10  8.604200e+00  8.604200e+00      âœ…\n",
      "  Neural         27.45          10    10  8.607000e-01  8.607000e-01      âœ…\n",
      "  Hybrid         38.43         128    10 -9.549512e+09 -1.397195e+09      âœ…\n",
      "\n",
      "   âš¡ Fastest: BM25 (9.88ms)\n",
      "   â­ Best Score: BM25 (8.6042)\n",
      "\n",
      "\n",
      "ğŸ” Abstract Query\n",
      "   Query: 'how does training work in neural networks'\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Strategy  Latency (ms)  Total Hits  Docs     Max Score     Avg Score Status\n",
      "    BM25         16.74       10000    10  6.744700e+00  5.698600e+00      âœ…\n",
      "  Neural         37.02          10    10  8.596000e-01  8.544000e-01      âœ…\n",
      "  Hybrid         46.23         459    10 -9.549512e+09 -1.397195e+09      âœ…\n",
      "\n",
      "   âš¡ Fastest: BM25 (16.74ms)\n",
      "   â­ Best Score: BM25 (6.7447)\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "ğŸ“ˆ OVERALL STRATEGY PERFORMANCE\n",
      "========================================================================================================================\n",
      "          Avg Latency  Min Latency  Max Latency  Avg Total Hits  Avg Max Score     Avg Score  Avg Docs\n",
      "strategy                                                                                              \n",
      "BM25            18.78         9.88        24.85          8030.5   8.400000e+00  6.440000e+00      10.0\n",
      "Neural          46.75        27.45        80.63            10.0   8.600000e-01  8.500000e-01      10.0\n",
      "Hybrid          57.84        38.43        81.02           506.0  -9.549512e+09 -1.397195e+09      10.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .final-comparison {{\n",
       "        width: 100%;\n",
       "        border-collapse: collapse;\n",
       "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "        box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
       "        margin: 20px 0;\n",
       "    }}\n",
       "    .final-comparison th {{\n",
       "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "        color: white;\n",
       "        padding: 15px;\n",
       "        text-align: left;\n",
       "        font-weight: 600;\n",
       "        font-size: 14px;\n",
       "        border: 1px solid #5568d3;\n",
       "    }}\n",
       "    .final-comparison td {{\n",
       "        padding: 12px 15px;\n",
       "        border: 1px solid #e0e0e0;\n",
       "        font-size: 13px;\n",
       "    }}\n",
       "    .final-comparison tr:hover {{\n",
       "        background-color: #f5f7ff;\n",
       "    }}\n",
       "    .final-comparison tr:nth-child(even) {{\n",
       "        background-color: #fafbff;\n",
       "    }}\n",
       "    .strategy-bm25 {{ background-color: #e3f2fd !important; }}\n",
       "    .strategy-neural {{ background-color: #f3e5f5 !important; }}\n",
       "    .strategy-hybrid {{ background-color: #fff3e0 !important; }}\n",
       "    .recommendation {{\n",
       "        margin-top: 20px;\n",
       "        padding: 15px;\n",
       "        border-left: 4px solid #667eea;\n",
       "        background-color: #f5f7ff;\n",
       "        border-radius: 4px;\n",
       "    }}\n",
       "</style>\n",
       "\n",
       "<div class=\"final-comparison\">\n",
       "    <h2>ğŸ† Search Strategy Comparison & Recommendations</h2>\n",
       "    <table class=\"final-comparison\">\n",
       "        <thead>\n",
       "            <tr>\n",
       "                <th>Strategy</th>\n",
       "                <th>Avg Latency (ms)</th>\n",
       "                <th>Avg Max Score</th>\n",
       "                <th>ğŸ’ª Strengths</th>\n",
       "                <th>âš ï¸ Weaknesses</th>\n",
       "                <th>âœ… Best Use Cases</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "\n",
       "            <tr class=\"strategy-bm25\">\n",
       "                <td><strong>BM25</strong></td>\n",
       "                <td>18.78 ms</td>\n",
       "                <td>8.4000</td>\n",
       "                <td>Fast, keyword-focused, exact term matching</td>\n",
       "                <td>May miss semantic meaning, synonym issues</td>\n",
       "                <td>Exact match, known entities, structured queries</td>\n",
       "            </tr>\n",
       "        \n",
       "            <tr class=\"strategy-neural\">\n",
       "                <td><strong>Neural</strong></td>\n",
       "                <td>46.75 ms</td>\n",
       "                <td>0.8600</td>\n",
       "                <td>Semantic understanding, handles synonyms/paraphrasing</td>\n",
       "                <td>Slower due to embedding generation, resource intensive</td>\n",
       "                <td>Conceptual search, natural language queries, cross-lingual</td>\n",
       "            </tr>\n",
       "        \n",
       "            <tr class=\"strategy-hybrid\">\n",
       "                <td><strong>Hybrid</strong></td>\n",
       "                <td>57.84 ms</td>\n",
       "                <td>-9549512000.0000</td>\n",
       "                <td>Best of both worlds, balanced precision/recall</td>\n",
       "                <td>Most complex, may need tuning, highest latency</td>\n",
       "                <td>Production systems needing both precision and semantic understanding</td>\n",
       "            </tr>\n",
       "        \n",
       "        </tbody>\n",
       "    </table>\n",
       "</div>\n",
       "\n",
       "<div class=\"recommendation\">\n",
       "    <h3>ğŸ’¡ Key Insights & Recommendations</h3>\n",
       "    <ul>\n",
       "        <li><strong>Speed Priority:</strong> Use <strong>BM25</strong> for fastest response times and exact keyword matching</li>\n",
       "        <li><strong>Semantic Understanding:</strong> Use <strong>Neural Search</strong> when meaning matters more than exact terms</li>\n",
       "        <li><strong>Production Systems:</strong> Use <strong>Hybrid Search</strong> for best overall results combining precision and recall</li>\n",
       "        <li><strong>Cost Consideration:</strong> Neural search requires more compute resources for embedding generation</li>\n",
       "        <li><strong>Index Size:</strong> Vector embeddings significantly increase index size (see Phase 1 comparison)</li>\n",
       "    </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "ğŸ¯ FINAL RECOMMENDATIONS\n",
      "========================================================================================================================\n",
      "\n",
      "ğŸ“Š Performance Summary:\n",
      "   âš¡ Fastest: BM25 (18.78ms average)\n",
      "   â­ Best Relevance: BM25 (8.4000 average score)\n",
      "\n",
      "ğŸ’¡ Choose Based On:\n",
      "   â€¢ BM25: Simple keyword searches, known entities, minimal resources\n",
      "   â€¢ Neural: Semantic/conceptual queries, multilingual, paraphrase handling\n",
      "   â€¢ Hybrid: Production apps needing both precision and semantic understanding\n",
      "\n",
      "âš ï¸  Important Notes:\n",
      "   â€¢ Neural search requires deployed ML model (memory intensive)\n",
      "   â€¢ Vector indices are larger (embeddings for each field)\n",
      "   â€¢ Hybrid may not be available in all OpenSearch versions\n",
      "   â€¢ Consider query patterns and user needs when choosing strategy\n",
      "\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive final comparison table\n",
    "df_comparison = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(\"ğŸ“Š DETAILED COMPARISON BY QUERY\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "for query_name in df_comparison['query_name'].unique():\n",
    "    query_data = df_comparison[df_comparison['query_name'] == query_name]\n",
    "    \n",
    "    print(f\"\\nğŸ” {query_name}\")\n",
    "    print(f\"   Query: '{query_data.iloc[0]['query_text']}'\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Create display dataframe\n",
    "    display_df = query_data[['strategy', 'latency_ms', 'total_hits', 'returned_docs', 'max_score', 'avg_score', 'status']].copy()\n",
    "    display_df.columns = ['Strategy', 'Latency (ms)', 'Total Hits', 'Docs', 'Max Score', 'Avg Score', 'Status']\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Highlight winner\n",
    "    valid_results = display_df[display_df['Status'] == 'âœ…']\n",
    "    if len(valid_results) > 0:\n",
    "        fastest = valid_results.loc[valid_results['Latency (ms)'].idxmin()]\n",
    "        best_score = valid_results.loc[valid_results['Max Score'].idxmax()]\n",
    "        print(f\"\\n   âš¡ Fastest: {fastest['Strategy']} ({fastest['Latency (ms)']}ms)\")\n",
    "        print(f\"   â­ Best Score: {best_score['Strategy']} ({best_score['Max Score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics by strategy\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ“ˆ OVERALL STRATEGY PERFORMANCE\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "strategy_summary = df_comparison.groupby('strategy').agg({\n",
    "    'latency_ms': ['mean', 'min', 'max'],\n",
    "    'total_hits': 'mean',\n",
    "    'max_score': 'mean',\n",
    "    'avg_score': 'mean',\n",
    "    'returned_docs': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "strategy_summary.columns = ['Avg Latency', 'Min Latency', 'Max Latency', 'Avg Total Hits', 'Avg Max Score', 'Avg Score', 'Avg Docs']\n",
    "strategy_summary = strategy_summary.sort_values('Avg Latency')\n",
    "\n",
    "print(strategy_summary.to_string())\n",
    "\n",
    "# Create styled comparison table with recommendations\n",
    "recommendations = {\n",
    "    \"BM25\": {\n",
    "        \"strength\": \"Fast, keyword-focused, exact term matching\",\n",
    "        \"weakness\": \"May miss semantic meaning, synonym issues\",\n",
    "        \"use_case\": \"Exact match, known entities, structured queries\"\n",
    "    },\n",
    "    \"Neural\": {\n",
    "        \"strength\": \"Semantic understanding, handles synonyms/paraphrasing\",\n",
    "        \"weakness\": \"Slower due to embedding generation, resource intensive\",\n",
    "        \"use_case\": \"Conceptual search, natural language queries, cross-lingual\"\n",
    "    },\n",
    "    \"Hybrid\": {\n",
    "        \"strength\": \"Best of both worlds, balanced precision/recall\",\n",
    "        \"weakness\": \"Most complex, may need tuning, highest latency\",\n",
    "        \"use_case\": \"Production systems needing both precision and semantic understanding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create detailed comparison HTML table\n",
    "html_comparison = \"\"\"\n",
    "<style>\n",
    "    .final-comparison {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
    "        margin: 20px 0;\n",
    "    }}\n",
    "    .final-comparison th {{\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        padding: 15px;\n",
    "        text-align: left;\n",
    "        font-weight: 600;\n",
    "        font-size: 14px;\n",
    "        border: 1px solid #5568d3;\n",
    "    }}\n",
    "    .final-comparison td {{\n",
    "        padding: 12px 15px;\n",
    "        border: 1px solid #e0e0e0;\n",
    "        font-size: 13px;\n",
    "    }}\n",
    "    .final-comparison tr:hover {{\n",
    "        background-color: #f5f7ff;\n",
    "    }}\n",
    "    .final-comparison tr:nth-child(even) {{\n",
    "        background-color: #fafbff;\n",
    "    }}\n",
    "    .strategy-bm25 {{ background-color: #e3f2fd !important; }}\n",
    "    .strategy-neural {{ background-color: #f3e5f5 !important; }}\n",
    "    .strategy-hybrid {{ background-color: #fff3e0 !important; }}\n",
    "    .recommendation {{\n",
    "        margin-top: 20px;\n",
    "        padding: 15px;\n",
    "        border-left: 4px solid #667eea;\n",
    "        background-color: #f5f7ff;\n",
    "        border-radius: 4px;\n",
    "    }}\n",
    "</style>\n",
    "\n",
    "<div class=\"final-comparison\">\n",
    "    <h2>ğŸ† Search Strategy Comparison & Recommendations</h2>\n",
    "    <table class=\"final-comparison\">\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Strategy</th>\n",
    "                <th>Avg Latency (ms)</th>\n",
    "                <th>Avg Max Score</th>\n",
    "                <th>ğŸ’ª Strengths</th>\n",
    "                <th>âš ï¸ Weaknesses</th>\n",
    "                <th>âœ… Best Use Cases</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "\"\"\"\n",
    "\n",
    "for strategy in ['BM25', 'Neural', 'Hybrid']:\n",
    "    if strategy in strategy_summary.index:\n",
    "        stats = strategy_summary.loc[strategy]\n",
    "        rec = recommendations[strategy]\n",
    "        row_class = f\"strategy-{strategy.lower()}\"\n",
    "        \n",
    "        html_comparison += f\"\"\"\n",
    "            <tr class=\"{row_class}\">\n",
    "                <td><strong>{strategy}</strong></td>\n",
    "                <td>{stats['Avg Latency']:.2f} ms</td>\n",
    "                <td>{stats['Avg Max Score']:.4f}</td>\n",
    "                <td>{rec['strength']}</td>\n",
    "                <td>{rec['weakness']}</td>\n",
    "                <td>{rec['use_case']}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "\n",
    "html_comparison += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "</div>\n",
    "\n",
    "<div class=\"recommendation\">\n",
    "    <h3>ğŸ’¡ Key Insights & Recommendations</h3>\n",
    "    <ul>\n",
    "        <li><strong>Speed Priority:</strong> Use <strong>BM25</strong> for fastest response times and exact keyword matching</li>\n",
    "        <li><strong>Semantic Understanding:</strong> Use <strong>Neural Search</strong> when meaning matters more than exact terms</li>\n",
    "        <li><strong>Production Systems:</strong> Use <strong>Hybrid Search</strong> for best overall results combining precision and recall</li>\n",
    "        <li><strong>Cost Consideration:</strong> Neural search requires more compute resources for embedding generation</li>\n",
    "        <li><strong>Index Size:</strong> Vector embeddings significantly increase index size (see Phase 1 comparison)</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_comparison))\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ¯ FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\nğŸ“Š Performance Summary:\")\n",
    "fastest_strategy = strategy_summary.index[0]\n",
    "best_score_strategy = strategy_summary['Avg Max Score'].idxmax()\n",
    "\n",
    "print(f\"   âš¡ Fastest: {fastest_strategy} ({strategy_summary.loc[fastest_strategy, 'Avg Latency']:.2f}ms average)\")\n",
    "print(f\"   â­ Best Relevance: {best_score_strategy} ({strategy_summary.loc[best_score_strategy, 'Avg Max Score']:.4f} average score)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Choose Based On:\")\n",
    "print(\"   â€¢ BM25: Simple keyword searches, known entities, minimal resources\")\n",
    "print(\"   â€¢ Neural: Semantic/conceptual queries, multilingual, paraphrase handling\")  \n",
    "print(\"   â€¢ Hybrid: Production apps needing both precision and semantic understanding\")\n",
    "\n",
    "print(\"\\nâš ï¸  Important Notes:\")\n",
    "print(\"   â€¢ Neural search requires deployed ML model (memory intensive)\")\n",
    "print(\"   â€¢ Vector indices are larger (embeddings for each field)\")\n",
    "print(\"   â€¢ Hybrid may not be available in all OpenSearch versions\")\n",
    "print(\"   â€¢ Consider query patterns and user needs when choosing strategy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef85f09",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary & Conclusion\n",
    "\n",
    "This comprehensive analysis compared multiple approaches for indexing and searching the SQUAD dataset in OpenSearch:\n",
    "\n",
    "### ğŸ“Š Phase 1: Text Analyzers\n",
    "We tested 5 different analyzers (standard, english, keyword, whitespace, simple) and measured:\n",
    "- **Ingestion Performance**: Throughput (docs/sec), time, and index size\n",
    "- **Key Finding**: Different analyzers have similar ingestion speeds but vary in index size\n",
    "\n",
    "### ğŸ” Phase 2: Query Performance  \n",
    "We executed various query types (match, multi_match, match_phrase) across all analyzer indices:\n",
    "- **Latency Comparison**: Measured query response times\n",
    "- **Relevance Scoring**: Compared how well each analyzer returned relevant results\n",
    "- **Key Finding**: Analyzer choice significantly impacts relevance for different query types\n",
    "\n",
    "### ğŸ§  Phase 3: Neural Search\n",
    "We deployed a sentence transformer model and created vector embeddings:\n",
    "- **Model**: huggingface/sentence-transformers/msmarco-distilbert-base-tas-b\n",
    "- **Vectorized Fields**: title, context, question, answers (all except id)\n",
    "- **Trade-offs**: Slower ingestion, larger index size, but semantic understanding\n",
    "\n",
    "### âš¡ Phase 4: Search Strategy Comparison\n",
    "We compared three search approaches:\n",
    "1. **BM25 (Traditional)**: Fast lexical matching\n",
    "2. **Neural (k-NN)**: Semantic vector similarity  \n",
    "3. **Hybrid**: Combined approach\n",
    "\n",
    "### ğŸ¯ Key Takeaways\n",
    "- **Speed vs Semantics**: BM25 is fastest but neural search understands meaning\n",
    "- **Resource Requirements**: Neural search needs deployed models and more storage\n",
    "- **Production Recommendation**: Hybrid search for best overall results\n",
    "- **Use Case Matters**: Choose strategy based on query patterns and requirements\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: \n",
    "- Tune analyzer settings for specific use cases\n",
    "- Experiment with different embedding models\n",
    "- Implement re-ranking strategies\n",
    "- Add search result evaluation metrics (NDCG, MRR, MAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-search-with-opensearch-intermediate (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
