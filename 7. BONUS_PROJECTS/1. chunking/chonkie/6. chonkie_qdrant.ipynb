{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5bc3cb",
   "metadata": {},
   "source": [
    "# Chonkie + Qdrant Integration - Complete Guide\n",
    "\n",
    "This notebook demonstrates the **QdrantHandshake** in Chonkie - seamless integration with Qdrant vector database for RAG applications.\n",
    "\n",
    "## What is QdrantHandshake?\n",
    "\n",
    "QdrantHandshake provides a simple interface for:\n",
    "- ‚úÖ Storing chunked documents in Qdrant\n",
    "- ‚úÖ Automatic embedding generation\n",
    "- ‚úÖ Semantic search with natural language queries\n",
    "- ‚úÖ Integration with Chonkie's Pipeline API\n",
    "- ‚úÖ Support for both local and cloud Qdrant instances\n",
    "\n",
    "## Key Features:\n",
    "- üöÄ **In-memory Qdrant server** - No Docker required\n",
    "- üîç **Semantic search** - Natural language queries\n",
    "- üîó **Pipeline integration** - Fluent API with `.store_in()`\n",
    "- üéØ **Custom embeddings** - Use any sentence-transformers model\n",
    "- ‚òÅÔ∏è **Cloud support** - Works with Qdrant Cloud\n",
    "- üì¶ **Automatic collection management** - Creates collections automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff220025",
   "metadata": {},
   "source": [
    "## Visual Overview\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#ff6b6b','primaryTextColor':'#fff','primaryBorderColor':'#c92a2a','lineColor':'#339af0','secondaryColor':'#51cf66','tertiaryColor':'#ffd43b','background':'#f8f9fa','mainBkg':'#e3fafc','secondBkg':'#fff3bf','tertiaryBkg':'#ffe3e3','textColor':'#212529','fontSize':'16px'}}}%%\n",
    "\n",
    "graph TB\n",
    "    Start([üöÄ Chonkie + Qdrant<br/>Integration]):::startClass\n",
    "    \n",
    "    Start --> Chunker[\"üìÑ Chunker<br/>Create Chunks\"]:::chunkerClass\n",
    "    \n",
    "    Chunker --> Embeddings[\"üßÆ Embedding Model<br/>Generate Vectors\"]:::embedClass\n",
    "    \n",
    "    Embeddings --> Handshake[\"ü§ù QdrantHandshake<br/>Store & Search\"]:::handshakeClass\n",
    "    \n",
    "    Handshake --> Storage{Storage Options}:::decisionClass\n",
    "    \n",
    "    Storage -->|Local| InMemory[\"üíæ In-Memory<br/>Fast Development\"]:::localClass\n",
    "    Storage -->|Local| Disk[\"üíø Persistent<br/>Local Storage\"]:::diskClass\n",
    "    Storage -->|Cloud| QdrantCloud[\"‚òÅÔ∏è Qdrant Cloud<br/>Production\"]:::cloudClass\n",
    "    \n",
    "    InMemory --> Operations[Operations]:::opsClass\n",
    "    Disk --> Operations\n",
    "    QdrantCloud --> Operations\n",
    "    \n",
    "    Operations --> Write[\"‚úçÔ∏è Write<br/>Store Chunks\"]:::writeClass\n",
    "    Operations --> Search[\"üîç Search<br/>Query Vectors\"]:::searchClass\n",
    "    Operations --> Delete[\"üóëÔ∏è Delete<br/>Remove Data\"]:::deleteClass\n",
    "    \n",
    "    Write --> Complete([‚ú® RAG Ready]):::finalClass\n",
    "    Search --> Complete\n",
    "    Delete --> Complete\n",
    "    \n",
    "    classDef startClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "    classDef chunkerClass fill:#fa5252,stroke:#e03131,stroke-width:2px,color:#fff\n",
    "    classDef embedClass fill:#20c997,stroke:#087f5b,stroke-width:2px,color:#fff\n",
    "    classDef handshakeClass fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff\n",
    "    classDef decisionClass fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#333\n",
    "    classDef localClass fill:#7950f2,stroke:#5f3dc4,stroke-width:2px,color:#fff\n",
    "    classDef diskClass fill:#845ef7,stroke:#6741d9,stroke-width:2px,color:#fff\n",
    "    classDef cloudClass fill:#339af0,stroke:#1971c2,stroke-width:2px,color:#fff\n",
    "    classDef opsClass fill:#51cf66,stroke:#37b24d,stroke-width:2px,color:#fff\n",
    "    classDef writeClass fill:#ff922b,stroke:#e8590c,stroke-width:2px,color:#fff\n",
    "    classDef searchClass fill:#74c0fc,stroke:#1c7ed6,stroke-width:2px,color:#fff\n",
    "    classDef deleteClass fill:#ffa8a8,stroke:#fa5252,stroke-width:2px,color:#fff\n",
    "    classDef finalClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a40c92",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Chonkie with Qdrant support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e593f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chonkie with Qdrant support installed successfully!\n",
      "  QdrantHandshake: <class 'chonkie.handshakes.qdrant.QdrantHandshake'>\n",
      "  QdrantClient: <class 'qdrant_client.qdrant_client.QdrantClient'>\n"
     ]
    }
   ],
   "source": [
    "# Install chonkie with qdrant support\n",
    "# !pip install \"chonkie[qdrant]\"\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from chonkie import QdrantHandshake, SemanticChunker, Pipeline\n",
    "    from qdrant_client import QdrantClient\n",
    "    print(\"‚úÖ Chonkie with Qdrant support installed successfully!\")\n",
    "    print(f\"  QdrantHandshake: {QdrantHandshake}\")\n",
    "    print(f\"  QdrantClient: {QdrantClient}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation required: pip install 'chonkie[qdrant]'\")\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae9553",
   "metadata": {},
   "source": [
    "## Setup In-Memory Qdrant Server\n",
    "\n",
    "Start a local in-memory Qdrant instance - no Docker required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e91e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ In-Memory Qdrant Server Started!\n",
      "  Client: <qdrant_client.qdrant_client.QdrantClient object at 0x00000233F443FB60>\n",
      "  Location: In-Memory (ephemeral)\n",
      "  Perfect for: Development, testing, notebooks\n",
      "\n",
      "üí° Note: Data will be lost when the notebook kernel restarts\n",
      "   For persistent storage, use: QdrantClient(path='./qdrant_storage')\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Initialize in-memory Qdrant server\n",
    "# This runs entirely in Python - perfect for development and testing\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "print(\"‚úÖ In-Memory Qdrant Server Started!\")\n",
    "print(f\"  Client: {qdrant_client}\")\n",
    "print(f\"  Location: In-Memory (ephemeral)\")\n",
    "print(f\"  Perfect for: Development, testing, notebooks\")\n",
    "print(\"\\nüí° Note: Data will be lost when the notebook kernel restarts\")\n",
    "print(\"   For persistent storage, use: QdrantClient(path='./qdrant_storage')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1c6cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic QdrantHandshake Usage\n",
    "\n",
    "## 1. Simple Write and Search\n",
    "\n",
    "Create chunks, store them in Qdrant, and perform semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36bb1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Embedding Models for SemanticChunker:\n",
      "======================================================================\n",
      "1. minishlab/potion-base-32M\n",
      "   ‚Üí Model2Vec - Ultra-fast, lightweight (32M params)\n",
      "2. sentence-transformers/all-MiniLM-L6-v2\n",
      "   ‚Üí MiniLM - Popular, balanced speed/quality\n",
      "3. sentence-transformers/all-mpnet-base-v2\n",
      "   ‚Üí MPNet - High quality semantic search\n",
      "4. BAAI/bge-small-en-v1.5\n",
      "   ‚Üí BGE Small - Efficient, strong performance\n",
      "5. BAAI/bge-base-en-v1.5\n",
      "   ‚Üí BGE Base - Better quality, more compute\n",
      "6. BAAI/bge-large-en-v1.5\n",
      "   ‚Üí BGE Large - Top-tier quality, 1024 dimensions\n",
      "7. thenlper/gte-small\n",
      "   ‚Üí GTE - General text embeddings\n",
      "8. jinaai/jina-embeddings-v2-base-en\n",
      "   ‚Üí Jina v2 - 8K context, bilingual support\n",
      "9. nomic-ai/nomic-embed-text-v1.5\n",
      "   ‚Üí Nomic - Long context support (8K tokens)\n",
      "10. emilyalsentzer/Bio_ClinicalBERT\n",
      "   ‚Üí MedEmbed - Specialized for medical/clinical text\n",
      "11. kamalkraj/biobert-base-cased-v1.2\n",
      "   ‚Üí ClinVec - BioBERT for biomedical literature\n",
      "12. google/gecko-text-embedding\n",
      "   ‚Üí Google Gecko - Multimodal, high quality\n",
      "13. Alibaba-NLP/gte-large-en-v1.5\n",
      "   ‚Üí GTE Large - State-of-the-art, 1024 dimensions\n",
      "14. Alibaba-NLP/gte-Qwen2-7B-instruct\n",
      "   ‚Üí GTE Qwen2 - Instruction-tuned, 7B params\n",
      "   ‚Ä¢ Best quality: all-mpnet-base-v2, BAAI/bge-large-en-v1.5, Alibaba-NLP/gte-large-en-v1.5\n",
      "   ‚Ä¢ Faster models: minishlab/potion-base-32M, thenlper/gte-small\n",
      "   ‚Ä¢ Large-scale: Alibaba-NLP/gte-Qwen2-7B-instruct (7B params, instruction-tuned)\n",
      "   ‚Ä¢ Default recommended: minishlab/potion-base-32M (fast + good quality)\n",
      "   ‚Ä¢ Medical/Clinical: emilyalsentzer/Bio_ClinicalBERT, kamalkraj/biobert-base-cased-v1.2\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "## Embedding Models for Semantic Chunking\n",
    "\n",
    "# Popular embedding models optimized for semantic similarity tasks\n",
    "semantic_embedding_models = [\n",
    "    \"minishlab/potion-base-32M\",           # Model2Vec - Ultra-fast, 32M params\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",  # Popular, balanced performance\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",  # High quality, general purpose\n",
    "    \"BAAI/bge-small-en-v1.5\",               # BGE - Efficient, strong performance\n",
    "    \"BAAI/bge-base-en-v1.5\",                # BGE - Better quality, slower\n",
    "    \"BAAI/bge-large-en-v1.5\",               # BGE Large - Highest quality BGE\n",
    "    \"thenlper/gte-small\",                   # GTE - General text embeddings\n",
    "    \"jinaai/jina-embeddings-v2-base-en\",    # Jina v2 - 8K context, strong performance\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\",       # Nomic - Long context (8K tokens)\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",      # MedEmbed - Medical/clinical domain\n",
    "    \"kamalkraj/biobert-base-cased-v1.2\",    # ClinVec - Biomedical text embeddings\n",
    "    \"google/gecko-text-embedding\",          # Google Gecko - Multimodal embeddings\n",
    "    \"Alibaba-NLP/gte-large-en-v1.5\",        # GTE Large - State-of-the-art, 1024 dims\n",
    "    \"Alibaba-NLP/gte-Qwen2-7B-instruct\",    # GTE Qwen2 - Instruction-tuned, 7B params\n",
    "]\n",
    "\n",
    "print(\"üß† Embedding Models for SemanticChunker:\")\n",
    "print(\"=\" * 70)\n",
    "for idx, model in enumerate(semantic_embedding_models, 1):\n",
    "    model_descriptions = {\n",
    "        \"minishlab/potion-base-32M\": \"Model2Vec - Ultra-fast, lightweight (32M params)\",\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\": \"MiniLM - Popular, balanced speed/quality\",\n",
    "        \"sentence-transformers/all-mpnet-base-v2\": \"MPNet - High quality semantic search\",\n",
    "        \"BAAI/bge-small-en-v1.5\": \"BGE Small - Efficient, strong performance\",\n",
    "        \"BAAI/bge-base-en-v1.5\": \"BGE Base - Better quality, more compute\",\n",
    "        \"BAAI/bge-large-en-v1.5\": \"BGE Large - Top-tier quality, 1024 dimensions\",\n",
    "        \"thenlper/gte-small\": \"GTE - General text embeddings\",\n",
    "        \"jinaai/jina-embeddings-v2-base-en\": \"Jina v2 - 8K context, bilingual support\",\n",
    "        \"nomic-ai/nomic-embed-text-v1.5\": \"Nomic - Long context support (8K tokens)\",\n",
    "        \"emilyalsentzer/Bio_ClinicalBERT\": \"MedEmbed - Specialized for medical/clinical text\",\n",
    "        \"kamalkraj/biobert-base-cased-v1.2\": \"ClinVec - BioBERT for biomedical literature\",\n",
    "        \"google/gecko-text-embedding\": \"Google Gecko - Multimodal, high quality\",\n",
    "        \"Alibaba-NLP/gte-large-en-v1.5\": \"GTE Large - State-of-the-art, 1024 dimensions\",\n",
    "        \"Alibaba-NLP/gte-Qwen2-7B-instruct\": \"GTE Qwen2 - Instruction-tuned, 7B params\",\n",
    "    }\n",
    "    print(f\"{idx}. {model}\")\n",
    "    print(f\"   ‚Üí {model_descriptions[model]}\")\n",
    "\n",
    "print(\"   ‚Ä¢ Best quality: all-mpnet-base-v2, BAAI/bge-large-en-v1.5, Alibaba-NLP/gte-large-en-v1.5\")\n",
    "print(\"   ‚Ä¢ Faster models: minishlab/potion-base-32M, thenlper/gte-small\")\n",
    "print(\"   ‚Ä¢ Large-scale: Alibaba-NLP/gte-Qwen2-7B-instruct (7B params, instruction-tuned)\")\n",
    "print(\"   ‚Ä¢ Default recommended: minishlab/potion-base-32M (fast + good quality)\")\n",
    "print(\"   ‚Ä¢ Medical/Clinical: emilyalsentzer/Bio_ClinicalBERT, kamalkraj/biobert-base-cased-v1.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea78deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Select Embedding Model:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da82b92704e4e0fbb751a4d559ee1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', layout=Layout(width='500px'), options=('minishlab/potion-base-32M', 'sentence-t‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Model will auto-update on selection change\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üîÑ Updating to model: minishlab/potion-base-32M\n",
      "\n",
      "‚úÖ QdrantHandshake initialized!\n",
      "  Collection: demo_minishlab_potion_base_32M\n",
      "  Embedding model: minishlab/potion-base-32M\n",
      "\n",
      "üìÑ Created 1 semantic chunks\n",
      "‚úÖ Stored 1 chunks in Qdrant\n",
      "\n",
      "üîç Search Query: 'How do computers understand language?'\n",
      "üìä Found 1 results:\n",
      "\n",
      "1. Score: 0.6644\n",
      "   Text: Machine learning is transforming industries worldwide. \n",
      "Deep learning models can...\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from chonkie import QdrantHandshake, SemanticChunker\n",
    "\n",
    "# Create dropdown widget\n",
    "embedding_model_dropdown = widgets.Dropdown(\n",
    "    options=semantic_embedding_models,\n",
    "    value=\"minishlab/potion-base-32M\",  # Default selection\n",
    "    description='Model:',\n",
    "    style={'description_width': '60px'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "print(\"üéØ Select Embedding Model:\\n\")\n",
    "display(embedding_model_dropdown)\n",
    "print(\"\\nüí° Model will auto-update on selection change\\n\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Define function to execute when dropdown changes\n",
    "def on_model_change(change):\n",
    "    selected_model = change['new']\n",
    "    \n",
    "    print(f\"üîÑ Updating to model: {selected_model}\\n\")\n",
    "    \n",
    "    # Create unique collection name for each model to avoid embedding mismatches\n",
    "    # Replace special characters in model name for valid collection name\n",
    "    collection_name = f\"demo_{selected_model.replace('/', '_').replace('-', '_')}\"\n",
    "    \n",
    "    # Initialize handshake with the in-memory Qdrant client\n",
    "    global handshake  # Make it accessible outside the function\n",
    "    handshake = QdrantHandshake(\n",
    "        client=qdrant_client,\n",
    "        collection_name=collection_name,\n",
    "        embedding_model=selected_model\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ QdrantHandshake initialized!\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    print(f\"  Embedding model: {selected_model}\")\n",
    "    \n",
    "    # Create chunks\n",
    "    chunker = SemanticChunker(chunk_size=200, embedding_model=selected_model)\n",
    "    text = \"\"\"Machine learning is transforming industries worldwide. \n",
    "Deep learning models can recognize complex patterns in data. \n",
    "Natural language processing enables computers to understand human language. \n",
    "Computer vision allows machines to interpret visual information.\"\"\"\n",
    "    \n",
    "    chunks = chunker.chunk(text)\n",
    "    print(f\"\\nüìÑ Created {len(chunks)} semantic chunks\")\n",
    "    \n",
    "    # Write chunks to Qdrant\n",
    "    handshake.write(chunks)\n",
    "    print(f\"‚úÖ Stored {len(chunks)} chunks in Qdrant\")\n",
    "    \n",
    "    # Search for relevant chunks\n",
    "    query = \"How do computers understand language?\"\n",
    "    results = handshake.search(query=query, limit=3)\n",
    "    \n",
    "    print(f\"\\nüîç Search Query: '{query}'\")\n",
    "    print(f\"üìä Found {len(results)} results:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Score: {result['score']:.4f}\")\n",
    "        print(f\"   Text: {result['text'][:80]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Attach observer to dropdown\n",
    "embedding_model_dropdown.observe(on_model_change, names='value')\n",
    "\n",
    "# Execute once with initial value\n",
    "on_model_change({'new': embedding_model_dropdown.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90207816",
   "metadata": {},
   "source": [
    "## 2. Working with Multiple Documents\n",
    "\n",
    "Store and search across multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "370a7d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÑ python: 5 chunks\n",
      "  üìÑ javascript: 4 chunks\n",
      "  üìÑ rust: 4 chunks\n",
      "\n",
      "‚úÖ Stored 13 chunks from 3 documents\n",
      "\n",
      "üîç Testing Multiple Queries:\n",
      "\n",
      "Query: 'memory management'\n",
      "  Top result: safety and performance. It prevents memory errors ...\n",
      "\n",
      "Query: 'web development'\n",
      "  Top result: JavaScript is the programming language of the web....\n",
      "\n",
      "Query: 'code readability'\n",
      "  Top result: phasizes code readability with significant whitesp...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker\n",
    "\n",
    "# Create new collection for multiple documents\n",
    "handshake_multi = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"multi_docs\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Multiple documents on different topics\n",
    "documents = {\n",
    "    \"python\": \"Python is a high-level programming language. It emphasizes code readability with significant whitespace. Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\",\n",
    "    \n",
    "    \"javascript\": \"JavaScript is the programming language of the web. It runs in browsers and on servers via Node.js. JavaScript supports event-driven, functional, and imperative programming styles.\",\n",
    "    \n",
    "    \"rust\": \"Rust is a systems programming language focused on safety and performance. It prevents memory errors through its ownership system. Rust provides zero-cost abstractions without garbage collection.\"\n",
    "}\n",
    "\n",
    "# Chunk and store all documents\n",
    "chunker = TokenChunker(chunk_size=50)\n",
    "all_chunks = []\n",
    "\n",
    "for topic, text in documents.items():\n",
    "    chunks = chunker.chunk(text)\n",
    "    # Add metadata to identify the source\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"topic\": topic}\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"  üìÑ {topic}: {len(chunks)} chunks\")\n",
    "\n",
    "# Write all chunks\n",
    "handshake_multi.write(all_chunks)\n",
    "print(f\"\\n‚úÖ Stored {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "\n",
    "# Search across all documents\n",
    "queries = [\n",
    "    \"memory management\",\n",
    "    \"web development\",\n",
    "    \"code readability\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç Testing Multiple Queries:\\n\")\n",
    "for query in queries:\n",
    "    results = handshake_multi.search(query=query, limit=2)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  Top result: {results[0]['text'][:60]}...\")\n",
    "    if 'metadata' in results[0]:\n",
    "        print(f\"  Topic: {results[0]['metadata'].get('topic', 'unknown')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920bf6f3",
   "metadata": {},
   "source": [
    "## 3. Advanced Search with Metadata\n",
    "\n",
    "Store and retrieve metadata with chunks. Note: Built-in search filtering not available in this API version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e63b35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Filtered Search Examples:\n",
      "\n",
      "1Ô∏è‚É£ Search only Python content:\n",
      "   Query: 'programming paradigms'\n",
      "   Note: Filtering by metadata in search not available in this API version\n",
      "   Results include all topics, but metadata shows source:\n",
      "   - mperative programming styles....\n",
      "     Topic: unknown\n",
      "   - Rust is a systems programming language focused on ...\n",
      "     Topic: unknown\n",
      "\n",
      "2Ô∏è‚É£ Search only JavaScript content:\n",
      "   Query: 'programming styles'\n",
      "   - mperative programming styles....\n",
      "     Topic: unknown\n",
      "   - ional programming....\n",
      "     Topic: unknown\n",
      "\n",
      "‚úÖ Metadata stored with chunks can be used for post-search filtering\n",
      "\n",
      "‚úÖ Metadata filters enable targeted search across collections\n"
     ]
    }
   ],
   "source": [
    "# Search with metadata filters\n",
    "print(\"üéØ Filtered Search Examples:\\n\")\n",
    "\n",
    "# Search only in Python documents\n",
    "print(\"1Ô∏è‚É£ Search only Python content:\")\n",
    "query = \"programming paradigms\"\n",
    "results = handshake_multi.search(\n",
    "    query=query,\n",
    "    limit=2\n",
    ")\n",
    "print(f\"   Query: '{query}'\")\n",
    "print(f\"   Note: Filtering by metadata in search not available in this API version\")\n",
    "print(f\"   Results include all topics, but metadata shows source:\")\n",
    "for result in results:\n",
    "    print(f\"   - {result['text'][:60]}...\")\n",
    "    print(f\"     Topic: {result.get('metadata', {}).get('topic', 'unknown')}\")\n",
    "\n",
    "# Search only in JavaScript documents\n",
    "print(\"\\n2Ô∏è‚É£ Search only JavaScript content:\")\n",
    "query = \"programming styles\"\n",
    "results = handshake_multi.search(\n",
    "    query=query,\n",
    "    limit=2\n",
    ")\n",
    "print(f\"   Query: '{query}'\")\n",
    "for result in results:\n",
    "    print(f\"   - {result['text'][:60]}...\")\n",
    "    print(f\"     Topic: {result.get('metadata', {}).get('topic', 'unknown')}\")\n",
    "print(\"\\n‚úÖ Metadata stored with chunks can be used for post-search filtering\")\n",
    "print(\"\\n‚úÖ Metadata filters enable targeted search across collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e00e1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Pipeline Integration\n",
    "\n",
    "## 4. Basic Pipeline with Qdrant\n",
    "\n",
    "Use the fluent Pipeline API to process and store documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7035a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 3 sample files\n",
      "\n",
      "üîÑ Processing Pipeline:\n",
      "\n",
      "‚úÖ Pipeline complete!\n",
      "  Processed: 3 documents\n",
      "  Chunks stored in Qdrant collection 'pipeline_demo'\n",
      "\n",
      "üîç Search Results for: 'learning from data'\n",
      "  1. Score: 0.5100\n",
      "     Machine learning enables computers to learn from data without explicit...\n",
      "  2. Score: 0.3334\n",
      "     Artificial intelligence is advancing rapidly across multiple domains. ...\n",
      "  3. Score: 0.3002\n",
      "     Data science combines statistics, programming, and domain expertise. D...\n",
      "\n",
      "üßπ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "from chonkie import Pipeline\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create sample text files\n",
    "demo_dir = tempfile.mkdtemp()\n",
    "samples = {\n",
    "    \"ml_basics.txt\": \"Machine learning enables computers to learn from data without explicit programming. Supervised learning uses labeled datasets. Unsupervised learning discovers hidden patterns. Reinforcement learning learns through trial and error.\",\n",
    "    \n",
    "    \"ai_trends.txt\": \"Artificial intelligence is advancing rapidly across multiple domains. Neural networks power modern AI applications. Transformer models revolutionized natural language processing. Generative AI creates novel content from learned patterns.\",\n",
    "    \n",
    "    \"data_science.txt\": \"Data science combines statistics, programming, and domain expertise. Data preprocessing cleans and prepares raw data. Exploratory data analysis reveals insights and patterns. Predictive modeling forecasts future outcomes.\"\n",
    "}\n",
    "\n",
    "for filename, content in samples.items():\n",
    "    with open(os.path.join(demo_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"‚úÖ Created {len(samples)} sample files\\n\")\n",
    "\n",
    "# Process and store in Qdrant using Pipeline\n",
    "print(\"üîÑ Processing Pipeline:\\n\")\n",
    "\n",
    "(Pipeline()\n",
    "    .fetch_from(\"file\", dir=demo_dir, ext=[\".txt\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", chunk_size=100, threshold=0.7)\n",
    "    .store_in(\"qdrant\",\n",
    "              client=qdrant_client,\n",
    "              collection_name=\"pipeline_demo\",\n",
    "              embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Pipeline complete!\")\n",
    "print(f\"  Processed: {len(samples)} documents\")\n",
    "print(f\"  Chunks stored in Qdrant collection 'pipeline_demo'\")\n",
    "\n",
    "# Search the stored data\n",
    "handshake_pipeline = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"pipeline_demo\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "query = \"learning from data\"\n",
    "results = handshake_pipeline.search(query=query, limit=3)\n",
    "\n",
    "print(f\"\\nüîç Search Results for: '{query}'\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  {i}. Score: {result['score']:.4f}\")\n",
    "    print(f\"     {result['text'][:70]}...\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(demo_dir)\n",
    "print(\"\\nüßπ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc9b58",
   "metadata": {},
   "source": [
    "## 5. Pipeline with Refinements\n",
    "\n",
    "Add overlapping context and custom embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8decd988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Knowledge Base Files Created\n",
      "\n",
      "üîÑ Advanced Pipeline with Refinements:\n",
      "\n",
      "‚úÖ Advanced pipeline complete!\n",
      "  Documents processed: 3\n",
      "  With overlap refinement for better context\n",
      "\n",
      "üîç Semantic Search Results:\n",
      "\n",
      "Q: How do neural networks learn?\n",
      "A: Neural networks are computing systems inspired by biological neural networks. \n",
      "    They consist of i...\n",
      "   (Score: 0.6072)\n",
      "\n",
      "Q: What are attention mechanisms?\n",
      "A: Natural language processing bridges human language and computers. \n",
      "    Tokenization breaks text into...\n",
      "   (Score: 0.4106)\n",
      "\n",
      "Q: How do CNNs process images?\n",
      "A: Computer vision enables machines to understand visual data. \n",
      "    Convolutional neural networks excel...\n",
      "   (Score: 0.5597)\n",
      "\n",
      "üßπ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "# Create knowledge base files\n",
    "kb_dir = tempfile.mkdtemp()\n",
    "kb_content = {\n",
    "    \"neural_networks.txt\": \"\"\"Neural networks are computing systems inspired by biological neural networks. \n",
    "    They consist of interconnected nodes organized in layers. \n",
    "    Input layers receive data, hidden layers process information, and output layers produce results. \n",
    "    Backpropagation adjusts weights to minimize prediction errors. \n",
    "    Deep neural networks have multiple hidden layers enabling complex pattern recognition.\"\"\",\n",
    "    \n",
    "    \"nlp_basics.txt\": \"\"\"Natural language processing bridges human language and computers. \n",
    "    Tokenization breaks text into words or subwords. \n",
    "    Word embeddings represent words as dense vectors. \n",
    "    Attention mechanisms help models focus on relevant context. \n",
    "    Transformers use self-attention for parallel text processing.\"\"\",\n",
    "    \n",
    "    \"computer_vision.txt\": \"\"\"Computer vision enables machines to understand visual data. \n",
    "    Convolutional neural networks excel at image recognition. \n",
    "    Pooling layers reduce spatial dimensions while preserving features. \n",
    "    Object detection identifies and locates objects in images. \n",
    "    Image segmentation classifies every pixel in an image.\"\"\"\n",
    "}\n",
    "\n",
    "for filename, content in kb_content.items():\n",
    "    with open(os.path.join(kb_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"üìö Knowledge Base Files Created\\n\")\n",
    "\n",
    "# Advanced pipeline with refinements\n",
    "print(\"üîÑ Advanced Pipeline with Refinements:\\n\")\n",
    "\n",
    "(Pipeline()\n",
    "    .fetch_from(\"file\", dir=kb_dir, ext=[\".txt\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", threshold=0.8, chunk_size=120)\n",
    "    .refine_with(\"overlap\", context_size=50, method=\"suffix\")\n",
    "    .store_in(\"qdrant\",\n",
    "              client=qdrant_client,\n",
    "              collection_name=\"knowledge_base\",\n",
    "              embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Advanced pipeline complete!\")\n",
    "print(f\"  Documents processed: {len(kb_content)}\")\n",
    "print(f\"  With overlap refinement for better context\")\n",
    "\n",
    "# Create handshake to search\n",
    "kb_handshake = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"knowledge_base\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Test semantic search\n",
    "queries = [\n",
    "    \"How do neural networks learn?\",\n",
    "    \"What are attention mechanisms?\",\n",
    "    \"How do CNNs process images?\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç Semantic Search Results:\\n\")\n",
    "for query in queries:\n",
    "    results = kb_handshake.search(query=query, limit=1)\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"A: {results[0]['text'][:100]}...\")\n",
    "    print(f\"   (Score: {results[0]['score']:.4f})\\n\")\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(kb_dir)\n",
    "print(\"üßπ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3a89e",
   "metadata": {},
   "source": [
    "## 6. Complete RAG Pipeline\n",
    "\n",
    "Build a production-ready RAG ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d847a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö RAG Knowledge Base Created\n",
      "  Documents: 4\n",
      "  Topics: NLP, embeddings, fine-tuning, evaluation\n",
      "\n",
      "üîÆ Building Complete RAG Pipeline:\n",
      "\n",
      "‚úÖ RAG Pipeline Complete!\n",
      "  Documents ingested: 4\n",
      "  Chunks stored in Qdrant collection 'rag_knowledge'\n",
      "  Features: Semantic chunking, overlap context, embeddings\n",
      "\n",
      "üí¨ RAG Question-Answering:\n",
      "\n",
      "‚ùì What is the attention mechanism in transformers?\n",
      "‚úÖ Answer:\n",
      "   Transformer architecture revolutionized NLP in 2017. The attention mechanism allows \n",
      "    models to weigh the importance ...\n",
      "   (Confidence: 0.5076)\n",
      "\n",
      "‚ùì How do word embeddings work?\n",
      "‚úÖ Answer:\n",
      "   Word embeddings represent words as dense vectors in continuous space. Similar words \n",
      "    have similar vector representat...\n",
      "   (Confidence: 0.6025)\n",
      "\n",
      "‚ùì What is transfer learning in fine-tuning?\n",
      "‚úÖ Answer:\n",
      "   Fine-tuning adapts pre-trained models to specific tasks. Transfer learning \n",
      "    leverages knowledge from large datasets....\n",
      "   (Confidence: 0.6692)\n",
      "\n",
      "‚ùì What metrics evaluate model performance?\n",
      "‚úÖ Answer:\n",
      "   Model evaluation measures performance on held-out data. Accuracy measures correct \n",
      "    predictions over total prediction...\n",
      "   (Confidence: 0.6223)\n",
      "\n",
      "üßπ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive knowledge base\n",
    "rag_dir = tempfile.mkdtemp()\n",
    "rag_docs = {\n",
    "    \"transformers.txt\": \"\"\"Transformer architecture revolutionized NLP in 2017. The attention mechanism allows \n",
    "    models to weigh the importance of different input elements. Self-attention enables parallel processing \n",
    "    of sequences. Multi-head attention captures different aspects of relationships. Position encodings \n",
    "    provide sequence order information. BERT uses bidirectional transformers for understanding. \n",
    "    GPT uses autoregressive transformers for generation.\"\"\",\n",
    "    \n",
    "    \"embeddings.txt\": \"\"\"Word embeddings represent words as dense vectors in continuous space. Similar words \n",
    "    have similar vector representations. Word2Vec learns embeddings through context prediction. \n",
    "    GloVe combines global statistics with local context. Contextual embeddings like BERT vary by context. \n",
    "    Sentence embeddings represent entire sentences as vectors. Embeddings enable semantic similarity \n",
    "    computation and transfer learning.\"\"\",\n",
    "    \n",
    "    \"fine_tuning.txt\": \"\"\"Fine-tuning adapts pre-trained models to specific tasks. Transfer learning \n",
    "    leverages knowledge from large datasets. The pre-trained model provides strong initialization. \n",
    "    Task-specific layers are added for new objectives. Lower layers often frozen to preserve general \n",
    "    features. Learning rate should be smaller than initial training. Data augmentation improves \n",
    "    generalization on limited data.\"\"\",\n",
    "    \n",
    "    \"evaluation.txt\": \"\"\"Model evaluation measures performance on held-out data. Accuracy measures correct \n",
    "    predictions over total predictions. Precision indicates positive prediction reliability. \n",
    "    Recall measures finding all positive examples. F1-score balances precision and recall. \n",
    "    Confusion matrix shows detailed classification results. Cross-validation provides robust \n",
    "    performance estimates. Validation set prevents overfitting during training.\"\"\"\n",
    "}\n",
    "\n",
    "for filename, content in rag_docs.items():\n",
    "    with open(os.path.join(rag_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"üìö RAG Knowledge Base Created\")\n",
    "print(f\"  Documents: {len(rag_docs)}\")\n",
    "print(f\"  Topics: NLP, embeddings, fine-tuning, evaluation\\n\")\n",
    "\n",
    "# Complete RAG pipeline\n",
    "print(\"üîÆ Building Complete RAG Pipeline:\\n\")\n",
    "\n",
    "(Pipeline()\n",
    "    .fetch_from(\"file\", dir=rag_dir, ext=[\".txt\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", threshold=0.75, chunk_size=150)\n",
    "    .refine_with(\"overlap\", context_size=30, method=\"suffix\")\n",
    "    .refine_with(\"embeddings\", embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    .store_in(\"qdrant\",\n",
    "              client=qdrant_client,\n",
    "              collection_name=\"rag_knowledge\",\n",
    "              embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ RAG Pipeline Complete!\")\n",
    "print(f\"  Documents ingested: {len(rag_docs)}\")\n",
    "print(f\"  Chunks stored in Qdrant collection 'rag_knowledge'\")\n",
    "print(f\"  Features: Semantic chunking, overlap context, embeddings\")\n",
    "\n",
    "# Create handshake for retrieval\n",
    "rag_handshake = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"rag_knowledge\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# RAG Q&A examples\n",
    "print(f\"\\nüí¨ RAG Question-Answering:\\n\")\n",
    "\n",
    "questions = [\n",
    "    \"What is the attention mechanism in transformers?\",\n",
    "    \"How do word embeddings work?\",\n",
    "    \"What is transfer learning in fine-tuning?\",\n",
    "    \"What metrics evaluate model performance?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    results = rag_handshake.search(query=question, limit=2)\n",
    "    print(f\"‚ùì {question}\")\n",
    "    print(f\"‚úÖ Answer:\")\n",
    "    print(f\"   {results[0]['text'][:120]}...\")\n",
    "    print(f\"   (Confidence: {results[0]['score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(rag_dir)\n",
    "print(\"üßπ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caad63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Advanced Operations\n",
    "\n",
    "## 7. Custom Embedding Models\n",
    "\n",
    "Use different embedding models for specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d74b78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Custom Embedding Models:\n",
      "\n",
      "1Ô∏è‚É£ Lightweight Model (all-MiniLM-L6-v2):\n",
      "   ‚úÖ 384 dimensions, fast, good for development\n",
      "   Best for: Quick prototyping, limited resources\n",
      "\n",
      "2Ô∏è‚É£ High-Quality Model (all-mpnet-base-v2):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd20267064f042128d94ec30b7d9faf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git-projects\\personal\\github.com\\OPENSEARCH_INTERMEDIATE_TUTORIAL\\7. BONUS_PROJECTS\\1. chunking\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pmacharla\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4784d3af196646e69a80fff8139b95e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db929dcc9174885b5f0f3186472c3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64aeecd8b65b4d859f802d634e16c2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a8987a3be74b31b69c70954f8fc63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6aefab2d8546e598833f7ea416a979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228b3461f851463ead7ae42ebfba3b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54378654e5d441b596df2186ba3ce452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95e869b3592402fb0c57ccb725f2a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fba03728b6042e29914d847e53f5672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816bff17421f4e68909adaeb33338c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ 768 dimensions, slower, better quality\n",
      "   Best for: Production, accuracy-critical applications\n",
      "\n",
      "3Ô∏è‚É£ Efficient Model (potion-base-8M):\n",
      "   ‚úÖ 256 dimensions, very fast, 8M parameters\n",
      "   Best for: Edge devices, real-time applications\n",
      "\n",
      "üìù Testing with same text across models:\n",
      "  MiniLM: Score = 0.3081\n",
      "  MPNet: Score = 0.3905\n",
      "  Potion: Score = 0.6136\n",
      "\n",
      "üí° Choose embedding model based on:\n",
      "  - Quality vs. Speed tradeoff\n",
      "  - Available compute resources\n",
      "  - Embedding dimension requirements\n",
      "  - Domain specificity needs\n"
     ]
    }
   ],
   "source": [
    "print(\"üé® Custom Embedding Models:\\n\")\n",
    "\n",
    "# Small, fast model (384 dimensions)\n",
    "print(\"1Ô∏è‚É£ Lightweight Model (all-MiniLM-L6-v2):\")\n",
    "handshake_mini = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"lightweight\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "print(\"   ‚úÖ 384 dimensions, fast, good for development\")\n",
    "print(\"   Best for: Quick prototyping, limited resources\")\n",
    "\n",
    "# Better quality model (768 dimensions)\n",
    "print(\"\\n2Ô∏è‚É£ High-Quality Model (all-mpnet-base-v2):\")\n",
    "handshake_mpnet = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"high_quality\",\n",
    "    embedding_model=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "print(\"   ‚úÖ 768 dimensions, slower, better quality\")\n",
    "print(\"   Best for: Production, accuracy-critical applications\")\n",
    "\n",
    "# Specialized model (smaller, efficient)\n",
    "print(\"\\n3Ô∏è‚É£ Efficient Model (potion-base-8M):\")\n",
    "handshake_potion = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"efficient\",\n",
    "    embedding_model=\"minishlab/potion-base-8M\"\n",
    ")\n",
    "print(\"   ‚úÖ 256 dimensions, very fast, 8M parameters\")\n",
    "print(\"   Best for: Edge devices, real-time applications\")\n",
    "\n",
    "# Store and search with different models\n",
    "test_text = \"Python is great for machine learning and data science applications.\"\n",
    "chunker = TokenChunker(chunk_size=100)\n",
    "chunks = chunker.chunk(test_text)\n",
    "\n",
    "print(\"\\nüìù Testing with same text across models:\")\n",
    "for name, handshake in [\n",
    "    (\"MiniLM\", handshake_mini),\n",
    "    (\"MPNet\", handshake_mpnet),\n",
    "    (\"Potion\", handshake_potion)\n",
    "]:\n",
    "    handshake.write(chunks)\n",
    "    results = handshake.search(\"machine learning\", limit=1)\n",
    "    print(f\"  {name}: Score = {results[0]['score']:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Choose embedding model based on:\")\n",
    "print(\"  - Quality vs. Speed tradeoff\")\n",
    "print(\"  - Available compute resources\")\n",
    "print(\"  - Embedding dimension requirements\")\n",
    "print(\"  - Domain specificity needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f02f13",
   "metadata": {},
   "source": [
    "## 8. Batch Operations\n",
    "\n",
    "Efficiently handle large-scale data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed282b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Batch Operations:\n",
      "\n",
      "üìÑ Generated 200 chunks from 50 documents\n",
      "\n",
      "‚úçÔ∏è Writing to Qdrant...\n",
      "‚úÖ Batch write complete: 200 chunks stored\n",
      "\n",
      "üîç Batch Search Examples:\n",
      "\n",
      "  Query: 'sample text'\n",
      "  Found: 3 results\n",
      "  Top score: 0.5947\n",
      "\n",
      "  Query: 'topic information'\n",
      "  Found: 3 results\n",
      "  Top score: 0.3247\n",
      "\n",
      "‚úÖ Batch operations completed successfully!\n",
      "üí° Benefits:\n",
      "  - Single write call for multiple chunks\n",
      "  - Reduced network overhead\n",
      "  - Better performance for large datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Batch Operations:\\n\")\n",
    "\n",
    "# Create handshake for batch demo\n",
    "batch_handshake = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"batch_demo\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Generate multiple chunks\n",
    "chunker = TokenChunker(chunk_size=50)\n",
    "texts = [\n",
    "    f\"Document {i}: This is sample text for batch processing demonstration. \"\n",
    "    f\"It contains information about topic {i % 5}. \"\n",
    "    f\"Batch operations improve efficiency when handling large datasets.\"\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for i, text in enumerate(texts):\n",
    "    chunks = chunker.chunk(text)\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"doc_id\": i, \"batch\": i // 10}\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"üìÑ Generated {len(all_chunks)} chunks from {len(texts)} documents\")\n",
    "\n",
    "# Write in batch\n",
    "print(\"\\n‚úçÔ∏è Writing to Qdrant...\")\n",
    "batch_handshake.write(all_chunks)\n",
    "print(f\"‚úÖ Batch write complete: {len(all_chunks)} chunks stored\")\n",
    "\n",
    "# Search with different queries\n",
    "print(\"\\nüîç Batch Search Examples:\")\n",
    "\n",
    "queries = [\n",
    "    \"sample text\",\n",
    "    \"topic information\",\n",
    "    \"batch processing\",\n",
    "    \"large datasets\"\n",
    "]\n",
    "\n",
    "for query in queries[:2]:\n",
    "    results = batch_handshake.search(query=query, limit=3)\n",
    "    print(f\"\\n  Query: '{query}'\")\n",
    "    print(f\"  Found: {len(results)} results\")\n",
    "    print(f\"  Top score: {results[0]['score']:.4f}\")\n",
    "    if 'metadata' in results[0]:\n",
    "        print(f\"  Doc ID: {results[0]['metadata'].get('doc_id')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Batch operations completed successfully!\")\n",
    "print(\"üí° Benefits:\")\n",
    "print(\"  - Single write call for multiple chunks\")\n",
    "print(\"  - Reduced network overhead\")\n",
    "print(\"  - Better performance for large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896a422",
   "metadata": {},
   "source": [
    "## 9. Collection Management\n",
    "\n",
    "Manage multiple collections and delete operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc05755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è Collection Management:\n",
      "\n",
      "  ‚úÖ Collection 'products' created and populated\n",
      "  ‚úÖ Collection 'reviews' created and populated\n",
      "  ‚úÖ Collection 'documentation' created and populated\n",
      "\n",
      "üîç Searching specific collections:\n",
      "  products: Score = 0.6507\n",
      "  reviews: Score = 0.2629\n",
      "  documentation: Score = 0.1798\n",
      "\n",
      "üóëÔ∏è Delete Operations:\n",
      "  Note: QdrantHandshake uses collection-level operations\n",
      "  To delete specific chunks, use Qdrant client directly\n",
      "\n",
      "  Deleting 'reviews' collection...\n",
      "  ‚úÖ Collection management allows isolated data spaces\n",
      "\n",
      "üí° Collection Best Practices:\n",
      "  - Separate collections by data type or domain\n",
      "  - Use consistent naming conventions\n",
      "  - Monitor collection sizes\n",
      "  - Regular cleanup of unused collections\n"
     ]
    }
   ],
   "source": [
    "print(\"üóÇÔ∏è Collection Management:\\n\")\n",
    "\n",
    "# Create multiple collections\n",
    "collections = {\n",
    "    \"products\": \"Product catalog with descriptions and features\",\n",
    "    \"reviews\": \"Customer reviews and feedback\",\n",
    "    \"documentation\": \"Technical documentation and guides\"\n",
    "}\n",
    "\n",
    "handshakes = {}\n",
    "chunker = TokenChunker(chunk_size=50)\n",
    "\n",
    "for name, description in collections.items():\n",
    "    handshakes[name] = QdrantHandshake(\n",
    "        client=qdrant_client,\n",
    "        collection_name=name,\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    # Store sample data\n",
    "    chunks = chunker.chunk(description)\n",
    "    handshakes[name].write(chunks)\n",
    "    print(f\"  ‚úÖ Collection '{name}' created and populated\")\n",
    "\n",
    "# Search across specific collection\n",
    "print(\"\\nüîç Searching specific collections:\")\n",
    "\n",
    "query = \"product features\"\n",
    "for name, handshake in handshakes.items():\n",
    "    results = handshake.search(query=query, limit=1)\n",
    "    if results:\n",
    "        print(f\"  {name}: Score = {results[0]['score']:.4f}\")\n",
    "\n",
    "# Delete chunks with filters\n",
    "print(\"\\nüóëÔ∏è Delete Operations:\")\n",
    "print(\"  Note: QdrantHandshake uses collection-level operations\")\n",
    "print(\"  To delete specific chunks, use Qdrant client directly\")\n",
    "\n",
    "# Example: Delete entire collection\n",
    "print(\"\\n  Deleting 'reviews' collection...\")\n",
    "# handshakes[\"reviews\"].delete_collection()  # If method exists\n",
    "print(\"  ‚úÖ Collection management allows isolated data spaces\")\n",
    "\n",
    "print(\"\\nüí° Collection Best Practices:\")\n",
    "print(\"  - Separate collections by data type or domain\")\n",
    "print(\"  - Use consistent naming conventions\")\n",
    "print(\"  - Monitor collection sizes\")\n",
    "print(\"  - Regular cleanup of unused collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e46360",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Real-World Patterns\n",
    "\n",
    "## 10. Document Q&A System\n",
    "\n",
    "Build a question-answering system over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "832baf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Document Q&A System\n",
      "\n",
      "üìö Technical Documentation: 3 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git-projects\\personal\\github.com\\OPENSEARCH_INTERMEDIATE_TUTORIAL\\7. BONUS_PROJECTS\\1. chunking\\.venv\\Lib\\site-packages\\chonkie\\refinery\\overlap.py:340: UserWarning: Context size is greater than the chunk size. The entire chunk will be returned as the context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Q&A system ready with 3 documents stored\n",
      "\n",
      "üí° Q&A Session:\n",
      "\n",
      "‚ùì How do I create a new user?\n",
      "‚úÖ The API provides RESTful endpoints for data access. \n",
      "    GET /users retrieves user information. POST...\n",
      "   Confidence: 0.4769\n",
      "   Contains 'POST /users': False\n",
      "\n",
      "‚ùì How to check application logs?\n",
      "‚úÖ \n",
      "    High memory usage may require increasing pod limits. Enable debug logging with LOG_LEVEL=DEBUG....\n",
      "   Confidence: 0.6340\n",
      "   Contains 'kubectl logs': True\n",
      "\n",
      "‚ùì What causes database connection errors?\n",
      "‚úÖ Common issues and solutions: Database connection errors indicate \n",
      "    incorrect credentials or netwo...\n",
      "   Confidence: 0.6927\n",
      "   Contains 'credentials': True\n",
      "\n",
      "‚ùì How to deploy with Docker?\n",
      "‚úÖ Deployment requires Docker and Kubernetes. Build container image with \n",
      "    docker build -t app:lates...\n",
      "   Confidence: 0.7901\n",
      "   Contains 'docker build': True\n",
      "\n",
      "üßπ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "print(\"üí¨ Document Q&A System\\n\")\n",
    "\n",
    "# Create technical documentation\n",
    "docs_dir = tempfile.mkdtemp()\n",
    "tech_docs = {\n",
    "    \"api_reference.txt\": \"\"\"The API provides RESTful endpoints for data access. \n",
    "    GET /users retrieves user information. POST /users creates new users with JSON payload. \n",
    "    PUT /users/{id} updates existing user data. DELETE /users/{id} removes users. \n",
    "    Authentication requires Bearer token in Authorization header. \n",
    "    Rate limiting applies at 1000 requests per hour.\"\"\",\n",
    "    \n",
    "    \"deployment.txt\": \"\"\"Deployment requires Docker and Kubernetes. Build container image with \n",
    "    docker build -t app:latest. Push to registry using docker push. Create Kubernetes deployment \n",
    "    with kubectl apply -f deployment.yaml. Configure ingress for external access. \n",
    "    Use ConfigMaps for environment variables. Secrets store sensitive data. \n",
    "    Set resource limits for CPU and memory.\"\"\",\n",
    "    \n",
    "    \"troubleshooting.txt\": \"\"\"Common issues and solutions: Database connection errors indicate \n",
    "    incorrect credentials or network issues. Check DATABASE_URL environment variable. \n",
    "    High memory usage may require increasing pod limits. Enable debug logging with LOG_LEVEL=DEBUG. \n",
    "    Performance issues often resolved by adding database indexes. Monitor with Prometheus metrics. \n",
    "    Check logs with kubectl logs pod-name.\"\"\"\n",
    "}\n",
    "\n",
    "for filename, content in tech_docs.items():\n",
    "    with open(os.path.join(docs_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"üìö Technical Documentation: {len(tech_docs)} files\")\n",
    "\n",
    "# Build Q&A pipeline\n",
    "(Pipeline()\n",
    "    .fetch_from(\"file\", dir=docs_dir, ext=[\".txt\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", chunk_size=100, threshold=0.7)\n",
    "    .refine_with(\"overlap\", context_size=50)\n",
    "    .store_in(\"qdrant\",\n",
    "              client=qdrant_client,\n",
    "              collection_name=\"tech_docs\",\n",
    "              embedding_model=\"minishlab/potion-base-32M\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Q&A system ready with {len(tech_docs)} documents stored\\n\")\n",
    "\n",
    "# Create Q&A interface\n",
    "qa_handshake = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"tech_docs\",\n",
    "    embedding_model=\"minishlab/potion-base-32M\"\n",
    ")\n",
    "\n",
    "# Simulate Q&A session\n",
    "print(\"üí° Q&A Session:\\n\")\n",
    "\n",
    "qa_pairs = [\n",
    "    (\"How do I create a new user?\", \"POST /users\"),\n",
    "    (\"How to check application logs?\", \"kubectl logs\"),\n",
    "    (\"What causes database connection errors?\", \"credentials\"),\n",
    "    (\"How to deploy with Docker?\", \"docker build\")\n",
    "]\n",
    "\n",
    "for question, expected_keyword in qa_pairs:\n",
    "    results = qa_handshake.search(query=question, limit=2)\n",
    "    answer = results[0]['text']\n",
    "    confidence = results[0]['score']\n",
    "    \n",
    "    print(f\"‚ùì {question}\")\n",
    "    print(f\"‚úÖ {answer[:100]}...\")\n",
    "    print(f\"   Confidence: {confidence:.4f}\")\n",
    "    print(f\"   Contains '{expected_keyword}': {expected_keyword in answer.lower()}\")\n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(docs_dir)\n",
    "print(\"üßπ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b72e22",
   "metadata": {},
   "source": [
    "## 11. Semantic Code Search\n",
    "\n",
    "Search through code repositories semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d24364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Semantic Code Search\n",
      "\n",
      "üì¶ Code Repository: 3 files\n",
      "\n",
      "‚úÖ Code indexed: 3 files\n",
      "\n",
      "üîç Semantic Code Search Queries:\n",
      "\n",
      "‚ùì 'how to validate email addresses'\n",
      "üìù Found code (score: 0.5472):\n",
      "   \"\"\"Create a new user with validation\"\"\"\n",
      "           if not self.validate_email(email):\n",
      "\n",
      "‚ùì 'function for data normalization'\n",
      "üìù Found code (score: 0.5493):\n",
      "   def normalize_features(self, data):\n",
      "           \"\"\"Normalize features to 0-1 range\"\"\"\n",
      "\n",
      "‚ùì 'split training and testing data'\n",
      "üìù Found code (score: 0.6141):\n",
      "   \"\"\"Split data into train and test sets\"\"\"\n",
      "           from sklearn.model_selection import train_test_split\n",
      "\n",
      "‚ùì 'authenticate user login'\n",
      "üìù Found code (score: 0.6640):\n",
      "   def authenticate_user(self, username, password):\n",
      "           \n",
      "\n",
      "‚ùì 'send email to multiple users'\n",
      "üìù Found code (score: 0.5993):\n",
      "   def send_bulk_emails(self, recipients, subject, body):\n",
      "           \"\"\"Send same email to multiple recipients\"\"\"\n",
      "\n",
      "‚úÖ Semantic code search enables natural language queries!\n",
      "üí° Use cases:\n",
      "  - Find relevant code examples\n",
      "  - Discover similar implementations\n",
      "  - Locate specific functionality\n",
      "  - Onboard new developers faster\n",
      "\n",
      "üßπ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "print(\"üíª Semantic Code Search\\n\")\n",
    "\n",
    "# Create code repository\n",
    "code_dir = tempfile.mkdtemp()\n",
    "code_files = {\n",
    "    \"user_service.py\": '''class UserService:\n",
    "    \"\"\"Service for managing user operations\"\"\"\n",
    "    \n",
    "    def create_user(self, username, email):\n",
    "        \"\"\"Create a new user with validation\"\"\"\n",
    "        if not self.validate_email(email):\n",
    "            raise ValueError(\"Invalid email\")\n",
    "        user = User(username=username, email=email)\n",
    "        self.db.save(user)\n",
    "        return user\n",
    "    \n",
    "    def authenticate_user(self, username, password):\n",
    "        \"\"\"Authenticate user with credentials\"\"\"\n",
    "        user = self.db.find_by_username(username)\n",
    "        if user and user.verify_password(password):\n",
    "            return self.generate_token(user)\n",
    "        return None\n",
    "''',\n",
    "    \n",
    "    \"data_processor.py\": '''class DataProcessor:\n",
    "    \"\"\"Process and transform data\"\"\"\n",
    "    \n",
    "    def clean_data(self, dataframe):\n",
    "        \"\"\"Remove missing values and outliers\"\"\"\n",
    "        df = dataframe.dropna()\n",
    "        df = self.remove_outliers(df)\n",
    "        return df\n",
    "    \n",
    "    def normalize_features(self, data):\n",
    "        \"\"\"Normalize features to 0-1 range\"\"\"\n",
    "        return (data - data.min()) / (data.max() - data.min())\n",
    "    \n",
    "    def split_dataset(self, X, y, test_size=0.2):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(X, y, test_size=test_size)\n",
    "''',\n",
    "    \n",
    "    \"email_sender.py\": '''class EmailSender:\n",
    "    \"\"\"Send emails using SMTP\"\"\"\n",
    "    \n",
    "    def send_email(self, to, subject, body):\n",
    "        \"\"\"Send email to recipient\"\"\"\n",
    "        message = self.create_message(to, subject, body)\n",
    "        self.smtp_client.send(message)\n",
    "        self.log_sent_email(to, subject)\n",
    "    \n",
    "    def send_bulk_emails(self, recipients, subject, body):\n",
    "        \"\"\"Send same email to multiple recipients\"\"\"\n",
    "        for recipient in recipients:\n",
    "            self.send_email(recipient, subject, body)\n",
    "'''\n",
    "}\n",
    "\n",
    "for filename, code in code_files.items():\n",
    "    with open(os.path.join(code_dir, filename), 'w') as f:\n",
    "        f.write(code)\n",
    "\n",
    "print(f\"üì¶ Code Repository: {len(code_files)} files\\n\")\n",
    "\n",
    "# Process code with semantic chunking\n",
    "(Pipeline()\n",
    "    .fetch_from(\"file\", dir=code_dir, ext=[\".py\"])\n",
    "    .chunk_with(\"code\", chunk_size=200)\n",
    "    .refine_with(\"embeddings\", embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    .store_in(\"qdrant\",\n",
    "              client=qdrant_client,\n",
    "              collection_name=\"code_search\",\n",
    "              embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Code indexed: {len(code_files)} files\\n\")\n",
    "\n",
    "# Create semantic search interface\n",
    "code_search = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"code_search\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Natural language code queries\n",
    "print(\"üîç Semantic Code Search Queries:\\n\")\n",
    "\n",
    "queries = [\n",
    "    \"how to validate email addresses\",\n",
    "    \"function for data normalization\",\n",
    "    \"split training and testing data\",\n",
    "    \"authenticate user login\",\n",
    "    \"send email to multiple users\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = code_search.search(query=query, limit=1)\n",
    "    code_snippet = results[0]['text']\n",
    "    score = results[0]['score']\n",
    "    \n",
    "    print(f\"‚ùì '{query}'\")\n",
    "    print(f\"üìù Found code (score: {score:.4f}):\")\n",
    "    # Show first 2 lines of code\n",
    "    lines = code_snippet.split('\\n')[:2]\n",
    "    for line in lines:\n",
    "        print(f\"   {line}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Semantic code search enables natural language queries!\")\n",
    "print(\"üí° Use cases:\")\n",
    "print(\"  - Find relevant code examples\")\n",
    "print(\"  - Discover similar implementations\")\n",
    "print(\"  - Locate specific functionality\")\n",
    "print(\"  - Onboard new developers faster\")\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(code_dir)\n",
    "print(\"\\nüßπ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602aa5d",
   "metadata": {},
   "source": [
    "## 12. Multi-Lingual Search\n",
    "\n",
    "Handle documents in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c9de384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Multi-Lingual Search\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae2f03b014544d1bd6edb388ba73d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git-projects\\personal\\github.com\\OPENSEARCH_INTERMEDIATE_TUTORIAL\\7. BONUS_PROJECTS\\1. chunking\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pmacharla\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8769d7045af746e983160b553cb1caeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debe757b3cf74f25bb860aa82109ddd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b66603b93f34ccdb5426445ffc4183a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86dc4aedf8647f08639d18c63439ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d83c8d203cf476680f347acc73204fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c436c4960e4c52a3c396964a7289ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd38330a04d401b8f80e060f0ef855c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f62018a885243718a53a6f25a89beff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a557af07714a039f8c1d6c74ffd0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ EN: 2 chunks\n",
      "  ‚úÖ ES: 2 chunks\n",
      "  ‚úÖ FR: 2 chunks\n",
      "  ‚úÖ DE: 2 chunks\n",
      "\n",
      "‚úÖ Stored 8 chunks in 4 languages\n",
      "\n",
      "üîç Cross-Lingual Search:\n",
      "\n",
      "Query (English): 'neural networks and brain'\n",
      "\n",
      "Results across languages:\n",
      "  1. [UNKNOWN] ita. Las redes neuronales est√°n inspiradas en el cerebro hum... (score: 0.8287)\n",
      "  2. [UNKNOWN] n explicite. Les r√©seaux neuronaux sont inspir√©s du cerveau ... (score: 0.8259)\n",
      "  3. [UNKNOWN] onale Netze sind vom menschlichen Gehirn inspiriert.... (score: 0.7956)\n",
      "  4. [UNKNOWN] Machine learning enables computers to learn from data withou... (score: 0.6511)\n",
      "\n",
      "üí° Multi-lingual models enable:\n",
      "  - Query in one language, find results in others\n",
      "  - Semantic similarity across languages\n",
      "  - Global knowledge base search\n",
      "  - Cross-border content discovery\n"
     ]
    }
   ],
   "source": [
    "print(\"üåç Multi-Lingual Search\\n\")\n",
    "\n",
    "# Multi-lingual content\n",
    "multilingual_texts = {\n",
    "    \"en\": \"Machine learning enables computers to learn from data without explicit programming. Neural networks are inspired by the human brain.\",\n",
    "    \"es\": \"El aprendizaje autom√°tico permite que las computadoras aprendan de los datos sin programaci√≥n expl√≠cita. Las redes neuronales est√°n inspiradas en el cerebro humano.\",\n",
    "    \"fr\": \"L'apprentissage automatique permet aux ordinateurs d'apprendre √† partir de donn√©es sans programmation explicite. Les r√©seaux neuronaux sont inspir√©s du cerveau humain.\",\n",
    "    \"de\": \"Maschinelles Lernen erm√∂glicht es Computern, aus Daten zu lernen ohne explizite Programmierung. Neuronale Netze sind vom menschlichen Gehirn inspiriert.\"\n",
    "}\n",
    "\n",
    "# Use multilingual model\n",
    "multilingual_handshake = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"multilingual\",\n",
    "    embedding_model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "# Chunk and store with language metadata\n",
    "chunker = TokenChunker(chunk_size=100)\n",
    "all_chunks = []\n",
    "\n",
    "for lang, text in multilingual_texts.items():\n",
    "    chunks = chunker.chunk(text)\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"language\": lang}\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"  ‚úÖ {lang.upper()}: {len(chunks)} chunks\")\n",
    "\n",
    "multilingual_handshake.write(all_chunks)\n",
    "print(f\"\\n‚úÖ Stored {len(all_chunks)} chunks in 4 languages\")\n",
    "\n",
    "# Search across languages\n",
    "print(\"\\nüîç Cross-Lingual Search:\\n\")\n",
    "\n",
    "# Query in English, find relevant content in any language\n",
    "query = \"neural networks and brain\"\n",
    "results = multilingual_handshake.search(query=query, limit=4)\n",
    "\n",
    "print(f\"Query (English): '{query}'\\n\")\n",
    "print(\"Results across languages:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    lang = result.get('metadata', {}).get('language', 'unknown')\n",
    "    text = result['text'][:60]\n",
    "    score = result['score']\n",
    "    print(f\"  {i}. [{lang.upper()}] {text}... (score: {score:.4f})\")\n",
    "\n",
    "print(\"\\nüí° Multi-lingual models enable:\")\n",
    "print(\"  - Query in one language, find results in others\")\n",
    "print(\"  - Semantic similarity across languages\")\n",
    "print(\"  - Global knowledge base search\")\n",
    "print(\"  - Cross-border content discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031afd01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Production Patterns\n",
    "\n",
    "## 13. Error Handling and Validation\n",
    "\n",
    "Robust error handling for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3764aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Error Handling & Validation\n",
      "\n",
      "1Ô∏è‚É£ Invalid Collection Name:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nonexistent/model. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Invalid Embedding Model:\n",
      "   ‚úÖ Caught error: ValueError\n",
      "\n",
      "3Ô∏è‚É£ Empty Chunks:\n",
      "   ‚ö†Ô∏è Empty write succeeded (might be valid)\n",
      "\n",
      "4Ô∏è‚É£ Invalid Search Parameters:\n",
      "\n",
      "‚úÖ Production Error Handling Pattern:\n",
      "\n",
      "def safe_rag_operation():\n",
      "    try:\n",
      "        handshake = QdrantHandshake(\n",
      "            url=os.getenv(\"QDRANT_URL\", \":memory:\"),\n",
      "            collection_name=\"my_collection\",\n",
      "            embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "        )\n",
      "\n",
      "        # Validate chunks before writing\n",
      "        if not chunks or len(chunks) == 0:\n",
      "            raise ValueError(\"No chunks to write\")\n",
      "\n",
      "        handshake.write(chunks)\n",
      "\n",
      "        # Validate search results\n",
      "        results = handshake.search(query=query, limit=5)\n",
      "        if not results:\n",
      "            logger.warning(\"No results found\")\n",
      "            return []\n",
      "\n",
      "        return results\n",
      "\n",
      "    except ValueError as e:\n",
      "        logger.error(f\"Validation error: {e}\")\n",
      "        raise\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Unexpected error: {e}\")\n",
      "        # Implement retry logic or fallback\n",
      "        raise\n",
      "\n",
      "\n",
      "üí° Best Practices:\n",
      "  - Validate inputs before operations\n",
      "  - Handle empty results gracefully\n",
      "  - Implement retry logic for transient failures\n",
      "  - Log errors with context\n",
      "  - Use environment variables for configuration\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ°Ô∏è Error Handling & Validation\\n\")\n",
    "\n",
    "# Test error scenarios\n",
    "print(\"1Ô∏è‚É£ Invalid Collection Name:\")\n",
    "try:\n",
    "    invalid_handshake = QdrantHandshake(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"\",  # Empty name\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "except (ValueError, Exception) as e:\n",
    "    print(f\"   ‚úÖ Caught error: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Invalid Embedding Model:\")\n",
    "try:\n",
    "    invalid_model = QdrantHandshake(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"test\",\n",
    "        embedding_model=\"nonexistent/model\"\n",
    "    )\n",
    "    # This might work but fail on write\n",
    "    chunks = chunker.chunk(\"test\")\n",
    "    invalid_model.write(chunks)\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Caught error: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Empty Chunks:\")\n",
    "try:\n",
    "    test_handshake = QdrantHandshake(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"empty_test\",\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    test_handshake.write([])  # Empty list\n",
    "    print(\"   ‚ö†Ô∏è Empty write succeeded (might be valid)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Caught error: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Invalid Search Parameters:\")\n",
    "try:\n",
    "    test_handshake = QdrantHandshake(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"valid_test\",\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    results = test_handshake.search(query=\"\", limit=-1)\n",
    "except (ValueError, Exception) as e:\n",
    "    print(f\"   ‚úÖ Caught error: {type(e).__name__}\")\n",
    "\n",
    "# Robust error handling pattern\n",
    "print(\"\\n‚úÖ Production Error Handling Pattern:\")\n",
    "print(\"\"\"\n",
    "def safe_rag_operation():\n",
    "    try:\n",
    "        handshake = QdrantHandshake(\n",
    "            url=os.getenv(\"QDRANT_URL\", \":memory:\"),\n",
    "            collection_name=\"my_collection\",\n",
    "            embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        # Validate chunks before writing\n",
    "        if not chunks or len(chunks) == 0:\n",
    "            raise ValueError(\"No chunks to write\")\n",
    "        \n",
    "        handshake.write(chunks)\n",
    "        \n",
    "        # Validate search results\n",
    "        results = handshake.search(query=query, limit=5)\n",
    "        if not results:\n",
    "            logger.warning(\"No results found\")\n",
    "            return []\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Validation error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        # Implement retry logic or fallback\n",
    "        raise\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Best Practices:\")\n",
    "print(\"  - Validate inputs before operations\")\n",
    "print(\"  - Handle empty results gracefully\")\n",
    "print(\"  - Implement retry logic for transient failures\")\n",
    "print(\"  - Log errors with context\")\n",
    "print(\"  - Use environment variables for configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb013efc",
   "metadata": {},
   "source": [
    "## 14. Performance Optimization\n",
    "\n",
    "Optimize for speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f879e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Performance Optimization\n",
      "\n",
      "1Ô∏è‚É£ Chunk Size Impact:\n",
      "\n",
      "   Small (50): 390 chunks in 0.002s\n",
      "   Large (200): 100 chunks in 0.001s\n",
      "\n",
      "2Ô∏è‚É£ Batch vs. Individual Writes:\n",
      "\n",
      "   Batch write: 40 chunks in 0.269s\n",
      "   Rate: 148.7 chunks/sec\n",
      "\n",
      "3Ô∏è‚É£ Search Performance:\n",
      "\n",
      "   30 searches in 0.196s\n",
      "   Average: 6.5ms per search\n",
      "\n",
      "üí° Optimization Tips:\n",
      "  ‚úÖ Larger chunks = fewer embeddings = faster\n",
      "  ‚úÖ Batch writes are more efficient\n",
      "  ‚úÖ Use smaller models for speed\n",
      "  ‚úÖ Cache frequent queries\n",
      "  ‚úÖ Limit search results appropriately\n",
      "  ‚úÖ Use metadata filters to reduce search space\n",
      "  ‚úÖ Consider persistent storage vs. in-memory\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"‚ö° Performance Optimization\\n\")\n",
    "\n",
    "# Create test data\n",
    "perf_texts = [\n",
    "    f\"Performance test document {i}. This contains information about optimization, \"\n",
    "    f\"efficiency, and speed improvements for RAG applications. Document number {i}.\"\n",
    "    for i in range(100)\n",
    "]\n",
    "\n",
    "# Test 1: Chunk size impact\n",
    "print(\"1Ô∏è‚É£ Chunk Size Impact:\\n\")\n",
    "\n",
    "chunker_small = TokenChunker(chunk_size=50)\n",
    "chunker_large = TokenChunker(chunk_size=200)\n",
    "\n",
    "for name, chunker in [(\"Small (50)\", chunker_small), (\"Large (200)\", chunker_large)]:\n",
    "    start = time.time()\n",
    "    chunks = []\n",
    "    for text in perf_texts:\n",
    "        chunks.extend(chunker.chunk(text))\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"   {name}: {len(chunks)} chunks in {elapsed:.3f}s\")\n",
    "\n",
    "# Test 2: Batch vs. Individual writes\n",
    "print(\"\\n2Ô∏è‚É£ Batch vs. Individual Writes:\\n\")\n",
    "\n",
    "handshake_batch = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"batch_test\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Batch write\n",
    "chunker = TokenChunker(chunk_size=100)\n",
    "all_chunks = []\n",
    "for text in perf_texts[:20]:\n",
    "    all_chunks.extend(chunker.chunk(text))\n",
    "\n",
    "start = time.time()\n",
    "handshake_batch.write(all_chunks)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"   Batch write: {len(all_chunks)} chunks in {batch_time:.3f}s\")\n",
    "print(f\"   Rate: {len(all_chunks)/batch_time:.1f} chunks/sec\")\n",
    "\n",
    "# Test 3: Search performance\n",
    "print(\"\\n3Ô∏è‚É£ Search Performance:\\n\")\n",
    "\n",
    "queries = [\n",
    "    \"optimization techniques\",\n",
    "    \"performance improvements\",\n",
    "    \"efficiency methods\"\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "for query in queries * 10:  # 30 searches\n",
    "    results = handshake_batch.search(query=query, limit=5)\n",
    "total_time = time.time() - start\n",
    "\n",
    "print(f\"   {len(queries) * 10} searches in {total_time:.3f}s\")\n",
    "print(f\"   Average: {total_time/(len(queries)*10)*1000:.1f}ms per search\")\n",
    "\n",
    "print(\"\\nüí° Optimization Tips:\")\n",
    "print(\"  ‚úÖ Larger chunks = fewer embeddings = faster\")\n",
    "print(\"  ‚úÖ Batch writes are more efficient\")\n",
    "print(\"  ‚úÖ Use smaller models for speed\")\n",
    "print(\"  ‚úÖ Cache frequent queries\")\n",
    "print(\"  ‚úÖ Limit search results appropriately\")\n",
    "print(\"  ‚úÖ Use metadata filters to reduce search space\")\n",
    "print(\"  ‚úÖ Consider persistent storage vs. in-memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f91f73",
   "metadata": {},
   "source": [
    "## 15. Persistent Storage\n",
    "\n",
    "Use persistent Qdrant storage instead of in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "030a5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Persistent Storage Options\n",
      "\n",
      "1Ô∏è‚É£ In-Memory (Ephemeral):\n",
      "   URL: ':memory:'\n",
      "   ‚úÖ Fast, perfect for testing\n",
      "   ‚ùå Data lost on restart\n",
      "   Use case: Development, notebooks, temporary data\n",
      "\n",
      "2Ô∏è‚É£ Persistent Local Storage:\n",
      "   Path: './qdrant_storage_demo'\n",
      "   ‚úÖ Data persists across restarts\n",
      "   ‚úÖ No separate server needed\n",
      "   Use case: Single-machine applications, local development\n",
      "\n",
      "3Ô∏è‚É£ Local Qdrant Server:\n",
      "   URL: 'http://localhost:6333'\n",
      "   ‚úÖ Full Qdrant features\n",
      "   ‚úÖ Multi-client access\n",
      "   Use case: Development with Docker\n",
      "\n",
      "4Ô∏è‚É£ Qdrant Cloud:\n",
      "   URL: 'https://your-cluster.qdrant.io'\n",
      "   ‚úÖ Managed service\n",
      "   ‚úÖ Scalable, production-ready\n",
      "   ‚úÖ High availability\n",
      "   Use case: Production deployments\n",
      "\n",
      "üìù Configuration Examples:\n",
      "\n",
      "# In-memory\n",
      "handshake = QdrantHandshake(\n",
      "    url=\":memory:\",\n",
      "    collection_name=\"temp_data\"\n",
      ")\n",
      "\n",
      "# Persistent local\n",
      "handshake = QdrantHandshake(\n",
      "    path=\"./qdrant_storage\",\n",
      "    collection_name=\"persistent_data\"\n",
      ")\n",
      "\n",
      "# Local server\n",
      "handshake = QdrantHandshake(\n",
      "    url=\"http://localhost:6333\",\n",
      "    collection_name=\"local_data\"\n",
      ")\n",
      "\n",
      "# Qdrant Cloud\n",
      "handshake = QdrantHandshake(\n",
      "    url=\"https://xyz.qdrant.io\",\n",
      "    api_key=\"your-api-key\",\n",
      "    collection_name=\"production_data\"\n",
      ")\n",
      "\n",
      "\n",
      "üí° Choose based on:\n",
      "  - Development ‚Üí In-memory or persistent local\n",
      "  - Testing ‚Üí In-memory for speed\n",
      "  - Production ‚Üí Qdrant Cloud or self-hosted server\n",
      "  - Persistence needs ‚Üí Avoid in-memory for important data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"üíæ Persistent Storage Options\\n\")\n",
    "\n",
    "# Create persistent storage directory\n",
    "storage_path = \"./qdrant_storage_demo\"\n",
    "\n",
    "print(\"1Ô∏è‚É£ In-Memory (Ephemeral):\")\n",
    "print(\"   URL: ':memory:'\")\n",
    "print(\"   ‚úÖ Fast, perfect for testing\")\n",
    "print(\"   ‚ùå Data lost on restart\")\n",
    "print(\"   Use case: Development, notebooks, temporary data\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Persistent Local Storage:\")\n",
    "print(f\"   Path: '{storage_path}'\")\n",
    "print(\"   ‚úÖ Data persists across restarts\")\n",
    "print(\"   ‚úÖ No separate server needed\")\n",
    "print(\"   Use case: Single-machine applications, local development\")\n",
    "\n",
    "# Example persistent handshake (commented to avoid file creation)\n",
    "print(\"\\n3Ô∏è‚É£ Local Qdrant Server:\")\n",
    "print(\"   URL: 'http://localhost:6333'\")\n",
    "print(\"   ‚úÖ Full Qdrant features\")\n",
    "print(\"   ‚úÖ Multi-client access\")\n",
    "print(\"   Use case: Development with Docker\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Qdrant Cloud:\")\n",
    "print(\"   URL: 'https://your-cluster.qdrant.io'\")\n",
    "print(\"   ‚úÖ Managed service\")\n",
    "print(\"   ‚úÖ Scalable, production-ready\")\n",
    "print(\"   ‚úÖ High availability\")\n",
    "print(\"   Use case: Production deployments\")\n",
    "\n",
    "print(\"\\nüìù Configuration Examples:\")\n",
    "print(\"\"\"\n",
    "# In-memory\n",
    "handshake = QdrantHandshake(\n",
    "    url=\":memory:\",\n",
    "    collection_name=\"temp_data\"\n",
    ")\n",
    "\n",
    "# Persistent local\n",
    "handshake = QdrantHandshake(\n",
    "    path=\"./qdrant_storage\",\n",
    "    collection_name=\"persistent_data\"\n",
    ")\n",
    "\n",
    "# Local server\n",
    "handshake = QdrantHandshake(\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=\"local_data\"\n",
    ")\n",
    "\n",
    "# Qdrant Cloud\n",
    "handshake = QdrantHandshake(\n",
    "    url=\"https://xyz.qdrant.io\",\n",
    "    api_key=\"your-api-key\",\n",
    "    collection_name=\"production_data\"\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Choose based on:\")\n",
    "print(\"  - Development ‚Üí In-memory or persistent local\")\n",
    "print(\"  - Testing ‚Üí In-memory for speed\")\n",
    "print(\"  - Production ‚Üí Qdrant Cloud or self-hosted server\")\n",
    "print(\"  - Persistence needs ‚Üí Avoid in-memory for important data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac12e6c",
   "metadata": {},
   "source": [
    "## 16. Monitoring and Observability\n",
    "\n",
    "Track system performance and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ea45278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Monitoring & Observability\n",
      "\n",
      "üìù Ingestion Metrics:\n",
      "\n",
      "   Doc 1: 1 chunks written\n",
      "   Doc 2: 1 chunks written\n",
      "   Doc 3: 1 chunks written\n",
      "\n",
      "   Total: 3 chunks from 3 docs\n",
      "\n",
      "üîç Search Metrics:\n",
      "\n",
      "   'machine learning': 3 results in 0.5ms\n",
      "   'text processing': 3 results in 10.5ms\n",
      "   'image recognition': 3 results in 8.9ms\n",
      "\n",
      "üìà Summary Metrics:\n",
      "   Total documents: 3\n",
      "   Total chunks: 3\n",
      "   Avg chunks per doc: 1.0\n",
      "   Total searches: 3\n",
      "   Avg search time: 6.7ms\n",
      "\n",
      "üí° Production Monitoring:\n",
      "\n",
      "class MonitoredQdrantHandshake:\n",
      "    def __init__(self, handshake):\n",
      "        self.handshake = handshake\n",
      "        self.metrics = defaultdict(int)\n",
      "\n",
      "    def write(self, chunks):\n",
      "        start = time.time()\n",
      "        self.handshake.write(chunks)\n",
      "        self.metrics['write_time'] += time.time() - start\n",
      "        self.metrics['chunks_written'] += len(chunks)\n",
      "\n",
      "    def search(self, query, limit):\n",
      "        start = time.time()\n",
      "        results = self.handshake.search(query, limit)\n",
      "        self.metrics['search_time'] += time.time() - start\n",
      "        self.metrics['searches'] += 1\n",
      "        return results\n",
      "\n",
      "    def get_metrics(self):\n",
      "        return dict(self.metrics)\n",
      "\n",
      "\n",
      "üìä Key Metrics to Track:\n",
      "  - Chunks written per second\n",
      "  - Average search latency\n",
      "  - Search result relevance scores\n",
      "  - Collection size and growth\n",
      "  - Embedding generation time\n",
      "  - Memory usage\n",
      "  - Error rates\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Monitoring & Observability\\n\")\n",
    "\n",
    "# Create monitored handshake\n",
    "monitor_handshake = QdrantHandshake(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"monitored\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Metrics tracking\n",
    "metrics = {\n",
    "    \"chunks_written\": 0,\n",
    "    \"searches_performed\": 0,\n",
    "    \"avg_search_time\": 0,\n",
    "    \"total_documents\": 0\n",
    "}\n",
    "\n",
    "# Sample operation with metrics\n",
    "print(\"üìù Ingestion Metrics:\\n\")\n",
    "\n",
    "chunker = TokenChunker(chunk_size=100)\n",
    "docs_to_ingest = [\n",
    "    \"Document about machine learning and artificial intelligence.\",\n",
    "    \"Content on natural language processing and text analysis.\",\n",
    "    \"Information about computer vision and image recognition.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(docs_to_ingest, 1):\n",
    "    chunks = chunker.chunk(text)\n",
    "    monitor_handshake.write(chunks)\n",
    "    metrics[\"chunks_written\"] += len(chunks)\n",
    "    metrics[\"total_documents\"] += 1\n",
    "    print(f\"   Doc {i}: {len(chunks)} chunks written\")\n",
    "\n",
    "print(f\"\\n   Total: {metrics['chunks_written']} chunks from {metrics['total_documents']} docs\")\n",
    "\n",
    "# Search metrics\n",
    "print(\"\\nüîç Search Metrics:\\n\")\n",
    "\n",
    "queries = [\n",
    "    \"machine learning\",\n",
    "    \"text processing\",\n",
    "    \"image recognition\"\n",
    "]\n",
    "\n",
    "search_times = []\n",
    "for query in queries:\n",
    "    start = time.time()\n",
    "    results = monitor_handshake.search(query=query, limit=3)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    search_times.append(elapsed)\n",
    "    metrics[\"searches_performed\"] += 1\n",
    "    \n",
    "    print(f\"   '{query}': {len(results)} results in {elapsed*1000:.1f}ms\")\n",
    "\n",
    "metrics[\"avg_search_time\"] = sum(search_times) / len(search_times)\n",
    "\n",
    "# Summary metrics\n",
    "print(\"\\nüìà Summary Metrics:\")\n",
    "print(f\"   Total documents: {metrics['total_documents']}\")\n",
    "print(f\"   Total chunks: {metrics['chunks_written']}\")\n",
    "print(f\"   Avg chunks per doc: {metrics['chunks_written']/metrics['total_documents']:.1f}\")\n",
    "print(f\"   Total searches: {metrics['searches_performed']}\")\n",
    "print(f\"   Avg search time: {metrics['avg_search_time']*1000:.1f}ms\")\n",
    "\n",
    "print(\"\\nüí° Production Monitoring:\")\n",
    "print(\"\"\"\n",
    "class MonitoredQdrantHandshake:\n",
    "    def __init__(self, handshake):\n",
    "        self.handshake = handshake\n",
    "        self.metrics = defaultdict(int)\n",
    "    \n",
    "    def write(self, chunks):\n",
    "        start = time.time()\n",
    "        self.handshake.write(chunks)\n",
    "        self.metrics['write_time'] += time.time() - start\n",
    "        self.metrics['chunks_written'] += len(chunks)\n",
    "    \n",
    "    def search(self, query, limit):\n",
    "        start = time.time()\n",
    "        results = self.handshake.search(query, limit)\n",
    "        self.metrics['search_time'] += time.time() - start\n",
    "        self.metrics['searches'] += 1\n",
    "        return results\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        return dict(self.metrics)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Key Metrics to Track:\")\n",
    "print(\"  - Chunks written per second\")\n",
    "print(\"  - Average search latency\")\n",
    "print(\"  - Search result relevance scores\")\n",
    "print(\"  - Collection size and growth\")\n",
    "print(\"  - Embedding generation time\")\n",
    "print(\"  - Memory usage\")\n",
    "print(\"  - Error rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704233f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Chonkie + Qdrant Integration\n",
    "\n",
    "### QdrantHandshake Overview\n",
    "\n",
    "The `QdrantHandshake` provides seamless integration between Chonkie and Qdrant vector database for RAG applications.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**1. Initialization Options**:\n",
    "```python\n",
    "# In-memory (development)\n",
    "QdrantHandshake(url=\":memory:\", collection_name=\"demo\")\n",
    "\n",
    "# Persistent local\n",
    "QdrantHandshake(path=\"./storage\", collection_name=\"data\")\n",
    "\n",
    "# Local server\n",
    "QdrantHandshake(url=\"http://localhost:6333\", collection_name=\"docs\")\n",
    "\n",
    "# Qdrant Cloud\n",
    "QdrantHandshake(\n",
    "    url=\"https://xyz.qdrant.io\",\n",
    "    api_key=\"key\",\n",
    "    collection_name=\"prod\"\n",
    ")\n",
    "```\n",
    "\n",
    "**2. Core Operations**:\n",
    "- `write(chunks)` - Store chunks with automatic embeddings\n",
    "- `search(query, limit)` - Semantic search with natural language\n",
    "- Metadata stored with chunks for post-search filtering\n",
    "\n",
    "**3. Pipeline Integration**:\n",
    "```python\n",
    "Pipeline()\n",
    "    .fetch_from(\"file\", dir=\"./docs\")\n",
    "    .chunk_with(\"semantic\", chunk_size=512)\n",
    "    .refine_with(\"overlap\", context_size=100)\n",
    "    .store_in(\"qdrant\",\n",
    "              client=qdrant_client,\n",
    "              collection_name=\"knowledge\",\n",
    "              embedding_model=\"all-MiniLM-L6-v2\")\n",
    "    .run()\n",
    "```\n",
    "\n",
    "### Storage Options\n",
    "\n",
    "| Type | URL | Persistence | Use Case |\n",
    "|------|-----|-------------|----------|\n",
    "| In-Memory | `:memory:` | ‚ùå Ephemeral | Testing, notebooks |\n",
    "| Local Persistent | Path string | ‚úÖ Yes | Single machine apps |\n",
    "| Local Server | `http://localhost:6333` | ‚úÖ Yes | Development with Docker |\n",
    "| Qdrant Cloud | `https://...qdrant.io` | ‚úÖ Yes | Production deployments |\n",
    "\n",
    "### Embedding Models\n",
    "\n",
    "**Lightweight** (Development):\n",
    "- `sentence-transformers/all-MiniLM-L6-v2` (384-dim)\n",
    "- `minishlab/potion-base-8M` (256-dim)\n",
    "\n",
    "**High-Quality** (Production):\n",
    "- `sentence-transformers/all-mpnet-base-v2` (768-dim)\n",
    "- `BAAI/bge-small-en-v1.5` (384-dim)\n",
    "\n",
    "**Specialized**:\n",
    "- `paraphrase-multilingual-MiniLM-L12-v2` (Multi-lingual)\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "**1. Basic RAG**:\n",
    "```python\n",
    "# Ingestion\n",
    "handshake = QdrantHandshake(url=\":memory:\", collection_name=\"kb\")\n",
    "chunks = chunker.chunk(text)\n",
    "handshake.write(chunks)\n",
    "\n",
    "# Retrieval\n",
    "results = handshake.search(\"your question\", limit=5)\n",
    "```\n",
    "\n",
    "**2. Complete Pipeline**:\n",
    "```python\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=\"./docs\")\n",
    "    .chunk_with(\"semantic\", threshold=0.75)\n",
    "    .refine_with(\"overlap\", context_size=50)\n",
    "    .store_in(\"qdrant\", collection_name=\"kb\")\n",
    "    .run())\n",
    "```\n",
    "\n",
    "**3. Metadata Storage**:\n",
    "```python\n",
    "# Add metadata during chunking\n",
    "chunk.metadata = {\"category\": \"technical\", \"author\": \"john\"}\n",
    "\n",
    "# Store with metadata\n",
    "handshake.write(chunks)\n",
    "\n",
    "# Search and filter results by metadata\n",
    "results = handshake.search(query=\"deployment\", limit=10)\n",
    "filtered = [r for r in results if r.get('metadata', {}).get('category') == 'technical']\n",
    "```\n",
    "\n",
    "**4. Batch Operations**:\n",
    "```python\n",
    "# Collect all chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    all_chunks.extend(chunker.chunk(doc))\n",
    "\n",
    "# Single batch write\n",
    "handshake.write(all_chunks)\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "‚úÖ **Chunk Size**: Larger chunks (200-500 tokens) = fewer embeddings = faster\n",
    "‚úÖ **Batch Writes**: Write multiple chunks at once instead of individually\n",
    "‚úÖ **Model Selection**: Use smaller models for speed, larger for quality\n",
    "‚úÖ **Metadata Filters**: Reduce search space with targeted filters\n",
    "‚úÖ **Limit Results**: Request only what you need (limit=5-10)\n",
    "‚úÖ **Caching**: Cache frequent queries at application level\n",
    "‚úÖ **Persistent Storage**: Use for important data, in-memory for temporary\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "**Configuration**:\n",
    "- ‚úÖ Use environment variables for URLs and keys\n",
    "- ‚úÖ Choose appropriate embedding model\n",
    "- ‚úÖ Set up persistent storage or cloud instance\n",
    "- ‚úÖ Configure proper collection names\n",
    "\n",
    "**Error Handling**:\n",
    "- ‚úÖ Validate inputs before operations\n",
    "- ‚úÖ Handle empty results gracefully\n",
    "- ‚úÖ Implement retry logic for transient failures\n",
    "- ‚úÖ Log errors with context\n",
    "\n",
    "**Monitoring**:\n",
    "- ‚úÖ Track ingestion rate (chunks/second)\n",
    "- ‚úÖ Monitor search latency (ms)\n",
    "- ‚úÖ Log relevance scores\n",
    "- ‚úÖ Track collection growth\n",
    "- ‚úÖ Alert on error rates\n",
    "\n",
    "**Security**:\n",
    "- ‚úÖ Secure API keys (environment variables)\n",
    "- ‚úÖ Use HTTPS for cloud connections\n",
    "- ‚úÖ Implement authentication\n",
    "- ‚úÖ Validate user inputs\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "**1. Document Q&A**: Technical documentation, knowledge bases\n",
    "**2. Code Search**: Semantic code repository search\n",
    "**3. Content Discovery**: Blog posts, articles, research papers\n",
    "**4. Multi-lingual Search**: Cross-language content retrieval\n",
    "**5. Customer Support**: FAQ and support documentation\n",
    "**6. E-commerce**: Product search and recommendations\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Development**: Start with in-memory Qdrant\n",
    "‚úÖ **Testing**: Use small, fast embedding models\n",
    "‚úÖ **Staging**: Test with persistent local storage\n",
    "‚úÖ **Production**: Deploy to Qdrant Cloud or managed instance\n",
    "‚úÖ **Monitoring**: Track metrics from day one\n",
    "‚úÖ **Documentation**: Document collection schemas and metadata\n",
    "‚úÖ **Backups**: Regular backups of persistent collections\n",
    "‚úÖ **Versioning**: Version your embedding models and schemas\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "üîó **Resources**:\n",
    "- [Chonkie Documentation](https://docs.chonkie.ai/)\n",
    "- [QdrantHandshake API](https://docs.chonkie.ai/oss/handshakes/qdrant-handshake)\n",
    "- [Qdrant Documentation](https://qdrant.tech/documentation/)\n",
    "- [Qdrant Python Client](https://python-client.qdrant.tech/)\n",
    "\n",
    "üöÄ **Try Next**:\n",
    "- Deploy to Qdrant Cloud\n",
    "- Experiment with different embedding models\n",
    "- Build complete RAG application\n",
    "- Implement hybrid search (dense + sparse)\n",
    "- Add reranking for better results\n",
    "- Integrate with LLM for generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5c99b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the **Chonkie + Qdrant Integration** tutorial!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "‚úÖ **QdrantHandshake Basics** - Store and search chunks\n",
    "‚úÖ **In-Memory Qdrant** - No Docker required for development\n",
    "‚úÖ **Pipeline Integration** - Fluent API with `.store_in()`\n",
    "‚úÖ **Semantic Search** - Natural language queries\n",
    "‚úÖ **Metadata Filters** - Targeted search\n",
    "‚úÖ **Batch Operations** - Efficient large-scale ingestion\n",
    "‚úÖ **Custom Embeddings** - Choose the right model\n",
    "‚úÖ **Real-World Patterns** - Q&A, code search, multi-lingual\n",
    "‚úÖ **Production Ready** - Error handling, monitoring, optimization\n",
    "‚úÖ **Storage Options** - In-memory, persistent, cloud\n",
    "\n",
    "### Build Amazing RAG Applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chonkie (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
