{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3c9f15",
   "metadata": {},
   "source": [
    "# Chonkie FileFetcher - Complete Guide\n",
    "\n",
    "This notebook demonstrates all possible combinations and usage patterns for the **FileFetcher** in Chonkie.\n",
    "\n",
    "## What is FileFetcher?\n",
    "\n",
    "FileFetcher retrieves files from your local filesystem for pipeline processing. It supports:\n",
    "- **Single file mode**: Fetch a specific file by path\n",
    "- **Directory mode**: Fetch multiple files from a directory\n",
    "- **Extension filtering**: Filter files by extension when using directory mode\n",
    "\n",
    "## Key Features:\n",
    "- âœ… Fetch single files or entire directories\n",
    "- âœ… Filter by file extensions (`.txt`, `.md`, `.pdf`, etc.)\n",
    "- âœ… Works both in pipelines and standalone\n",
    "- âœ… Clear error handling and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f85bd",
   "metadata": {},
   "source": [
    "## Visual Overview\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#ff6b6b','primaryTextColor':'#fff','primaryBorderColor':'#c92a2a','lineColor':'#339af0','secondaryColor':'#51cf66','tertiaryColor':'#ffd43b','background':'#f8f9fa','mainBkg':'#e3fafc','secondBkg':'#fff3bf','tertiaryBkg':'#ffe3e3','textColor':'#212529','fontSize':'16px'}}}%%\n",
    "\n",
    "graph TB\n",
    "    Start([ğŸ¯ FileFetcher<br/>Entry Point]):::startClass\n",
    "    \n",
    "    Start --> Mode{Choose Mode}:::decisionClass\n",
    "    \n",
    "    Mode -->|path parameter| Single[ğŸ“„ Single File Mode<br/>path='file.txt']:::singleClass\n",
    "    Mode -->|dir parameter| Dir[ğŸ“ Directory Mode<br/>dir='./folder']:::dirClass\n",
    "    \n",
    "    Single --> SingleReturn[Returns: Path Object]:::returnClass\n",
    "    \n",
    "    Dir --> Filter{Apply Extension<br/>Filter?}:::decisionClass\n",
    "    \n",
    "    Filter -->|No filter| AllFiles[ğŸ“¦ Fetch All Files<br/>All extensions]:::allClass\n",
    "    Filter -->|Single ext| OneExt[\"ğŸ“ Filter Single Type<br/>ext=.txt\"]:::filterClass\n",
    "    Filter -->|Multiple ext| MultiExt[\"ğŸ¯ Filter Multiple Types<br/>ext=.txt, .md\"]:::filterClass\n",
    "    \n",
    "    AllFiles --> DirReturn[Returns: list of Path]:::returnClass\n",
    "    OneExt --> DirReturn\n",
    "    MultiExt --> DirReturn\n",
    "    \n",
    "    SingleReturn --> Usage{Usage Context}:::decisionClass\n",
    "    DirReturn --> Usage\n",
    "    \n",
    "    Usage -->|Standalone| Standalone[ğŸ”§ Direct Usage<br/>fetcher.fetch&#40;...&#41;]:::standaloneClass\n",
    "    Usage -->|Pipeline| Pipeline[âš¡ Pipeline Integration<br/>.fetch_from&#40;'file', ...&#41;]:::pipelineClass\n",
    "    \n",
    "    Standalone --> StandaloneOut[Process Path Objects]:::outputClass\n",
    "    Pipeline --> PipelineOut[Process Documents<br/>with Chunks]:::outputClass\n",
    "    \n",
    "    Start -.->|Error Cases| Errors[âš ï¸ Error Handling]:::errorClass\n",
    "    Errors --> E1[âŒ FileNotFoundError<br/>File doesn't exist]:::errorDetailClass\n",
    "    Errors --> E2[âŒ ValueError<br/>Both path & dir]:::errorDetailClass\n",
    "    Errors --> E3[âŒ ValueError<br/>Neither path nor dir]:::errorDetailClass\n",
    "    \n",
    "    classDef startClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "    classDef decisionClass fill:#7950f2,stroke:#5f3dc4,stroke-width:2px,color:#fff\n",
    "    classDef singleClass fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:#fff\n",
    "    classDef dirClass fill:#fa5252,stroke:#e03131,stroke-width:2px,color:#fff\n",
    "    classDef allClass fill:#51cf66,stroke:#37b24d,stroke-width:2px,color:#fff\n",
    "    classDef filterClass fill:#20c997,stroke:#087f5b,stroke-width:2px,color:#fff\n",
    "    classDef returnClass fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#333\n",
    "    classDef standaloneClass fill:#ff922b,stroke:#e8590c,stroke-width:2px,color:#fff\n",
    "    classDef pipelineClass fill:#748ffc,stroke:#4c6ef5,stroke-width:2px,color:#fff\n",
    "    classDef outputClass fill:#69db7c,stroke:#40c057,stroke-width:2px,color:#fff\n",
    "    classDef errorClass fill:#ff8787,stroke:#fa5252,stroke-width:3px,color:#fff\n",
    "    classDef errorDetailClass fill:#ffc9c9,stroke:#ff6b6b,stroke-width:1px,color:#c92a2a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26459f",
   "metadata": {},
   "source": [
    "## Setup - Create Mock Files\n",
    "\n",
    "First, let's create sample files to demonstrate the fetcher capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5492edea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created test directory structure:\n",
      "ğŸ“ test_fetcher_files/\n",
      "ğŸ“ test_fetcher_files/\n",
      "  ğŸ“„ single_file.txt\n",
      "  ğŸ“ documents/\n",
      "    ğŸ“„ data.json\n",
      "    ğŸ“„ doc1.txt\n",
      "    ğŸ“„ doc2.txt\n",
      "    ğŸ“„ doc3.txt\n",
      "    ğŸ“„ guide.md\n",
      "    ğŸ“„ readme.md\n",
      "    ğŸ“ subfolder/\n",
      "      ğŸ“„ nested.txt\n",
      "  ğŸ“ mixed_files/\n",
      "    ğŸ“„ config.json\n",
      "    ğŸ“„ notes.md\n",
      "    ğŸ“„ script.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create test directories\n",
    "test_dir = Path(\"./test_fetcher_files\")\n",
    "subdirs = [\"documents\", \"documents/subfolder\", \"mixed_files\"]\n",
    "\n",
    "# Clean up if exists\n",
    "if test_dir.exists():\n",
    "    shutil.rmtree(test_dir)\n",
    "\n",
    "# Create directory structure\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "for subdir in subdirs:\n",
    "    (test_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create sample text files\n",
    "sample_texts = {\n",
    "    \"single_file.txt\": \"This is a standalone text file for testing single file mode.\",\n",
    "    \"documents/doc1.txt\": \"Document 1 contains information about machine learning algorithms.\",\n",
    "    \"documents/doc2.txt\": \"Document 2 discusses natural language processing techniques.\",\n",
    "    \"documents/doc3.txt\": \"Document 3 explores computer vision applications.\",\n",
    "    \"documents/subfolder/nested.txt\": \"This is a nested document in a subfolder.\",\n",
    "}\n",
    "\n",
    "# Create sample markdown files\n",
    "sample_markdown = {\n",
    "    \"documents/readme.md\": \"# README\\n\\nThis is a markdown readme file.\",\n",
    "    \"documents/guide.md\": \"# Guide\\n\\n## Section 1\\n\\nDetailed guide content here.\",\n",
    "    \"mixed_files/notes.md\": \"# Notes\\n\\n- Point 1\\n- Point 2\\n- Point 3\",\n",
    "}\n",
    "\n",
    "# Create sample JSON files\n",
    "sample_json = {\n",
    "    \"documents/data.json\": '{\"name\": \"test\", \"value\": 123}',\n",
    "    \"mixed_files/config.json\": '{\"setting\": \"enabled\", \"count\": 5}',\n",
    "}\n",
    "\n",
    "# Create sample Python files\n",
    "sample_python = {\n",
    "    \"mixed_files/script.py\": \"def hello():\\n    print('Hello, World!')\\n\",\n",
    "}\n",
    "\n",
    "# Write all files\n",
    "all_files = {**sample_texts, **sample_markdown, **sample_json, **sample_python}\n",
    "for file_path, content in all_files.items():\n",
    "    full_path = test_dir / file_path\n",
    "    full_path.write_text(content, encoding='utf-8')\n",
    "\n",
    "print(\"âœ… Created test directory structure:\")\n",
    "print(f\"ğŸ“ {test_dir}/\")\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    level = root.replace(str(test_dir), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}ğŸ“ {os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}ğŸ“„ {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79107d15",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Make sure Chonkie is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7343d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chonkie imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install chonkie if not already installed\n",
    "# !pip install chonkie\n",
    "\n",
    "# Import required modules\n",
    "from chonkie import FileFetcher\n",
    "from chonkie.pipeline import Pipeline\n",
    "\n",
    "print(\"âœ… Chonkie imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda71d65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Standalone Usage - Single File Mode\n",
    "\n",
    "Fetch a single file by providing the `path` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fc69c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Single File Fetched:\n",
      "  Type: <class 'pathlib.WindowsPath'>\n",
      "  Path: test_fetcher_files\\single_file.txt\n",
      "  Exists: True\n",
      "  Name: single_file.txt\n",
      "  Suffix: .txt\n",
      "\n",
      "ğŸ“– Content:\n",
      "  This is a standalone text file for testing single file mode.\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Fetch a single file\n",
    "single_file = fetcher.fetch(path=\"./test_fetcher_files/single_file.txt\")\n",
    "\n",
    "print(\"ğŸ“„ Single File Fetched:\")\n",
    "print(f\"  Type: {type(single_file)}\")\n",
    "print(f\"  Path: {single_file}\")\n",
    "print(f\"  Exists: {single_file.exists()}\")\n",
    "print(f\"  Name: {single_file.name}\")\n",
    "print(f\"  Suffix: {single_file.suffix}\")\n",
    "print(f\"\\nğŸ“– Content:\")\n",
    "print(f\"  {single_file.read_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d7287",
   "metadata": {},
   "source": [
    "## 2. Standalone Usage - Directory Mode (All Files)\n",
    "\n",
    "Fetch all files from a directory without filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cbf8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Fetched 7 files from directory:\n",
      "  ğŸ“„ data.json (.json)\n",
      "  ğŸ“„ doc1.txt (.txt)\n",
      "  ğŸ“„ doc2.txt (.txt)\n",
      "  ğŸ“„ doc3.txt (.txt)\n",
      "  ğŸ“„ guide.md (.md)\n",
      "  ğŸ“„ readme.md (.md)\n",
      "  ğŸ“„ nested.txt (.txt)\n",
      "\n",
      "ğŸ“Š File Type Summary:\n",
      "  .json: 1 file(s)\n",
      "  .txt: 4 file(s)\n",
      "  .md: 2 file(s)\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Fetch all files from a directory\n",
    "all_files = fetcher.fetch(dir=\"./test_fetcher_files/documents\")\n",
    "\n",
    "print(f\"ğŸ“ Fetched {len(all_files)} files from directory:\")\n",
    "for file in all_files:\n",
    "    print(f\"  ğŸ“„ {file.name} ({file.suffix})\")\n",
    "    \n",
    "print(f\"\\nğŸ“Š File Type Summary:\")\n",
    "from collections import Counter\n",
    "extensions = Counter([f.suffix for f in all_files])\n",
    "for ext, count in extensions.items():\n",
    "    print(f\"  {ext or 'no extension'}: {count} file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce4fb0",
   "metadata": {},
   "source": [
    "## 3. Standalone Usage - Directory Mode with Single Extension Filter\n",
    "\n",
    "Fetch only files with a specific extension (e.g., only `.txt` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b59e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Fetched 4 .txt files:\n",
      "  ğŸ“„ doc1.txt\n",
      "     Preview: Document 1 contains information about machine lear...\n",
      "\n",
      "  ğŸ“„ doc2.txt\n",
      "     Preview: Document 2 discusses natural language processing t...\n",
      "\n",
      "  ğŸ“„ doc3.txt\n",
      "     Preview: Document 3 explores computer vision applications....\n",
      "\n",
      "  ğŸ“„ nested.txt\n",
      "     Preview: This is a nested document in a subfolder....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Fetch only .txt files\n",
    "txt_files = fetcher.fetch(dir=\"./test_fetcher_files/documents\", ext=[\".txt\"])\n",
    "\n",
    "print(f\"ğŸ“ Fetched {len(txt_files)} .txt files:\")\n",
    "for file in txt_files:\n",
    "    print(f\"  ğŸ“„ {file.name}\")\n",
    "    # Show a preview of content\n",
    "    content = file.read_text()[:50]\n",
    "    print(f\"     Preview: {content}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c789db4",
   "metadata": {},
   "source": [
    "## 4. Standalone Usage - Directory Mode with Multiple Extension Filters\n",
    "\n",
    "Fetch files with multiple extensions (e.g., `.txt` and `.md` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111df22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Fetched 6 text/markdown files:\n",
      "  ğŸ“„ doc1.txt (.txt)\n",
      "  ğŸ“„ doc2.txt (.txt)\n",
      "  ğŸ“„ doc3.txt (.txt)\n",
      "  ğŸ“„ guide.md (.md)\n",
      "  ğŸ“„ readme.md (.md)\n",
      "  ğŸ“„ nested.txt (.txt)\n",
      "\n",
      "ğŸ“Š Breakdown:\n",
      "  .txt files: 4\n",
      "  .md files: 2\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Fetch both .txt and .md files\n",
    "text_files = fetcher.fetch(dir=\"./test_fetcher_files/documents\", ext=[\".txt\", \".md\"])\n",
    "\n",
    "print(f\"ğŸ“ Fetched {len(text_files)} text/markdown files:\")\n",
    "for file in text_files:\n",
    "    print(f\"  ğŸ“„ {file.name} ({file.suffix})\")\n",
    "\n",
    "# Group by extension\n",
    "txt_count = sum(1 for f in text_files if f.suffix == \".txt\")\n",
    "md_count = sum(1 for f in text_files if f.suffix == \".md\")\n",
    "print(f\"\\nğŸ“Š Breakdown:\")\n",
    "print(f\"  .txt files: {txt_count}\")\n",
    "print(f\"  .md files: {md_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8111f47",
   "metadata": {},
   "source": [
    "## 5. Standalone Usage - Mixed Directory with Various Extensions\n",
    "\n",
    "Demonstrate fetching different file types from a directory with mixed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca2c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Markdown files: 1\n",
      "  - notes.md\n",
      "\n",
      "ğŸ“¦ JSON files: 1\n",
      "  - config.json\n",
      "\n",
      "ğŸ Python files: 1\n",
      "  - script.py\n",
      "\n",
      "ğŸ¯ Multiple extensions (.md, .json, .py): 3 files\n",
      "  - config.json (.json)\n",
      "  - notes.md (.md)\n",
      "  - script.py (.py)\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Example 1: Fetch only markdown files\n",
    "md_files = fetcher.fetch(dir=\"./test_fetcher_files/mixed_files\", ext=[\".md\"])\n",
    "print(f\"ğŸ“ Markdown files: {len(md_files)}\")\n",
    "for f in md_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Example 2: Fetch only JSON files\n",
    "json_files = fetcher.fetch(dir=\"./test_fetcher_files/mixed_files\", ext=[\".json\"])\n",
    "print(f\"\\nğŸ“¦ JSON files: {len(json_files)}\")\n",
    "for f in json_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Example 3: Fetch Python files\n",
    "py_files = fetcher.fetch(dir=\"./test_fetcher_files/mixed_files\", ext=[\".py\"])\n",
    "print(f\"\\nğŸ Python files: {len(py_files)}\")\n",
    "for f in py_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Example 4: Fetch multiple types at once\n",
    "multi_files = fetcher.fetch(dir=\"./test_fetcher_files/mixed_files\", ext=[\".md\", \".json\", \".py\"])\n",
    "print(f\"\\nğŸ¯ Multiple extensions (.md, .json, .py): {len(multi_files)} files\")\n",
    "for f in multi_files:\n",
    "    print(f\"  - {f.name} ({f.suffix})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14def8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Pipeline Integration - Single File Mode\n",
    "\n",
    "Use FileFetcher within a Chonkie pipeline to fetch, process, and chunk a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7eb365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Single File Pipeline Result:\n",
      "  Document: N/A\n",
      "  Number of chunks: 3\n",
      "\n",
      "ğŸ“ Chunks:\n",
      "  Chunk 1: This is a standalone text...\n",
      "  Chunk 2:  file for testing single file...\n",
      "  Chunk 3:  mode....\n"
     ]
    }
   ],
   "source": [
    "# Pipeline with single file\n",
    "doc = (Pipeline()\n",
    "    .fetch_from(\"file\", path=\"./test_fetcher_files/single_file.txt\")\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"recursive\", chunk_size=30)  # Small chunk size for demo\n",
    "    .run())\n",
    "\n",
    "print(f\"ğŸ“„ Single File Pipeline Result:\")\n",
    "print(f\"  Document: {doc.source if hasattr(doc, 'source') else 'N/A'}\")\n",
    "print(f\"  Number of chunks: {len(doc.chunks)}\")\n",
    "print(f\"\\nğŸ“ Chunks:\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ba05e",
   "metadata": {},
   "source": [
    "## 7. Pipeline Integration - Directory Mode (All Files)\n",
    "\n",
    "Fetch all files from a directory and process them through a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41dba5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Directory Pipeline Result:\n",
      "  Processed 7 documents\n",
      "\n",
      "  1. Document 1\n",
      "     Chunks: 1\n",
      "     First chunk: {\"name\": \"test\", \"value\": 123}...\n",
      "\n",
      "  2. Document 2\n",
      "     Chunks: 2\n",
      "     First chunk: Document 1 contains information about ma...\n",
      "\n",
      "  3. Document 3\n",
      "     Chunks: 2\n",
      "     First chunk: Document 2 discusses natural language pr...\n",
      "\n",
      "  4. Document 4\n",
      "     Chunks: 1\n",
      "     First chunk: Document 3 explores computer vision appl...\n",
      "\n",
      "  5. Document 5\n",
      "     Chunks: 2\n",
      "     First chunk: # Guide\n",
      "\n",
      "## Section 1\n",
      "\n",
      "Detailed guide co...\n",
      "\n",
      "  6. Document 6\n",
      "     Chunks: 1\n",
      "     First chunk: # README\n",
      "\n",
      "This is a markdown readme file...\n",
      "\n",
      "  7. Document 7\n",
      "     Chunks: 1\n",
      "     First chunk: This is a nested document in a subfolder...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pipeline with directory (all files)\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=\"./test_fetcher_files/documents\")\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "\n",
    "print(f\"ğŸ“ Directory Pipeline Result:\")\n",
    "print(f\"  Processed {len(docs)} documents\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    source_name = Path(doc.source).name if hasattr(doc, 'source') else f\"Document {i}\"\n",
    "    print(f\"  {i}. {source_name}\")\n",
    "    print(f\"     Chunks: {len(doc.chunks)}\")\n",
    "    if len(doc.chunks) > 0:\n",
    "        print(f\"     First chunk: {doc.chunks[0].text[:40]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861613c2",
   "metadata": {},
   "source": [
    "## 8. Pipeline Integration - Directory Mode with Extension Filter\n",
    "\n",
    "Fetch only specific file types from a directory and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7290a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Filtered Pipeline Result (.txt and .md only):\n",
      "  Processed 6 documents\n",
      "\n",
      "ğŸ“„ Text files processed: 0\n",
      "\n",
      "ğŸ“ Markdown files processed: 0\n"
     ]
    }
   ],
   "source": [
    "# Pipeline with filtered extensions\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=\"./test_fetcher_files/documents\", ext=[\".txt\", \".md\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "\n",
    "print(f\"ğŸ“ Filtered Pipeline Result (.txt and .md only):\")\n",
    "print(f\"  Processed {len(docs)} documents\\n\")\n",
    "\n",
    "# Group by extension\n",
    "txt_docs = []\n",
    "md_docs = []\n",
    "\n",
    "for doc in docs:\n",
    "    if hasattr(doc, 'source'):\n",
    "        source_path = Path(doc.source)\n",
    "        if source_path.suffix == \".txt\":\n",
    "            txt_docs.append(doc)\n",
    "        elif source_path.suffix == \".md\":\n",
    "            md_docs.append(doc)\n",
    "\n",
    "print(f\"ğŸ“„ Text files processed: {len(txt_docs)}\")\n",
    "for doc in txt_docs:\n",
    "    print(f\"   - {Path(doc.source).name}: {len(doc.chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nğŸ“ Markdown files processed: {len(md_docs)}\")\n",
    "for doc in md_docs:\n",
    "    print(f\"   - {Path(doc.source).name}: {len(doc.chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935a91e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Error Handling - Invalid Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68a8ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ FileNotFoundError (expected): File not found: ./nonexistent_file.txt\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Test 1: File not found\n",
    "try:\n",
    "    fetcher.fetch(path=\"./nonexistent_file.txt\")\n",
    "    print(\"âœ… No error (unexpected)\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ FileNotFoundError (expected): {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Other error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf2534",
   "metadata": {},
   "source": [
    "## 10. Error Handling - Both Path and Dir Provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94c9b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ValueError (expected): Provide either 'path' or 'dir', not both\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Test 2: Both path and dir provided (should raise ValueError)\n",
    "try:\n",
    "    fetcher.fetch(path=\"./test_fetcher_files/single_file.txt\", dir=\"./test_fetcher_files/documents\")\n",
    "    print(\"âœ… No error (unexpected)\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ ValueError (expected): {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Other error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f489a4",
   "metadata": {},
   "source": [
    "## 11. Error Handling - Neither Path nor Dir Provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7c1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ValueError (expected): Must provide either 'path' or 'dir'\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Test 3: Neither path nor dir provided (should raise ValueError)\n",
    "try:\n",
    "    fetcher.fetch()\n",
    "    print(\"âœ… No error (unexpected)\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ ValueError (expected): {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Other error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a11f2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Advanced Use Case - Processing Multiple Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f044167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ./test_fetcher_files/documents: 7 files\n",
      "ğŸ“ ./test_fetcher_files/mixed_files: 3 files\n",
      "\n",
      "ğŸ“Š Total files from all directories: 10\n",
      "\n",
      "ğŸ“‹ All files:\n",
      "  - documents\\data.json (.json)\n",
      "  - documents\\doc1.txt (.txt)\n",
      "  - documents\\doc2.txt (.txt)\n",
      "  - documents\\doc3.txt (.txt)\n",
      "  - documents\\guide.md (.md)\n",
      "  - documents\\readme.md (.md)\n",
      "  - documents\\subfolder\\nested.txt (.txt)\n",
      "  - mixed_files\\config.json (.json)\n",
      "  - mixed_files\\notes.md (.md)\n",
      "  - mixed_files\\script.py (.py)\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Fetch from multiple directories and combine\n",
    "directories = [\n",
    "    \"./test_fetcher_files/documents\",\n",
    "    \"./test_fetcher_files/mixed_files\"\n",
    "]\n",
    "\n",
    "all_files = []\n",
    "for dir_path in directories:\n",
    "    files = fetcher.fetch(dir=dir_path)\n",
    "    all_files.extend(files)\n",
    "    print(f\"ğŸ“ {dir_path}: {len(files)} files\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total files from all directories: {len(all_files)}\")\n",
    "print(\"\\nğŸ“‹ All files:\")\n",
    "for file in all_files:\n",
    "    print(f\"  - {file.relative_to('./test_fetcher_files')} ({file.suffix})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b332ffa",
   "metadata": {},
   "source": [
    "## 13. Advanced Use Case - Selective Processing by File Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c8bd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Processing Strategy:\n",
      "\n",
      "ğŸ“„ Text files (4): Use larger chunks for analysis\n",
      "   - doc1.txt: 66 characters\n",
      "   - doc2.txt: 60 characters\n",
      "   - doc3.txt: 49 characters\n",
      "   - nested.txt: 41 characters\n",
      "\n",
      "ğŸ“ Markdown files (2): Use smaller chunks for semantic search\n",
      "   - guide.md: 51 characters, 3 headings\n",
      "   - readme.md: 41 characters, 1 headings\n",
      "\n",
      "ğŸ’¡ Use Case: Process txt files with chunk_size=512, md files with chunk_size=256\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "\n",
    "# Example: Process text files differently than markdown files\n",
    "txt_files = fetcher.fetch(dir=\"./test_fetcher_files/documents\", ext=[\".txt\"])\n",
    "md_files = fetcher.fetch(dir=\"./test_fetcher_files/documents\", ext=[\".md\"])\n",
    "\n",
    "print(\"ğŸ” Processing Strategy:\\n\")\n",
    "\n",
    "print(f\"ğŸ“„ Text files ({len(txt_files)}): Use larger chunks for analysis\")\n",
    "for file in txt_files:\n",
    "    content = file.read_text()\n",
    "    print(f\"   - {file.name}: {len(content)} characters\")\n",
    "\n",
    "print(f\"\\nğŸ“ Markdown files ({len(md_files)}): Use smaller chunks for semantic search\")\n",
    "for file in md_files:\n",
    "    content = file.read_text()\n",
    "    # Count headings\n",
    "    headings = content.count('#')\n",
    "    print(f\"   - {file.name}: {len(content)} characters, {headings} headings\")\n",
    "\n",
    "# Could process each type differently in pipelines\n",
    "print(\"\\nğŸ’¡ Use Case: Process txt files with chunk_size=512, md files with chunk_size=256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60591bc",
   "metadata": {},
   "source": [
    "## 14. Advanced Use Case - File Metadata Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712bb895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š File Metadata Analysis:\n",
      "\n",
      "ğŸ“„ data.json\n",
      "   Size: 30 bytes\n",
      "   Type: .json\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\data.json\n",
      "\n",
      "ğŸ“„ doc1.txt\n",
      "   Size: 66 bytes\n",
      "   Type: .txt\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\doc1.txt\n",
      "\n",
      "ğŸ“„ doc2.txt\n",
      "   Size: 60 bytes\n",
      "   Type: .txt\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\doc2.txt\n",
      "\n",
      "ğŸ“„ doc3.txt\n",
      "   Size: 49 bytes\n",
      "   Type: .txt\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\doc3.txt\n",
      "\n",
      "ğŸ“„ guide.md\n",
      "   Size: 55 bytes\n",
      "   Type: .md\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\guide.md\n",
      "\n",
      "ğŸ“„ readme.md\n",
      "   Size: 43 bytes\n",
      "   Type: .md\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\readme.md\n",
      "\n",
      "ğŸ“„ nested.txt\n",
      "   Size: 41 bytes\n",
      "   Type: .txt\n",
      "   Modified: 2026-01-11 14:00:05\n",
      "   Path: test_fetcher_files\\documents\\subfolder\\nested.txt\n",
      "\n",
      "ğŸ“ˆ Summary:\n",
      "   Total files: 7\n",
      "   Total size: 344 bytes (0.34 KB)\n"
     ]
    }
   ],
   "source": [
    "fetcher = FileFetcher()\n",
    "import datetime\n",
    "\n",
    "# Fetch all files and analyze metadata\n",
    "all_files = fetcher.fetch(dir=\"./test_fetcher_files/documents\")\n",
    "\n",
    "print(\"ğŸ“Š File Metadata Analysis:\\n\")\n",
    "for file in all_files:\n",
    "    # Get file stats\n",
    "    stats = file.stat()\n",
    "    size = stats.st_size\n",
    "    modified_time = datetime.datetime.fromtimestamp(stats.st_mtime)\n",
    "    \n",
    "    print(f\"ğŸ“„ {file.name}\")\n",
    "    print(f\"   Size: {size} bytes\")\n",
    "    print(f\"   Type: {file.suffix}\")\n",
    "    print(f\"   Modified: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"   Path: {file}\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics\n",
    "total_size = sum(f.stat().st_size for f in all_files)\n",
    "print(f\"ğŸ“ˆ Summary:\")\n",
    "print(f\"   Total files: {len(all_files)}\")\n",
    "print(f\"   Total size: {total_size} bytes ({total_size/1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a09dab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: All FileFetcher Combinations\n",
    "\n",
    "Here's a comprehensive reference of all possible usage patterns:\n",
    "\n",
    "### **Standalone Usage**\n",
    "| Mode | Parameters | Returns | Use Case |\n",
    "|------|-----------|---------|----------|\n",
    "| Single File | `path=\"file.txt\"` | `Path` | Process one specific file |\n",
    "| All Files in Dir | `dir=\"./folder\"` | `list[Path]` | Process all files in directory |\n",
    "| Filtered by Single Ext | `dir=\"./folder\", ext=[\".txt\"]` | `list[Path]` | Process only .txt files |\n",
    "| Filtered by Multiple Exts | `dir=\"./folder\", ext=[\".txt\", \".md\"]` | `list[Path]` | Process multiple file types |\n",
    "\n",
    "### **Pipeline Integration**\n",
    "| Mode | Pipeline Syntax | Output | Use Case |\n",
    "|------|----------------|--------|----------|\n",
    "| Single File | `.fetch_from(\"file\", path=\"doc.txt\")` | Single `Document` | Chunk one file |\n",
    "| Directory | `.fetch_from(\"file\", dir=\"./docs\")` | `list[Document]` | Chunk all files |\n",
    "| Filtered | `.fetch_from(\"file\", dir=\"./docs\", ext=[\".txt\"])` | `list[Document]` | Chunk filtered files |\n",
    "\n",
    "### **Error Conditions**\n",
    "- âŒ File not found: Raises `FileNotFoundError`\n",
    "- âŒ Both `path` and `dir` provided: Raises `ValueError`\n",
    "- âŒ Neither `path` nor `dir` provided: Raises `ValueError`\n",
    "\n",
    "### **Best Practices**\n",
    "- âœ… Use extension filtering for large directories to improve performance\n",
    "- âœ… Use absolute paths when possible for clarity\n",
    "- âœ… Handle errors appropriately with try-except blocks\n",
    "- âœ… Process different file types with appropriate chunking strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528508d1",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove the test files created for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f77b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test files cleaned up successfully\n"
     ]
    }
   ],
   "source": [
    "# Clean up test files\n",
    "import shutil\n",
    "\n",
    "test_dir = Path(\"./test_fetcher_files\")\n",
    "if test_dir.exists():\n",
    "    shutil.rmtree(test_dir)\n",
    "    print(\"âœ… Test files cleaned up successfully\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Test directory not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chonkie (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
