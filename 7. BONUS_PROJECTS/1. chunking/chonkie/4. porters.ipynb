{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6f284b",
   "metadata": {},
   "source": [
    "# Chonkie Porters - Complete Guide\n",
    "\n",
    "This notebook demonstrates all Porter types in Chonkie: **JSONPorter** and **DatasetsPorter**.\n",
    "\n",
    "## What are Porters?\n",
    "\n",
    "Porters are exporters that save chunks to different formats for storage, sharing, or integration with other tools:\n",
    "\n",
    "- **JSONPorter**: Exports chunks to JSON files for archiving and interoperability\n",
    "- **DatasetsPorter**: Exports chunks to Hugging Face Dataset format for ML workflows\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ Export chunks to standardized formats\n",
    "- ‚úÖ Save to disk or keep in memory\n",
    "- ‚úÖ Preserve all chunk metadata\n",
    "- ‚úÖ Integration with Hugging Face ecosystem\n",
    "- ‚úÖ Easy data sharing and archiving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12765c",
   "metadata": {},
   "source": [
    "## Visual Overview\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#ff6b6b','primaryTextColor':'#fff','primaryBorderColor':'#c92a2a','lineColor':'#339af0','secondaryColor':'#51cf66','tertiaryColor':'#ffd43b','background':'#f8f9fa','mainBkg':'#e3fafc','secondBkg':'#fff3bf','tertiaryBkg':'#ffe3e3','textColor':'#212529','fontSize':'16px'}}}%%\n",
    "\n",
    "graph TB\n",
    "    Start([üì¶ Porters<br/>Export Chunks]):::startClass\n",
    "    \n",
    "    Start --> Input[\"üîß Input: List of Chunks\"]:::inputClass\n",
    "    \n",
    "    Input --> PorterChoice{Choose Porter Type}:::decisionClass\n",
    "    \n",
    "    PorterChoice -->|JSON Format| JSONPorter[\"üìÑ JSONPorter<br/>Export to JSON\"]:::jsonClass\n",
    "    PorterChoice -->|HF Dataset| DatasetsPorter[\"ü§ó DatasetsPorter<br/>Export to Dataset\"]:::datasetClass\n",
    "    \n",
    "    JSONPorter --> JSONConfig{Configuration}:::decisionClass\n",
    "    DatasetsPorter --> DatasetConfig{Configuration}:::decisionClass\n",
    "    \n",
    "    JSONConfig -->|File Path| JSONPath[\"path='chunks.json'\"]:::paramClass\n",
    "    JSONConfig -->|Pretty Print| JSONPretty[\"indent=2\"]:::paramClass\n",
    "    \n",
    "    DatasetConfig -->|Save Option| SaveOption[\"save_to_disk=True/False\"]:::paramClass\n",
    "    DatasetConfig -->|Directory| DirPath[\"path='chunks'\"]:::paramClass\n",
    "    DatasetConfig -->|Extra Args| KwArgs[\"**kwargs for save_to_disk\"]:::paramClass\n",
    "    \n",
    "    JSONPath --> JSONProcess[\"Export Process\"]:::processClass\n",
    "    JSONPretty --> JSONProcess\n",
    "    \n",
    "    SaveOption --> DatasetProcess[\"Export Process\"]:::processClass\n",
    "    DirPath --> DatasetProcess\n",
    "    KwArgs --> DatasetProcess\n",
    "    \n",
    "    JSONProcess --> JSONOutput[\"üìÅ JSON File<br/>chunks.json\"]:::outputClass\n",
    "    DatasetProcess --> DatasetMemory[\"üíæ Dataset Object<br/>In Memory\"]:::outputClass\n",
    "    DatasetProcess --> DatasetDisk[\"üìÅ Dataset on Disk<br/>Arrow format\"]:::diskClass\n",
    "    \n",
    "    JSONOutput --> UseCases{Use Cases}:::decisionClass\n",
    "    DatasetMemory --> UseCases\n",
    "    DatasetDisk --> UseCases\n",
    "    \n",
    "    UseCases -->|Archive| Archive[\"üì¶ Data Archiving\"]:::useClass\n",
    "    UseCases -->|Share| Share[\"üîó Data Sharing\"]:::useClass\n",
    "    UseCases -->|ML| Training[\"üß† Model Training\"]:::useClass\n",
    "    UseCases -->|Integration| External[\"üîå External Tools\"]:::useClass\n",
    "    \n",
    "    classDef startClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "    classDef inputClass fill:#7950f2,stroke:#5f3dc4,stroke-width:2px,color:#fff\n",
    "    classDef decisionClass fill:#7950f2,stroke:#5f3dc4,stroke-width:2px,color:#fff\n",
    "    classDef jsonClass fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:#fff\n",
    "    classDef datasetClass fill:#20c997,stroke:#087f5b,stroke-width:2px,color:#fff\n",
    "    classDef paramClass fill:#748ffc,stroke:#4c6ef5,stroke-width:2px,color:#fff\n",
    "    classDef processClass fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#333\n",
    "    classDef outputClass fill:#51cf66,stroke:#37b24d,stroke-width:2px,color:#fff\n",
    "    classDef diskClass fill:#69db7c,stroke:#40c057,stroke-width:2px,color:#fff\n",
    "    classDef useClass fill:#ff922b,stroke:#e8590c,stroke-width:2px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e07d14",
   "metadata": {},
   "source": [
    "## Setup - Create Sample Chunks\n",
    "\n",
    "First, we'll create sample chunks to demonstrate each Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8c174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample chunks created:\n",
      "  Total chunks: 4\n",
      "  1. Machine learning is transforming industries worldw... (8 tokens)\n",
      "  2. Deep learning models can recognize complex pattern... (7 tokens)\n",
      "  3. Natural language processing enables human-computer... (8 tokens)\n",
      "  4. Computer vision allows machines to understand visu... (9 tokens)\n"
     ]
    }
   ],
   "source": [
    "from chonkie import Chunk\n",
    "\n",
    "# Create sample chunks with various properties\n",
    "sample_chunks = [\n",
    "    Chunk(\n",
    "        text=\"Machine learning is transforming industries worldwide.\",\n",
    "        start_index=0,\n",
    "        end_index=56,\n",
    "        token_count=8\n",
    "    ),\n",
    "    Chunk(\n",
    "        text=\"Deep learning models can recognize complex patterns.\",\n",
    "        start_index=57,\n",
    "        end_index=109,\n",
    "        token_count=7\n",
    "    ),\n",
    "    Chunk(\n",
    "        text=\"Natural language processing enables human-computer interaction.\",\n",
    "        start_index=110,\n",
    "        end_index=174,\n",
    "        token_count=8\n",
    "    ),\n",
    "    Chunk(\n",
    "        text=\"Computer vision allows machines to understand visual data.\",\n",
    "        start_index=175,\n",
    "        end_index=234,\n",
    "        token_count=9\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Sample chunks created:\")\n",
    "print(f\"  Total chunks: {len(sample_chunks)}\")\n",
    "for i, chunk in enumerate(sample_chunks, 1):\n",
    "    print(f\"  {i}. {chunk.text[:50]}... ({chunk.token_count} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecabc452",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Chonkie with optional dependencies for porters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8dede2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All Porters imported successfully!\n",
      "  üìÑ JSONPorter: <class 'chonkie.porters.json.JSONPorter'>\n",
      "  ü§ó DatasetsPorter: <class 'chonkie.porters.datasets.DatasetsPorter'>\n"
     ]
    }
   ],
   "source": [
    "# Install chonkie with datasets support\n",
    "# !pip install \"chonkie[datasets]\"\n",
    "\n",
    "from chonkie import JSONPorter, DatasetsPorter\n",
    "\n",
    "print(\"‚úÖ All Porters imported successfully!\")\n",
    "print(f\"  üìÑ JSONPorter: {JSONPorter}\")\n",
    "print(f\"  ü§ó DatasetsPorter: {DatasetsPorter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e23dca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: JSONPorter\n",
    "\n",
    "## 1. JSONPorter - Basic Export\n",
    "\n",
    "Export chunks to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442cfdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ JSONPorter - Basic Export:\n",
      "\n",
      "‚úÖ Chunks exported to: ./exported_chunks.json\n",
      "  File exists: True\n",
      "  File size: 1138 bytes\n",
      "  Format: JSON array (lines=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize JSONPorter with lines=False for JSON array format\n",
    "json_porter = JSONPorter(lines=False)\n",
    "\n",
    "# Export chunks to JSON file\n",
    "output_path = \"./exported_chunks.json\"\n",
    "json_porter.export(sample_chunks, file=output_path)\n",
    "\n",
    "print(\"üìÑ JSONPorter - Basic Export:\\n\")\n",
    "print(f\"‚úÖ Chunks exported to: {output_path}\")\n",
    "print(f\"  File exists: {os.path.exists(output_path)}\")\n",
    "print(f\"  File size: {os.path.getsize(output_path)} bytes\")\n",
    "print(f\"  Format: JSON array (lines=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b52be",
   "metadata": {},
   "source": [
    "## 2. JSONPorter - View Exported Content\n",
    "\n",
    "Read and display the exported JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca2e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Exported JSON Content:\n",
      "\n",
      "Number of chunks: 4\n",
      "\n",
      "First chunk structure:\n",
      "{\n",
      "  \"id\": \"chnk_e59cdda4ba38472b894ace02436c026c\",\n",
      "  \"text\": \"Machine learning is transforming industries worldwide.\",\n",
      "  \"start_index\": 0,\n",
      "  \"end_index\": 56,\n",
      "  \"token_count\": 8,\n",
      "  \"context\": null,\n",
      "  \"embedding\": null\n",
      "}\n",
      "\n",
      "üìä Summary:\n",
      "  Chunk 1: Machine learning is transforming industr...\n",
      "    Tokens: 8\n",
      "    Range: [0, 56]\n",
      "\n",
      "  Chunk 2: Deep learning models can recognize compl...\n",
      "    Tokens: 7\n",
      "    Range: [57, 109]\n",
      "\n",
      "  Chunk 3: Natural language processing enables huma...\n",
      "    Tokens: 8\n",
      "    Range: [110, 174]\n",
      "\n",
      "  Chunk 4: Computer vision allows machines to under...\n",
      "    Tokens: 9\n",
      "    Range: [175, 234]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the exported JSON file\n",
    "with open(\"./exported_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    exported_data = json.load(f)\n",
    "\n",
    "print(\"üìÑ Exported JSON Content:\\n\")\n",
    "print(f\"Number of chunks: {len(exported_data)}\\n\")\n",
    "\n",
    "# Display first chunk structure\n",
    "print(\"First chunk structure:\")\n",
    "print(json.dumps(exported_data[0], indent=2))\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä Summary:\")\n",
    "for i, chunk_data in enumerate(exported_data, 1):\n",
    "    print(f\"  Chunk {i}: {chunk_data['text'][:40]}...\")\n",
    "    print(f\"    Tokens: {chunk_data['token_count']}\")\n",
    "    print(f\"    Range: [{chunk_data['start_index']}, {chunk_data['end_index']}]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13d7f9",
   "metadata": {},
   "source": [
    "## 3. JSONPorter - Using as Callable\n",
    "\n",
    "Use the porter as a callable (shorthand syntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09328bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ JSONPorter - Callable Usage:\n",
      "\n",
      "‚úÖ Exported using callable syntax\n",
      "  File: ./callable_export.json\n",
      "  Chunks exported: 2\n",
      "  Format: JSON array (lines=False)\n",
      "\n",
      "üìä Exported chunks:\n",
      "  1. Python is excellent for data science.\n",
      "  2. JavaScript powers modern web applications.\n"
     ]
    }
   ],
   "source": [
    "# Create new chunks for this example\n",
    "new_chunks = [\n",
    "    Chunk(\n",
    "        text=\"Python is excellent for data science.\",\n",
    "        start_index=0,\n",
    "        end_index=38,\n",
    "        token_count=6\n",
    "    ),\n",
    "    Chunk(\n",
    "        text=\"JavaScript powers modern web applications.\",\n",
    "        start_index=39,\n",
    "        end_index=82,\n",
    "        token_count=5\n",
    "    )\n",
    "]\n",
    "\n",
    "# Use porter as callable with lines=False for JSON array format\n",
    "json_porter = JSONPorter(lines=False)\n",
    "json_porter(new_chunks, file=\"./callable_export.json\")\n",
    "\n",
    "print(\"üìÑ JSONPorter - Callable Usage:\\n\")\n",
    "print(f\"‚úÖ Exported using callable syntax\")\n",
    "print(f\"  File: ./callable_export.json\")\n",
    "print(f\"  Chunks exported: {len(new_chunks)}\")\n",
    "print(f\"  Format: JSON array (lines=False)\")\n",
    "\n",
    "# Verify content\n",
    "with open(\"./callable_export.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"\\nüìä Exported chunks:\")\n",
    "    for i, chunk in enumerate(data, 1):\n",
    "        print(f\"  {i}. {chunk['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c181122",
   "metadata": {},
   "source": [
    "## 4. JSONPorter - With Metadata\n",
    "\n",
    "Export chunks with custom metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3a7e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ JSONPorter - Export with Metadata:\n",
      "\n",
      "‚úÖ Exported 3 chunks with metadata\n",
      "\n",
      "Sample chunk with metadata:\n",
      "{\n",
      "  \"id\": \"chnk_e69a4dab00b94d1e9a48f2a16e6b68e6\",\n",
      "  \"text\": \"Artificial intelligence is revolutionizing technology.\",\n",
      "  \"start_index\": 0,\n",
      "  \"end_index\": 55,\n",
      "  \"token_count\": 7,\n",
      "  \"context\": null,\n",
      "  \"embedding\": null,\n",
      "  \"metadata\": {\n",
      "    \"source\": \"document1.txt\",\n",
      "    \"category\": \"AI\",\n",
      "    \"author\": \"John Doe\"\n",
      "  }\n",
      "}\n",
      "\n",
      "üìä All chunks:\n",
      "  1. [AI] Artificial intelligence is revolutionizi...\n",
      "     Source: document1.txt, Author: John Doe\n",
      "  2. [ML] Machine learning algorithms learn from d...\n",
      "     Source: document1.txt, Author: John Doe\n",
      "  3. [Deep Learning] Neural networks mimic human brain struct...\n",
      "     Source: document2.txt, Author: Jane Smith\n"
     ]
    }
   ],
   "source": [
    "# Create chunks with metadata\n",
    "chunk1 = Chunk(\n",
    "    text=\"Artificial intelligence is revolutionizing technology.\",\n",
    "    start_index=0,\n",
    "    end_index=55,\n",
    "    token_count=7\n",
    ")\n",
    "chunk1.metadata = {\"source\": \"document1.txt\", \"category\": \"AI\", \"author\": \"John Doe\"}\n",
    "\n",
    "chunk2 = Chunk(\n",
    "    text=\"Machine learning algorithms learn from data patterns.\",\n",
    "    start_index=56,\n",
    "    end_index=110,\n",
    "    token_count=8\n",
    ")\n",
    "chunk2.metadata = {\"source\": \"document1.txt\", \"category\": \"ML\", \"author\": \"John Doe\"}\n",
    "\n",
    "chunk3 = Chunk(\n",
    "    text=\"Neural networks mimic human brain structure.\",\n",
    "    start_index=111,\n",
    "    end_index=156,\n",
    "    token_count=6\n",
    ")\n",
    "chunk3.metadata = {\"source\": \"document2.txt\", \"category\": \"Deep Learning\", \"author\": \"Jane Smith\"}\n",
    "\n",
    "chunks_with_metadata = [chunk1, chunk2, chunk3]\n",
    "\n",
    "# Export with metadata (use lines=False for JSON array format)\n",
    "json_porter = JSONPorter(lines=False)\n",
    "json_porter.export(chunks_with_metadata, file=\"./chunks_with_metadata.json\")\n",
    "\n",
    "print(\"üìÑ JSONPorter - Export with Metadata:\\n\")\n",
    "print(f\"‚úÖ Exported {len(chunks_with_metadata)} chunks with metadata\\n\")\n",
    "\n",
    "# Display exported content\n",
    "with open(\"./chunks_with_metadata.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(\"Sample chunk with metadata:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "    \n",
    "    print(f\"\\nüìä All chunks:\")\n",
    "    for i, chunk in enumerate(data, 1):\n",
    "        print(f\"  {i}. [{chunk['metadata']['category']}] {chunk['text'][:40]}...\")\n",
    "        print(f\"     Source: {chunk['metadata']['source']}, Author: {chunk['metadata']['author']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423f33c",
   "metadata": {},
   "source": [
    "## 5. JSONPorter - Real Chunking Example\n",
    "\n",
    "Create chunks from actual text and export them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b020a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Created 16 chunks from text\n",
      "\n",
      "‚úÖ Exported to: ./real_chunks.json\n",
      "  Format: JSON array (lines=False)\n",
      "\n",
      "üìä Exported 16 chunks:\n",
      "  1. The field of artificial intell... (30 tokens)\n",
      "  2. igence has grown exponentially... (30 tokens)\n",
      "  3.  over the past decade. \n",
      "Machin... (30 tokens)\n",
      "  4. e learning algorithms now powe... (30 tokens)\n",
      "  5. r everything from recommendati... (30 tokens)\n",
      "  6. on systems to autonomous vehic... (30 tokens)\n",
      "  7. les. \n",
      "Deep learning, a subset ... (30 tokens)\n",
      "  8. of machine learning, uses neur... (30 tokens)\n",
      "  9. al networks with multiple laye... (30 tokens)\n",
      "  10. rs to process complex data. \n",
      "N... (30 tokens)\n",
      "  11. atural language processing ena... (30 tokens)\n",
      "  12. bles computers to understand a... (30 tokens)\n",
      "  13. nd generate human language. \n",
      "C... (30 tokens)\n",
      "  14. omputer vision allows machines... (30 tokens)\n",
      "  15.  to interpret and analyze visu... (30 tokens)\n",
      "  16. al information from the world.... (30 tokens)\n"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"\"\"The field of artificial intelligence has grown exponentially over the past decade. \n",
    "Machine learning algorithms now power everything from recommendation systems to autonomous vehicles. \n",
    "Deep learning, a subset of machine learning, uses neural networks with multiple layers to process complex data. \n",
    "Natural language processing enables computers to understand and generate human language. \n",
    "Computer vision allows machines to interpret and analyze visual information from the world.\"\"\"\n",
    "\n",
    "# Chunk the text\n",
    "chunker = TokenChunker(chunk_size=30)\n",
    "chunks = chunker(sample_text)\n",
    "\n",
    "print(f\"üìù Created {len(chunks)} chunks from text\\n\")\n",
    "\n",
    "# Export to JSON (use lines=False for JSON array format)\n",
    "json_porter = JSONPorter(lines=False)\n",
    "json_porter.export(chunks, file=\"./real_chunks.json\")\n",
    "\n",
    "print(f\"‚úÖ Exported to: ./real_chunks.json\")\n",
    "print(f\"  Format: JSON array (lines=False)\\n\")\n",
    "\n",
    "# Verify export\n",
    "with open(\"./real_chunks.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"üìä Exported {len(data)} chunks:\")\n",
    "    for i, chunk in enumerate(data, 1):\n",
    "        print(f\"  {i}. {chunk['text'][:60]}... ({chunk['token_count']} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94fa79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: DatasetsPorter\n",
    "\n",
    "## 6. DatasetsPorter - Basic Initialization\n",
    "\n",
    "Initialize DatasetsPorter and understand its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b0eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó DatasetsPorter Initialization:\n",
      "\n",
      "  Porter: <chonkie.porters.datasets.DatasetsPorter object at 0x0000021EBBF1F3B0>\n",
      "  Type: <class 'chonkie.porters.datasets.DatasetsPorter'>\n",
      "\n",
      "‚úÖ DatasetsPorter ready!\n",
      "\n",
      "üìã Features:\n",
      "  ‚Ä¢ Export to Hugging Face Dataset format\n",
      "  ‚Ä¢ Keep dataset in memory or save to disk\n",
      "  ‚Ä¢ Compatible with Hugging Face ecosystem\n",
      "  ‚Ä¢ Efficient Arrow format for large datasets\n"
     ]
    }
   ],
   "source": [
    "from chonkie import DatasetsPorter\n",
    "\n",
    "# Initialize DatasetsPorter\n",
    "datasets_porter = DatasetsPorter()\n",
    "\n",
    "print(\"ü§ó DatasetsPorter Initialization:\\n\")\n",
    "print(f\"  Porter: {datasets_porter}\")\n",
    "print(f\"  Type: {type(datasets_porter)}\")\n",
    "print(\"\\n‚úÖ DatasetsPorter ready!\")\n",
    "print(\"\\nüìã Features:\")\n",
    "print(\"  ‚Ä¢ Export to Hugging Face Dataset format\")\n",
    "print(\"  ‚Ä¢ Keep dataset in memory or save to disk\")\n",
    "print(\"  ‚Ä¢ Compatible with Hugging Face ecosystem\")\n",
    "print(\"  ‚Ä¢ Efficient Arrow format for large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b217ca2",
   "metadata": {},
   "source": [
    "## 7. DatasetsPorter - Return Dataset Object\n",
    "\n",
    "Export chunks to an in-memory Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba271c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52961c1c44264bd2bf5a65079e343bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó DatasetsPorter - In-Memory Dataset:\n",
      "\n",
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "\n",
      "Dataset info:\n",
      "Dataset({\n",
      "    features: ['id', 'text', 'start_index', 'end_index', 'token_count', 'context', 'embedding'],\n",
      "    num_rows: 4\n",
      "})\n",
      "\n",
      "üìä Dataset features:\n",
      "  ‚Ä¢ id: Value('string')\n",
      "  ‚Ä¢ text: Value('string')\n",
      "  ‚Ä¢ start_index: Value('int64')\n",
      "  ‚Ä¢ end_index: Value('int64')\n",
      "  ‚Ä¢ token_count: Value('int64')\n",
      "  ‚Ä¢ context: Value('null')\n",
      "  ‚Ä¢ embedding: Value('null')\n",
      "\n",
      "üìã Number of rows: 4\n",
      "\n",
      "üîç First example:\n",
      "  id: chnk_e59cdda4ba38472b894ace02436c026c\n",
      "  text: Machine learning is transforming industries worldw...\n",
      "  start_index: 0\n",
      "  end_index: 56\n",
      "  token_count: 8\n",
      "  context: None\n",
      "  embedding: None\n"
     ]
    }
   ],
   "source": [
    "# Export to Dataset (in memory)\n",
    "datasets_porter = DatasetsPorter()\n",
    "dataset = datasets_porter.export(sample_chunks)\n",
    "\n",
    "print(\"ü§ó DatasetsPorter - In-Memory Dataset:\\n\")\n",
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nüìä Dataset features:\")\n",
    "for feature_name, feature_type in dataset.features.items():\n",
    "    print(f\"  ‚Ä¢ {feature_name}: {feature_type}\")\n",
    "\n",
    "print(f\"\\nüìã Number of rows: {len(dataset)}\")\n",
    "\n",
    "# Access individual examples\n",
    "print(f\"\\nüîç First example:\")\n",
    "first_example = dataset[0]\n",
    "for key, value in first_example.items():\n",
    "    if key == 'text':\n",
    "        print(f\"  {key}: {value[:50]}...\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14516a4",
   "metadata": {},
   "source": [
    "## 8. DatasetsPorter - Save to Disk\n",
    "\n",
    "Save the dataset to disk for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd3db33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4048cab5af8466a83a3e2fa1834d216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó DatasetsPorter - Save to Disk:\n",
      "\n",
      "‚úÖ Dataset saved to: ./my_exported_chunks\n",
      "  Directory exists: True\n",
      "\n",
      "üìÅ Files in directory:\n",
      "  ‚Ä¢ data-00000-of-00001.arrow (1792 bytes)\n",
      "  ‚Ä¢ dataset_info.json (632 bytes)\n",
      "  ‚Ä¢ state.json (259 bytes)\n",
      "\n",
      "üíæ Dataset object still returned:\n",
      "  Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "  Rows: 4\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Clean up if directory exists\n",
    "dataset_dir = \"./my_exported_chunks\"\n",
    "if os.path.exists(dataset_dir):\n",
    "    shutil.rmtree(dataset_dir)\n",
    "\n",
    "# Export and save to disk\n",
    "datasets_porter = DatasetsPorter()\n",
    "dataset = datasets_porter.export(\n",
    "    sample_chunks,\n",
    "    save_to_disk=True,\n",
    "    path=dataset_dir\n",
    ")\n",
    "\n",
    "print(\"ü§ó DatasetsPorter - Save to Disk:\\n\")\n",
    "print(f\"‚úÖ Dataset saved to: {dataset_dir}\")\n",
    "print(f\"  Directory exists: {os.path.exists(dataset_dir)}\")\n",
    "\n",
    "# List files in directory\n",
    "if os.path.exists(dataset_dir):\n",
    "    print(f\"\\nüìÅ Files in directory:\")\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        file_path = os.path.join(dataset_dir, file)\n",
    "        size = os.path.getsize(file_path) if os.path.isfile(file_path) else 0\n",
    "        print(f\"  ‚Ä¢ {file} ({size} bytes)\")\n",
    "\n",
    "print(f\"\\nüíæ Dataset object still returned:\")\n",
    "print(f\"  Type: {type(dataset)}\")\n",
    "print(f\"  Rows: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29043f30",
   "metadata": {},
   "source": [
    "## 9. DatasetsPorter - Load from Disk\n",
    "\n",
    "Load a previously saved dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3717909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó DatasetsPorter - Load from Disk:\n",
      "\n",
      "‚úÖ Dataset loaded from: ./my_exported_chunks\n",
      "\n",
      "Loaded dataset:\n",
      "Dataset({\n",
      "    features: ['id', 'text', 'start_index', 'end_index', 'token_count', 'context', 'embedding'],\n",
      "    num_rows: 4\n",
      "})\n",
      "\n",
      "üìä Dataset contents:\n",
      "  1. Machine learning is transforming industries worldw... (8 tokens)\n",
      "  2. Deep learning models can recognize complex pattern... (7 tokens)\n",
      "  3. Natural language processing enables human-computer... (8 tokens)\n",
      "  4. Computer vision allows machines to understand visu... (9 tokens)\n",
      "\n",
      "‚úÖ Dataset loaded successfully and ready for use!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from disk\n",
    "loaded_dataset = load_from_disk(\"./my_exported_chunks\")\n",
    "\n",
    "print(\"ü§ó DatasetsPorter - Load from Disk:\\n\")\n",
    "print(f\"‚úÖ Dataset loaded from: ./my_exported_chunks\")\n",
    "print(f\"\\nLoaded dataset:\")\n",
    "print(loaded_dataset)\n",
    "\n",
    "print(f\"\\nüìä Dataset contents:\")\n",
    "for i in range(len(loaded_dataset)):\n",
    "    example = loaded_dataset[i]\n",
    "    print(f\"  {i+1}. {example['text'][:50]}... ({example['token_count']} tokens)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully and ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6418c",
   "metadata": {},
   "source": [
    "## 10. DatasetsPorter - Using as Callable\n",
    "\n",
    "Use the porter as a callable for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f34573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1b036f86564a73a4205dde5ffa0683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó DatasetsPorter - Callable Usage:\n",
      "\n",
      "Option 1: In-Memory\n",
      "  Dataset: Dataset({\n",
      "    features: ['id', 'text', 'start_index', 'end_index', 'token_count', 'context', 'embedding'],\n",
      "    num_rows: 3\n",
      "})\n",
      "  Rows: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9284aff31fd24960bcde0daa4fa2d6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 2: Saved to Disk\n",
      "  Path: ./callable_dataset\n",
      "  Exists: True\n",
      "  Dataset object returned: <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# Create test chunks\n",
    "test_chunks = [\n",
    "    Chunk(text=\"First test chunk.\", start_index=0, end_index=17, token_count=3),\n",
    "    Chunk(text=\"Second test chunk.\", start_index=18, end_index=36, token_count=3),\n",
    "    Chunk(text=\"Third test chunk.\", start_index=37, end_index=54, token_count=3)\n",
    "]\n",
    "\n",
    "# Use as callable - in memory\n",
    "datasets_porter = DatasetsPorter()\n",
    "dataset_memory = datasets_porter(test_chunks)\n",
    "\n",
    "print(\"ü§ó DatasetsPorter - Callable Usage:\\n\")\n",
    "print(\"Option 1: In-Memory\")\n",
    "print(f\"  Dataset: {dataset_memory}\")\n",
    "print(f\"  Rows: {len(dataset_memory)}\\n\")\n",
    "\n",
    "# Use as callable - save to disk\n",
    "dataset_dir = \"./callable_dataset\"\n",
    "if os.path.exists(dataset_dir):\n",
    "    shutil.rmtree(dataset_dir)\n",
    "\n",
    "dataset_disk = datasets_porter(test_chunks, save_to_disk=True, path=dataset_dir)\n",
    "\n",
    "print(\"Option 2: Saved to Disk\")\n",
    "print(f\"  Path: {dataset_dir}\")\n",
    "print(f\"  Exists: {os.path.exists(dataset_dir)}\")\n",
    "print(f\"  Dataset object returned: {type(dataset_disk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06affb",
   "metadata": {},
   "source": [
    "## 11. DatasetsPorter - Working with Dataset\n",
    "\n",
    "Perform common operations on the exported dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e008c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c595362b494448aab1282c44ae4dc526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó Working with Datasets:\n",
      "\n",
      "1Ô∏è‚É£ Filter chunks with more than 7 tokens:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c834fa627c541619864da2ad21ec62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original: 4 chunks\n",
      "  Filtered: 3 chunks\n",
      "\n",
      "2Ô∏è‚É£ Add uppercase text field:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c830312eb0447e8ade0713a5d2d40f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  New features: ['id', 'text', 'start_index', 'end_index', 'token_count', 'context', 'embedding', 'text_upper']\n",
      "  Sample: MACHINE LEARNING IS TRANSFORMING INDUSTRIES WORLDW...\n",
      "\n",
      "3Ô∏è‚É£ Select specific columns:\n",
      "  Selected features: ['text', 'token_count']\n",
      "\n",
      "4Ô∏è‚É£ Sort by token count:\n",
      "  1. Tokens: 7 - Deep learning models can recognize compl...\n",
      "  2. Tokens: 8 - Machine learning is transforming industr...\n",
      "  3. Tokens: 8 - Natural language processing enables huma...\n",
      "  4. Tokens: 9 - Computer vision allows machines to under...\n"
     ]
    }
   ],
   "source": [
    "# Export chunks to dataset\n",
    "datasets_porter = DatasetsPorter()\n",
    "dataset = datasets_porter(sample_chunks)\n",
    "\n",
    "print(\"ü§ó Working with Datasets:\\n\")\n",
    "\n",
    "# 1. Filtering\n",
    "print(\"1Ô∏è‚É£ Filter chunks with more than 7 tokens:\")\n",
    "filtered = dataset.filter(lambda x: x['token_count'] > 7)\n",
    "print(f\"  Original: {len(dataset)} chunks\")\n",
    "print(f\"  Filtered: {len(filtered)} chunks\\n\")\n",
    "\n",
    "# 2. Mapping\n",
    "print(\"2Ô∏è‚É£ Add uppercase text field:\")\n",
    "def add_uppercase(example):\n",
    "    example['text_upper'] = example['text'].upper()\n",
    "    return example\n",
    "\n",
    "mapped = dataset.map(add_uppercase)\n",
    "print(f\"  New features: {list(mapped.features.keys())}\")\n",
    "print(f\"  Sample: {mapped[0]['text_upper'][:50]}...\\n\")\n",
    "\n",
    "# 3. Selecting columns\n",
    "print(\"3Ô∏è‚É£ Select specific columns:\")\n",
    "selected = dataset.select_columns(['text', 'token_count'])\n",
    "print(f\"  Selected features: {list(selected.features.keys())}\\n\")\n",
    "\n",
    "# 4. Sorting\n",
    "print(\"4Ô∏è‚É£ Sort by token count:\")\n",
    "sorted_dataset = dataset.sort('token_count')\n",
    "for i in range(len(sorted_dataset)):\n",
    "    print(f\"  {i+1}. Tokens: {sorted_dataset[i]['token_count']} - {sorted_dataset[i]['text'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dffbf96",
   "metadata": {},
   "source": [
    "## 12. DatasetsPorter - Large Scale Export\n",
    "\n",
    "Export a larger number of chunks efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81ce7f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Created 33 chunks from long text\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d91a993cc8401d8d8786d10d201c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó Large Scale Export:\n",
      "\n",
      "‚úÖ Exported 33 chunks\n",
      "  Dataset: Dataset({\n",
      "    features: ['id', 'text', 'start_index', 'end_index', 'token_count', 'context', 'embedding'],\n",
      "    num_rows: 33\n",
      "})\n",
      "\n",
      "üìä Statistics:\n",
      "  Total chunks: 33\n",
      "  Total tokens: 1305\n",
      "  Avg tokens per chunk: 39.5\n",
      "  Min tokens: 25\n",
      "  Max tokens: 40\n"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker\n",
    "\n",
    "# Create a longer text\n",
    "long_text = \"\"\"Artificial intelligence represents one of the most significant technological advances of our time. \n",
    "Machine learning algorithms enable computers to learn from data without explicit programming. \n",
    "Deep learning, using neural networks with multiple layers, has revolutionized fields like computer vision and natural language processing. \n",
    "Convolutional neural networks excel at image recognition tasks. \n",
    "Recurrent neural networks are designed for sequential data like text and time series. \n",
    "Transformer architectures have become the foundation of modern language models. \n",
    "Applications of AI span healthcare, finance, transportation, and entertainment. \n",
    "Ethical considerations around AI include bias, privacy, and accountability. \n",
    "The future of AI promises even more transformative capabilities.\n",
    "\n",
    "Natural language processing enables computers to understand human language. \n",
    "Named entity recognition identifies important entities in text. \n",
    "Sentiment analysis determines emotional tone. \n",
    "Machine translation breaks down language barriers. \n",
    "Question answering systems provide direct answers to queries. \n",
    "Text summarization condenses long documents. \n",
    "Speech recognition converts audio to text. \n",
    "Text generation creates human-like content. \n",
    "These technologies power virtual assistants, search engines, and chatbots.\"\"\"\n",
    "\n",
    "# Chunk the text\n",
    "chunker = TokenChunker(chunk_size=40)\n",
    "large_chunks = chunker(long_text)\n",
    "\n",
    "print(f\"üìù Created {len(large_chunks)} chunks from long text\\n\")\n",
    "\n",
    "# Export to dataset\n",
    "datasets_porter = DatasetsPorter()\n",
    "large_dataset = datasets_porter(\n",
    "    large_chunks,\n",
    "    save_to_disk=True,\n",
    "    path=\"./large_dataset\"\n",
    ")\n",
    "\n",
    "print(\"ü§ó Large Scale Export:\\n\")\n",
    "print(f\"‚úÖ Exported {len(large_dataset)} chunks\")\n",
    "print(f\"  Dataset: {large_dataset}\")\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"  Total chunks: {len(large_dataset)}\")\n",
    "print(f\"  Total tokens: {sum(large_dataset['token_count'])}\")\n",
    "print(f\"  Avg tokens per chunk: {sum(large_dataset['token_count']) / len(large_dataset):.1f}\")\n",
    "print(f\"  Min tokens: {min(large_dataset['token_count'])}\")\n",
    "print(f\"  Max tokens: {max(large_dataset['token_count'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a68bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Comparing Porters\n",
    "\n",
    "## 13. Side-by-Side Comparison\n",
    "\n",
    "Compare JSONPorter and DatasetsPorter outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a97840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Porter Comparison:\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìÑ JSONPorter:\n",
      "  ‚úÖ Exported to: comparison.json\n",
      "  üìè File size: 536 bytes\n",
      "  üìã Format: JSON (human-readable)\n",
      "  üîß Use case: Archiving, sharing, interoperability\n",
      "\n",
      "ü§ó DatasetsPorter:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80afcf003a384190adbe15eafccb6f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Exported to: comparison_dataset/\n",
      "  üìè Total size: 2459 bytes\n",
      "  üìã Format: Arrow (binary, efficient)\n",
      "  üîß Use case: ML training, HF ecosystem, large datasets\n",
      "\n",
      "============================================================\n",
      "\n",
      "üí° Choose JSONPorter for simplicity and portability\n",
      "üí° Choose DatasetsPorter for ML workflows and efficiency\n"
     ]
    }
   ],
   "source": [
    "# Create test chunks\n",
    "comparison_chunks = [\n",
    "    Chunk(text=\"AI is transforming technology.\", start_index=0, end_index=30, token_count=5),\n",
    "    Chunk(text=\"ML algorithms learn from data.\", start_index=31, end_index=61, token_count=5),\n",
    "    Chunk(text=\"DL uses neural networks.\", start_index=62, end_index=86, token_count=4)\n",
    "]\n",
    "\n",
    "print(\"‚öñÔ∏è Porter Comparison:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export with JSONPorter\n",
    "print(\"\\nüìÑ JSONPorter:\")\n",
    "json_porter = JSONPorter()\n",
    "json_porter(comparison_chunks, file=\"./comparison.json\")\n",
    "json_size = os.path.getsize(\"./comparison.json\")\n",
    "print(f\"  ‚úÖ Exported to: comparison.json\")\n",
    "print(f\"  üìè File size: {json_size} bytes\")\n",
    "print(f\"  üìã Format: JSON (human-readable)\")\n",
    "print(f\"  üîß Use case: Archiving, sharing, interoperability\")\n",
    "\n",
    "# Export with DatasetsPorter\n",
    "print(\"\\nü§ó DatasetsPorter:\")\n",
    "datasets_porter = DatasetsPorter()\n",
    "dataset = datasets_porter(comparison_chunks, save_to_disk=True, path=\"./comparison_dataset\")\n",
    "\n",
    "# Calculate directory size\n",
    "dataset_size = sum(\n",
    "    os.path.getsize(os.path.join(\"./comparison_dataset\", f))\n",
    "    for f in os.listdir(\"./comparison_dataset\")\n",
    "    if os.path.isfile(os.path.join(\"./comparison_dataset\", f))\n",
    ")\n",
    "print(f\"  ‚úÖ Exported to: comparison_dataset/\")\n",
    "print(f\"  üìè Total size: {dataset_size} bytes\")\n",
    "print(f\"  üìã Format: Arrow (binary, efficient)\")\n",
    "print(f\"  üîß Use case: ML training, HF ecosystem, large datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Choose JSONPorter for simplicity and portability\")\n",
    "print(\"üí° Choose DatasetsPorter for ML workflows and efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8263a86",
   "metadata": {},
   "source": [
    "## 14. Complete Workflow: Chunk ‚Üí Refine ‚Üí Export\n",
    "\n",
    "Full pipeline from text to exported chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620c6cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Complete Pipeline\n",
      "\n",
      "Input text length: 323 characters\n",
      "\n",
      "üìù Step 1: Chunking...\n",
      "  Created 7 chunks\n",
      "\n",
      "üìä Step 2: Adding overlap context...\n",
      "  Added overlap to 7 chunks\n",
      "\n",
      "üì¶ Step 3: Exporting...\n",
      "  ‚úÖ JSON: Exported to pipeline_output.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb1b07dc3c64cca91ece5fb008b0488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Dataset: Exported to pipeline_dataset/\n",
      "     Features: ['id', 'text', 'start_index', 'end_index', 'token_count', 'context', 'embedding']\n",
      "\n",
      "‚ú® Pipeline complete!\n",
      "\n",
      "üìä Final Output:\n",
      "  Processed chunks: 7\n",
      "  Files created:\n",
      "    ‚Ä¢ pipeline_output.json\n",
      "    ‚Ä¢ pipeline_dataset/\n"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker, OverlapRefinery, JSONPorter, DatasetsPorter\n",
    "\n",
    "def complete_pipeline(text, export_format=\"both\"):\n",
    "    \"\"\"Complete pipeline: Chunk ‚Üí Refine ‚Üí Export\"\"\"\n",
    "    print(\"üöÄ Complete Pipeline\\n\")\n",
    "    print(f\"Input text length: {len(text)} characters\\n\")\n",
    "    \n",
    "    # Step 1: Chunk\n",
    "    print(\"üìù Step 1: Chunking...\")\n",
    "    chunker = TokenChunker(chunk_size=50)\n",
    "    chunks = chunker(text)\n",
    "    print(f\"  Created {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Step 2: Refine with overlap\n",
    "    print(\"üìä Step 2: Adding overlap context...\")\n",
    "    refinery = OverlapRefinery(context_size=0.3, method=\"suffix\", merge=True)\n",
    "    refined_chunks = refinery(chunks)\n",
    "    print(f\"  Added overlap to {len(refined_chunks)} chunks\\n\")\n",
    "    \n",
    "    # Step 3: Export\n",
    "    print(\"üì¶ Step 3: Exporting...\")\n",
    "    \n",
    "    if export_format in [\"json\", \"both\"]:\n",
    "        json_porter = JSONPorter()\n",
    "        json_porter(refined_chunks, file=\"./pipeline_output.json\")\n",
    "        print(f\"  ‚úÖ JSON: Exported to pipeline_output.json\")\n",
    "    \n",
    "    if export_format in [\"dataset\", \"both\"]:\n",
    "        datasets_porter = DatasetsPorter()\n",
    "        dataset = datasets_porter(refined_chunks, save_to_disk=True, path=\"./pipeline_dataset\")\n",
    "        print(f\"  ‚úÖ Dataset: Exported to pipeline_dataset/\")\n",
    "        print(f\"     Features: {list(dataset.features.keys())}\")\n",
    "    \n",
    "    print(\"\\n‚ú® Pipeline complete!\")\n",
    "    return refined_chunks\n",
    "\n",
    "# Run pipeline\n",
    "sample_text = \"\"\"Machine learning is a subset of artificial intelligence that enables systems to learn from data. \n",
    "It includes supervised learning, unsupervised learning, and reinforcement learning. \n",
    "Applications range from image recognition to natural language processing. \n",
    "The field continues to evolve with new algorithms and techniques.\"\"\"\n",
    "\n",
    "result = complete_pipeline(sample_text, export_format=\"both\")\n",
    "\n",
    "print(f\"\\nüìä Final Output:\")\n",
    "print(f\"  Processed chunks: {len(result)}\")\n",
    "print(f\"  Files created:\")\n",
    "print(f\"    ‚Ä¢ pipeline_output.json\")\n",
    "print(f\"    ‚Ä¢ pipeline_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4e7bd",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove exported files created during demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d38de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up exported files...\n",
      "\n",
      "  ‚úÖ Deleted file: ./exported_chunks.json\n",
      "  ‚úÖ Deleted file: ./callable_export.json\n",
      "  ‚úÖ Deleted file: ./chunks_with_metadata.json\n",
      "  ‚úÖ Deleted file: ./real_chunks.json\n",
      "  ‚úÖ Deleted directory: ./my_exported_chunks\n",
      "  ‚úÖ Deleted directory: ./callable_dataset\n",
      "  ‚úÖ Deleted directory: ./large_dataset\n",
      "  ‚úÖ Deleted file: ./comparison.json\n",
      "  ‚úÖ Deleted directory: ./comparison_dataset\n",
      "  ‚úÖ Deleted file: ./pipeline_output.json\n",
      "  ‚úÖ Deleted directory: ./pipeline_dataset\n",
      "\n",
      "‚úÖ Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of files and directories to clean up\n",
    "cleanup_items = [\n",
    "    \"./exported_chunks.json\",\n",
    "    \"./callable_export.json\",\n",
    "    \"./chunks_with_metadata.json\",\n",
    "    \"./real_chunks.json\",\n",
    "    \"./my_exported_chunks\",\n",
    "    \"./callable_dataset\",\n",
    "    \"./large_dataset\",\n",
    "    \"./comparison.json\",\n",
    "    \"./comparison_dataset\",\n",
    "    \"./pipeline_output.json\",\n",
    "    \"./pipeline_dataset\"\n",
    "]\n",
    "\n",
    "print(\"üßπ Cleaning up exported files...\\n\")\n",
    "\n",
    "for item in cleanup_items:\n",
    "    try:\n",
    "        if os.path.isfile(item):\n",
    "            os.remove(item)\n",
    "            print(f\"  ‚úÖ Deleted file: {item}\")\n",
    "        elif os.path.isdir(item):\n",
    "            shutil.rmtree(item)\n",
    "            print(f\"  ‚úÖ Deleted directory: {item}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ‚ÑπÔ∏è Not found: {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error deleting {item}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f63ea9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: All Porter Types and Capabilities\n",
    "\n",
    "### Porter Comparison Table\n",
    "\n",
    "| Porter | Format | Parameters | Return Type | Use Cases |\n",
    "|--------|--------|------------|-------------|-----------|\n",
    "| **JSONPorter** | JSON/JSONL (text) | `file` | None | Archiving, Sharing, Interoperability, Debugging |\n",
    "| **DatasetsPorter** | Arrow (binary) | `save_to_disk`, `path`, `**kwargs` | `Dataset` object | ML Training, HF Ecosystem, Large Datasets |\n",
    "\n",
    "### JSONPorter\n",
    "\n",
    "**Purpose**: Export chunks to JSON or JSON Lines format\n",
    "\n",
    "**Key Features**:\n",
    "- Human-readable format\n",
    "- Works with any JSON-compatible tool\n",
    "- Simple and portable\n",
    "- Good for small to medium datasets\n",
    "- Easy to inspect and debug\n",
    "- Supports both JSON array (`lines=False`) and JSONL (`lines=True`)\n",
    "\n",
    "**Usage**:\n",
    "```python\n",
    "# JSON Lines format (default)\n",
    "porter = JSONPorter(lines=True)\n",
    "porter.export(chunks, file=\"chunks.jsonl\")\n",
    "\n",
    "# JSON array format\n",
    "porter = JSONPorter(lines=False)\n",
    "porter.export(chunks, file=\"chunks.json\")\n",
    "\n",
    "# Or use as callable\n",
    "porter(chunks, file=\"output.json\")\n",
    "```\n",
    "\n",
    "### DatasetsPorter\n",
    "\n",
    "**Purpose**: Export chunks to Hugging Face Dataset format\n",
    "\n",
    "**Parameters**:\n",
    "- **save_to_disk**: `bool` (default: `True`) - Whether to save to disk\n",
    "- **path**: `str` (default: `\"chunks\"`) - Directory path for saving\n",
    "- **\\*\\*kwargs**: Additional arguments for `Dataset.save_to_disk()`\n",
    "\n",
    "**Key Features**:\n",
    "- Efficient Arrow format\n",
    "- Integration with Hugging Face ecosystem\n",
    "- Optimized for large datasets\n",
    "- Rich data manipulation APIs\n",
    "- Memory-efficient operations\n",
    "\n",
    "**Usage**:\n",
    "```python\n",
    "porter = DatasetsPorter()\n",
    "\n",
    "# In-memory\n",
    "dataset = porter.export(chunks)\n",
    "\n",
    "# Save to disk\n",
    "dataset = porter.export(chunks, save_to_disk=True, path=\"my_chunks\")\n",
    "\n",
    "# Load from disk\n",
    "from datasets import load_from_disk\n",
    "loaded = load_from_disk(\"my_chunks\")\n",
    "```\n",
    "\n",
    "### Methods Available\n",
    "\n",
    "**JSONPorter**:\n",
    "- `export(chunks, file=\"chunks.jsonl\")` - Export chunks to JSON file\n",
    "- `__call__(chunks, file=\"chunks.jsonl\")` - Callable interface\n",
    "\n",
    "**DatasetsPorter**:\n",
    "- `export(chunks, save_to_disk=True, path=\"chunks\", **kwargs)` - Export chunks to Dataset\n",
    "- `__call__(chunks, save_to_disk=True, path=\"chunks\", **kwargs)` - Callable interface\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **JSONPorter**:\n",
    "- Use for small to medium datasets (< 10,000 chunks)\n",
    "- Ideal for sharing with non-Python tools\n",
    "- Perfect for debugging and inspection\n",
    "- Good for archiving and version control\n",
    "- Easy to parse in any language\n",
    "- Use `lines=True` for streaming large files\n",
    "- Use `lines=False` for better readability\n",
    "\n",
    "‚úÖ **DatasetsPorter**:\n",
    "- Use for large datasets (> 10,000 chunks)\n",
    "- Essential for ML training pipelines\n",
    "- Better performance for data operations\n",
    "- Native integration with Hugging Face models\n",
    "- Memory-efficient for big data\n",
    "\n",
    "‚úÖ **General Tips**:\n",
    "- Always preserve metadata when exporting\n",
    "- Choose format based on downstream use case\n",
    "- Consider file size and performance needs\n",
    "- Use DatasetsPorter for HF ecosystem integration\n",
    "- Use JSONPorter for maximum portability\n",
    "\n",
    "### Common Workflows\n",
    "\n",
    "**1. Data Archiving**:\n",
    "```python\n",
    "# Chunk text\n",
    "chunks = chunker(text)\n",
    "\n",
    "# Export to JSON for archiving\n",
    "JSONPorter()(chunks, file=\"archive.json\")\n",
    "```\n",
    "\n",
    "**2. ML Training Pipeline**:\n",
    "```python\n",
    "# Process and refine chunks\n",
    "chunks = refinery(chunker(text))\n",
    "\n",
    "# Export to Dataset for training\n",
    "dataset = DatasetsPorter()(chunks, save_to_disk=True, path=\"train_data\")\n",
    "```\n",
    "\n",
    "**3. Data Sharing**:\n",
    "```python\n",
    "# Export to both formats\n",
    "JSONPorter()(chunks, file=\"chunks.json\")  # For general use\n",
    "DatasetsPorter()(chunks, save_to_disk=True, path=\"chunks_hf\")  # For HF users\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chonkie (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
