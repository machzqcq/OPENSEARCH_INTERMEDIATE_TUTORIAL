{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2baf00",
   "metadata": {},
   "source": [
    "# Chonkie Pipelines - Complete Guide\n",
    "\n",
    "This notebook demonstrates the **Pipeline API** in Chonkie - a fluent interface for building text processing workflows.\n",
    "\n",
    "## What are Pipelines?\n",
    "\n",
    "Pipelines provide a chainable API following the **CHOMP architecture** (CHOnkie's Multi-step Pipeline):\n",
    "\n",
    "**Fetcher ‚Üí Chef ‚Üí Chunker ‚Üí Refinery ‚Üí Porter/Handshake**\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ Fluent, chainable API for building workflows\n",
    "- ‚úÖ Automatic component reordering (follows CHOMP)\n",
    "- ‚úÖ Single file or directory processing\n",
    "- ‚úÖ Direct text input (no fetcher needed)\n",
    "- ‚úÖ Multiple refineries can be chained\n",
    "- ‚úÖ Export and storage options\n",
    "- ‚úÖ Recipe-based pipelines from Chonkie Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a28977",
   "metadata": {},
   "source": [
    "## Visual Overview\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#ff6b6b','primaryTextColor':'#fff','primaryBorderColor':'#c92a2a','lineColor':'#339af0','secondaryColor':'#51cf66','tertiaryColor':'#ffd43b','background':'#f8f9fa','mainBkg':'#e3fafc','secondBkg':'#fff3bf','tertiaryBkg':'#ffe3e3','textColor':'#212529','fontSize':'16px'}}}%%\n",
    "\n",
    "graph LR\n",
    "    Start([üöÄ Pipeline<br/>CHOMP Architecture]):::startClass\n",
    "    \n",
    "    Start --> Fetch[\"1Ô∏è‚É£ Fetcher<br/>(Optional)\"]:::fetchClass\n",
    "    Start --> TextInput[\"Direct Text Input\"]:::inputClass\n",
    "    \n",
    "    Fetch --> Chef[\"2Ô∏è‚É£ Chef<br/>(Optional)\"]:::chefClass\n",
    "    TextInput --> Chef\n",
    "    \n",
    "    Chef --> Chunker[\"3Ô∏è‚É£ Chunker<br/>(Required)\"]:::chunkerClass\n",
    "    TextInput --> Chunker\n",
    "    \n",
    "    Chunker --> Refine[\"4Ô∏è‚É£ Refinery<br/>(Optional, Chainable)\"]:::refineClass\n",
    "    \n",
    "    Refine --> Output{Output Options}:::decisionClass\n",
    "    \n",
    "    Output -->|Export| Porter[\"5Ô∏è‚É£ Porter<br/>JSON/Datasets\"]:::porterClass\n",
    "    Output -->|Store| Handshake[\"5Ô∏è‚É£ Handshake<br/>Vector DB\"]:::handshakeClass\n",
    "    Output -->|Return| Document[\"Document/List[Document]\"]:::docClass\n",
    "    \n",
    "    Porter --> Final([‚ú® Complete]):::finalClass\n",
    "    Handshake --> Final\n",
    "    Document --> Final\n",
    "    \n",
    "    classDef startClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "    classDef fetchClass fill:#7950f2,stroke:#5f3dc4,stroke-width:2px,color:#fff\n",
    "    classDef inputClass fill:#748ffc,stroke:#4c6ef5,stroke-width:2px,color:#fff\n",
    "    classDef chefClass fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:#fff\n",
    "    classDef chunkerClass fill:#fa5252,stroke:#e03131,stroke-width:3px,color:#fff\n",
    "    classDef refineClass fill:#20c997,stroke:#087f5b,stroke-width:2px,color:#fff\n",
    "    classDef decisionClass fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#333\n",
    "    classDef porterClass fill:#ff922b,stroke:#e8590c,stroke-width:2px,color:#fff\n",
    "    classDef handshakeClass fill:#cc5de8,stroke:#9c36b5,stroke-width:2px,color:#fff\n",
    "    classDef docClass fill:#51cf66,stroke:#37b24d,stroke-width:2px,color:#fff\n",
    "    classDef finalClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca066d0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import Chonkie and create sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eff81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Demo directory created: C:\\Users\\PMACHA~1\\AppData\\Local\\Temp\\tmpjlgca46s\n",
      "  Created: doc1.txt\n",
      "  Created: doc2.txt\n",
      "  Created: doc3.md\n",
      "\n",
      "‚úÖ Setup complete! Created 3 sample files\n"
     ]
    }
   ],
   "source": [
    "from chonkie import Pipeline\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create temporary directory for demo files\n",
    "demo_dir = tempfile.mkdtemp()\n",
    "print(f\"‚úÖ Demo directory created: {demo_dir}\")\n",
    "\n",
    "# Create sample text files\n",
    "sample_texts = {\n",
    "    \"doc1.txt\": \"Machine learning is transforming industries worldwide. Deep learning models can recognize complex patterns. Natural language processing enables human-computer interaction.\",\n",
    "    \"doc2.txt\": \"Artificial intelligence represents one of the most significant technological advances. Neural networks mimic human brain structure. Computer vision allows machines to understand visual data.\",\n",
    "    \"doc3.md\": \"\"\"# AI Overview\n",
    "\n",
    "## Machine Learning\n",
    "Machine learning algorithms learn from data without explicit programming.\n",
    "\n",
    "## Applications\n",
    "- Image recognition\n",
    "- Speech processing\n",
    "- Text analysis\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filename, content in sample_texts.items():\n",
    "    filepath = os.path.join(demo_dir, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"  Created: {filename}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup complete! Created {len(sample_texts)} sample files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68058ebb",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Chonkie (if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9f82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chonkie imported successfully!\n",
      "  Pipeline: <class 'chonkie.pipeline.pipeline.Pipeline'>\n",
      "  Document: <class 'chonkie.types.document.Document'>\n"
     ]
    }
   ],
   "source": [
    "# Install chonkie\n",
    "# !pip install chonkie\n",
    "\n",
    "from chonkie import Pipeline, Document\n",
    "\n",
    "print(\"‚úÖ Chonkie imported successfully!\")\n",
    "print(f\"  Pipeline: {Pipeline}\")\n",
    "print(f\"  Document: {Document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0f54d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic Pipeline Usage\n",
    "\n",
    "## 1. Single File Processing\n",
    "\n",
    "Process a single file with the simplest pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b6a2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Single File Processing:\n",
      "\n",
      "Document type: <class 'chonkie.types.document.Document'>\n",
      "Number of chunks: 6\n",
      "\n",
      "üìä Chunks:\n",
      "  1. Machine learning is transforming industries... (44 tokens)\n",
      "  2.  worldwide. ... (12 tokens)\n",
      "  3. Deep learning models can recognize complex... (43 tokens)\n",
      "  4.  patterns. ... (11 tokens)\n",
      "  5. Natural language processing enables human-... (42 tokens)\n",
      "  6. computer interaction.... (21 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Single file processing\n",
    "doc1_path = os.path.join(demo_dir, \"doc1.txt\")\n",
    "\n",
    "doc = (Pipeline()\n",
    "    .fetch_from(\"file\", path=doc1_path)\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "\n",
    "print(\"üìù Single File Processing:\\n\")\n",
    "print(f\"Document type: {type(doc)}\")\n",
    "print(f\"Number of chunks: {len(doc.chunks)}\")\n",
    "print(f\"\\nüìä Chunks:\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"  {i}. {chunk.text[:60]}... ({chunk.token_count} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213c84d",
   "metadata": {},
   "source": [
    "## 2. Directory Processing\n",
    "\n",
    "Process multiple files from a directory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ecf9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directory Processing:\n",
      "\n",
      "Result type: <class 'list'>\n",
      "Number of documents: 2\n",
      "\n",
      "üìÑ Document 1:\n",
      "  Chunks: 6\n",
      "  First chunk: Machine learning is transforming industries...\n",
      "\n",
      "üìÑ Document 2:\n",
      "  Chunks: 5\n",
      "  First chunk: Artificial intelligence represents one of the...\n"
     ]
    }
   ],
   "source": [
    "# Directory processing with extension filter\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=demo_dir, ext=[\".txt\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "\n",
    "print(\"üìÅ Directory Processing:\\n\")\n",
    "print(f\"Result type: {type(docs)}\")\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nüìÑ Document {i}:\")\n",
    "    print(f\"  Chunks: {len(doc.chunks)}\")\n",
    "    print(f\"  First chunk: {doc.chunks[0].text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11106680",
   "metadata": {},
   "source": [
    "## 3. Direct Text Input\n",
    "\n",
    "Skip the fetcher and provide text directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "471f0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Direct Text Input (Single):\n",
      "\n",
      "Document type: <class 'chonkie.types.document.Document'>\n",
      "Number of chunks: 1\n",
      "\n",
      "üìä Chunks:\n",
      "  1. Machine learning enables computers to learn from data. Deep learning uses neural networks. AI is transforming industries.\n",
      "\n",
      "============================================================\n",
      "\n",
      "üí¨ Direct Text Input (Multiple):\n",
      "\n",
      "Result type: <class 'list'>\n",
      "Number of documents: 3\n",
      "\n",
      "üìÑ Document 1: 3 chunks\n",
      "\n",
      "üìÑ Document 2: 2 chunks\n",
      "\n",
      "üìÑ Document 3: 2 chunks\n"
     ]
    }
   ],
   "source": [
    "# Single text input (no fetcher needed)\n",
    "doc = (Pipeline()\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", threshold=0.8)\n",
    "    .run(texts=\"Machine learning enables computers to learn from data. Deep learning uses neural networks. AI is transforming industries.\"))\n",
    "\n",
    "print(\"üí¨ Direct Text Input (Single):\\n\")\n",
    "print(f\"Document type: {type(doc)}\")\n",
    "print(f\"Number of chunks: {len(doc.chunks)}\")\n",
    "print(f\"\\nüìä Chunks:\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"  {i}. {chunk.text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Multiple texts input\n",
    "texts = [\n",
    "    \"Python is excellent for data science and machine learning.\",\n",
    "    \"JavaScript powers modern web applications and servers.\",\n",
    "    \"Rust provides memory safety without garbage collection.\"\n",
    "]\n",
    "\n",
    "docs = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=30)\n",
    "    .run(texts=texts))\n",
    "\n",
    "print(\"üí¨ Direct Text Input (Multiple):\\n\")\n",
    "print(f\"Result type: {type(docs)}\")\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nüìÑ Document {i}: {len(doc.chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79189088",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Pipeline Methods\n",
    "\n",
    "## 4. fetch_from() - Data Sources\n",
    "\n",
    "Fetch data from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb8281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Fetcher Examples:\n",
      "\n",
      "1Ô∏è‚É£ Single File:\n",
      "   .fetch_from('file', path='doc1.txt')\n",
      "\n",
      "2Ô∏è‚É£ Directory with Filter:\n",
      "   .fetch_from('file', dir='C:\\Users\\PMACHA~1\\AppData\\Local\\Temp\\tmpjlgca46s', ext=['.txt', '.md'])\n",
      "\n",
      "3Ô∏è‚É£ All Files in Directory:\n",
      "   .fetch_from('file', dir='C:\\Users\\PMACHA~1\\AppData\\Local\\Temp\\tmpjlgca46s')\n",
      "\n",
      "‚úÖ Fetcher configurations created (not executed)\n",
      "   Use .run() to execute the pipeline\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• Fetcher Examples:\\n\")\n",
    "\n",
    "# Example 1: Single file\n",
    "print(\"1Ô∏è‚É£ Single File:\")\n",
    "pipeline1 = Pipeline().fetch_from(\"file\", path=os.path.join(demo_dir, \"doc1.txt\"))\n",
    "print(f\"   .fetch_from('file', path='doc1.txt')\")\n",
    "\n",
    "# Example 2: Directory with extension filter\n",
    "print(\"\\n2Ô∏è‚É£ Directory with Filter:\")\n",
    "pipeline2 = Pipeline().fetch_from(\"file\", dir=demo_dir, ext=[\".txt\", \".md\"])\n",
    "print(f\"   .fetch_from('file', dir='{demo_dir}', ext=['.txt', '.md'])\")\n",
    "\n",
    "# Example 3: All files in directory\n",
    "print(\"\\n3Ô∏è‚É£ All Files in Directory:\")\n",
    "pipeline3 = Pipeline().fetch_from(\"file\", dir=demo_dir)\n",
    "print(f\"   .fetch_from('file', dir='{demo_dir}')\")\n",
    "\n",
    "print(\"\\n‚úÖ Fetcher configurations created (not executed)\")\n",
    "print(\"   Use .run() to execute the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd8720",
   "metadata": {},
   "source": [
    "## 5. process_with() - Chefs\n",
    "\n",
    "Process data with different chef types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f33c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë®‚Äçüç≥ Chef Examples:\n",
      "\n",
      "1Ô∏è‚É£ Text Chef:\n",
      "   Processed 6 chunks from text file\n",
      "\n",
      "2Ô∏è‚É£ Markdown Chef:\n",
      "   Processed markdown with 5 chunks\n",
      "   Found 0 tables\n",
      "   Found 0 code blocks\n",
      "\n",
      "3Ô∏è‚É£ No Chef (Direct):\n",
      "   Chunked directly: 2 chunks\n",
      "\n",
      "‚úÖ Different chef types demonstrated\n"
     ]
    }
   ],
   "source": [
    "print(\"üë®‚Äçüç≥ Chef Examples:\\n\")\n",
    "\n",
    "# Text Chef\n",
    "print(\"1Ô∏è‚É£ Text Chef:\")\n",
    "doc = (Pipeline()\n",
    "    .fetch_from(\"file\", path=os.path.join(demo_dir, \"doc1.txt\"))\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "print(f\"   Processed {len(doc.chunks)} chunks from text file\")\n",
    "\n",
    "# Markdown Chef\n",
    "print(\"\\n2Ô∏è‚É£ Markdown Chef:\")\n",
    "doc = (Pipeline()\n",
    "    .fetch_from(\"file\", path=os.path.join(demo_dir, \"doc3.md\"))\n",
    "    .process_with(\"markdown\")\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "print(f\"   Processed markdown with {len(doc.chunks)} chunks\")\n",
    "if hasattr(doc, 'tables'):\n",
    "    print(f\"   Found {len(doc.tables)} tables\")\n",
    "if hasattr(doc, 'code'):\n",
    "    print(f\"   Found {len(doc.code)} code blocks\")\n",
    "\n",
    "# Without Chef (direct chunking)\n",
    "print(\"\\n3Ô∏è‚É£ No Chef (Direct):\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=30)\n",
    "    .run(texts=\"Direct text chunking without preprocessing\"))\n",
    "print(f\"   Chunked directly: {len(doc.chunks)} chunks\")\n",
    "\n",
    "print(\"\\n‚úÖ Different chef types demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd584e02",
   "metadata": {},
   "source": [
    "## 6. chunk_with() - Chunkers (Required)\n",
    "\n",
    "Use different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b43192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Chunker Examples:\n",
      "\n",
      "1Ô∏è‚É£ Recursive Chunker:\n",
      "   Created 8 chunks\n",
      "   1. Machine learning enables computers to learn from... (49 tokens)\n",
      "   2.  data. ... (7 tokens)\n",
      "   3. Deep learning uses neural networks with multiple... (49 tokens)\n",
      "   4.  layers. ... (9 tokens)\n",
      "   5. Natural language processing helps computers... (44 tokens)\n",
      "   6.  understand human language. ... (28 tokens)\n",
      "   7. Computer vision allows machines to interpret... (45 tokens)\n",
      "   8.  visual information.... (20 tokens)\n",
      "\n",
      "2Ô∏è‚É£ Token Chunker:\n",
      "   Created 7 chunks\n",
      "   1. Machine learning enables computers to le... (40 tokens)\n",
      "   2. arn from data. Deep learning uses neural... (40 tokens)\n",
      "   3.  networks with multiple layers. Natural ... (40 tokens)\n",
      "   4. language processing helps computers unde... (40 tokens)\n",
      "   5. rstand human language. Computer vision a... (40 tokens)\n",
      "   6. llows machines to interpret visual infor... (40 tokens)\n",
      "   7. mation.... (7 tokens)\n",
      "\n",
      "3Ô∏è‚É£ Semantic Chunker:\n",
      "   Created 1 semantic chunks\n",
      "   1. Machine learning enables computers to learn from d...\n",
      "\n",
      "‚úÖ Different chunking strategies demonstrated\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Machine learning enables computers to learn from data. Deep learning uses neural networks with multiple layers. Natural language processing helps computers understand human language. Computer vision allows machines to interpret visual information.\"\n",
    "\n",
    "print(\"‚úÇÔ∏è Chunker Examples:\\n\")\n",
    "\n",
    "# Recursive Chunker\n",
    "print(\"1Ô∏è‚É£ Recursive Chunker:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run(texts=sample_text))\n",
    "print(f\"   Created {len(doc.chunks)} chunks\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"   {i}. {chunk.text[:50]}... ({chunk.token_count} tokens)\")\n",
    "\n",
    "# Token Chunker\n",
    "print(\"\\n2Ô∏è‚É£ Token Chunker:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"token\", chunk_size=40)\n",
    "    .run(texts=sample_text))\n",
    "print(f\"   Created {len(doc.chunks)} chunks\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"   {i}. {chunk.text[:50]}... ({chunk.token_count} tokens)\")\n",
    "\n",
    "# Semantic Chunker\n",
    "print(\"\\n3Ô∏è‚É£ Semantic Chunker:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"semantic\", threshold=0.7, chunk_size=100)\n",
    "    .run(texts=sample_text))\n",
    "print(f\"   Created {len(doc.chunks)} semantic chunks\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"   {i}. {chunk.text[:50]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Different chunking strategies demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7007dfc",
   "metadata": {},
   "source": [
    "## 7. refine_with() - Refineries (Optional, Chainable)\n",
    "\n",
    "Enhance chunks with refineries. Multiple refineries can be chained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2e432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Refinery Examples:\n",
      "\n",
      "1Ô∏è‚É£ Overlap Refinery:\n",
      "   Created 7 chunks with overlap context\n",
      "   First chunk context: olutioniz...\n",
      "\n",
      "2Ô∏è‚É£ Embeddings Refinery:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c393e3da73b4e2abb378aa1ea1b39f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/30.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git-projects\\personal\\github.com\\OPENSEARCH_INTERMEDIATE_TUTORIAL\\7. BONUS_PROJECTS\\1. chunking\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pmacharla\\.cache\\huggingface\\hub\\models--minishlab--potion-base-8M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d634447b440b493480bd1f504f2c009c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8129324be54bf58843e2e6a1752abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f99ce1eb505446cb7866f19a313ea8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 7 chunks with embeddings\n",
      "   Embedding shape: (256,)\n",
      "\n",
      "3Ô∏è‚É£ Chained Refineries (Overlap + Embeddings):\n",
      "   Created 7 chunks with both refineries\n",
      "   Has context: True\n",
      "   Has embedding: True\n",
      "\n",
      "‚úÖ Refineries can be chained for complex processing\n"
     ]
    }
   ],
   "source": [
    "from chonkie import OverlapRefinery, EmbeddingsRefinery\n",
    "\n",
    "sample_text = \"Artificial intelligence is revolutionizing technology. Machine learning enables computers to learn from data. Deep learning uses neural networks. Natural language processing helps understand text.\"\n",
    "\n",
    "print(\"üîß Refinery Examples:\\n\")\n",
    "\n",
    "# Overlap Refinery\n",
    "print(\"1Ô∏è‚É£ Overlap Refinery:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"token\", chunk_size=30)\n",
    "    .refine_with(\"overlap\", context_size=0.3, method=\"suffix\")\n",
    "    .run(texts=sample_text))\n",
    "print(f\"   Created {len(doc.chunks)} chunks with overlap context\")\n",
    "if doc.chunks[0].context:\n",
    "    print(f\"   First chunk context: {doc.chunks[0].context[:50]}...\")\n",
    "\n",
    "# Embeddings Refinery\n",
    "print(\"\\n2Ô∏è‚É£ Embeddings Refinery:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"token\", chunk_size=30)\n",
    "    .refine_with(\"embeddings\", embedding_model=\"minishlab/potion-base-8M\")\n",
    "    .run(texts=sample_text))\n",
    "print(f\"   Created {len(doc.chunks)} chunks with embeddings\")\n",
    "if doc.chunks[0].embedding is not None:\n",
    "    print(f\"   Embedding shape: {doc.chunks[0].embedding.shape}\")\n",
    "\n",
    "# Chain Multiple Refineries\n",
    "print(\"\\n3Ô∏è‚É£ Chained Refineries (Overlap + Embeddings):\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"token\", chunk_size=30)\n",
    "    .refine_with(\"overlap\", context_size=0.2, method=\"suffix\")\n",
    "    .refine_with(\"embeddings\", embedding_model=\"minishlab/potion-base-8M\")\n",
    "    .run(texts=sample_text))\n",
    "print(f\"   Created {len(doc.chunks)} chunks with both refineries\")\n",
    "print(f\"   Has context: {doc.chunks[0].context is not None}\")\n",
    "print(f\"   Has embedding: {doc.chunks[0].embedding is not None}\")\n",
    "\n",
    "print(\"\\n‚úÖ Refineries can be chained for complex processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16798e",
   "metadata": {},
   "source": [
    "## 8. export_with() - Porters (Optional)\n",
    "\n",
    "Export chunks to different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c653c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Export Examples:\n",
      "\n",
      "1Ô∏è‚É£ JSON Export:\n",
      "   ‚úÖ Exported 6 chunks to JSON Lines\n",
      "\n",
      "2Ô∏è‚É£ Datasets Export:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6117364acc9468c8d646d7ad3a30ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Exported dataset to: ./pipeline_dataset\n",
      "   Directory exists: True\n",
      "\n",
      "‚úÖ Chunks exported successfully\n",
      "   Note: Pipeline still returns Document object\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample_text = \"Machine learning is a subset of AI. Deep learning uses neural networks. NLP enables language understanding.\"\n",
    "\n",
    "print(\"üì¶ Export Examples:\\n\")\n",
    "\n",
    "# Export to JSON\n",
    "print(\"1Ô∏è‚É£ JSON Export:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"token\", chunk_size=20)\n",
    "    .export_with(\"json\", file=\"./pipeline_chunks.json\")\n",
    "    .run(texts=sample_text))\n",
    "\n",
    "if os.path.exists(\"./pipeline_chunks.json\"):\n",
    "    with open(\"./pipeline_chunks.json\", \"r\") as f:\n",
    "        # Try to load as JSON Lines (default)\n",
    "        f.seek(0)\n",
    "        content = f.read().strip()\n",
    "        if content.startswith('['):\n",
    "            # JSON array\n",
    "            f.seek(0)\n",
    "            data = json.load(f)\n",
    "            print(f\"   ‚úÖ Exported {len(data)} chunks to JSON array\")\n",
    "        else:\n",
    "            # JSON Lines\n",
    "            f.seek(0)\n",
    "            lines = f.readlines()\n",
    "            print(f\"   ‚úÖ Exported {len(lines)} chunks to JSON Lines\")\n",
    "\n",
    "# Export to Datasets\n",
    "print(\"\\n2Ô∏è‚É£ Datasets Export:\")\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"token\", chunk_size=20)\n",
    "    .export_with(\"datasets\", save_to_disk=True, path=\"./pipeline_dataset\")\n",
    "    .run(texts=sample_text))\n",
    "\n",
    "if os.path.exists(\"./pipeline_dataset\"):\n",
    "    print(f\"   ‚úÖ Exported dataset to: ./pipeline_dataset\")\n",
    "    print(f\"   Directory exists: {os.path.exists('./pipeline_dataset')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Chunks exported successfully\")\n",
    "print(\"   Note: Pipeline still returns Document object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f0229",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Advanced Examples\n",
    "\n",
    "## 9. Complete RAG Pipeline\n",
    "\n",
    "Build a complete RAG ingestion pipeline with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b09a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Complete RAG Pipeline:\n",
      "\n",
      "‚úÖ Processed 3 documents\n",
      "\n",
      "üìä Results:\n",
      "  Total chunks: 3\n",
      "\n",
      "üìÑ First Document:\n",
      "  Chunks: 1\n",
      "  First chunk: Deep learning uses artificial neural networks with multiple ...\n",
      "  Has context: False\n",
      "  Has embedding: True\n",
      "\n",
      "‚úÖ Exported to: rag_chunks.json\n",
      "\n",
      "‚ú® Complete RAG pipeline executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create sample knowledge base files\n",
    "kb_dir = tempfile.mkdtemp()\n",
    "kb_files = {\n",
    "    \"ml_basics.txt\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. Supervised learning uses labeled data for training. Unsupervised learning finds patterns in unlabeled data.\",\n",
    "    \"dl_intro.txt\": \"Deep learning uses artificial neural networks with multiple layers to process complex data. Convolutional neural networks excel at image recognition. Recurrent neural networks handle sequential data like text and time series.\",\n",
    "    \"nlp_guide.txt\": \"Natural language processing enables computers to understand human language. Named entity recognition identifies important entities in text. Sentiment analysis determines emotional tone. Machine translation breaks down language barriers.\"\n",
    "}\n",
    "\n",
    "for filename, content in kb_files.items():\n",
    "    with open(os.path.join(kb_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"üîÆ Complete RAG Pipeline:\\n\")\n",
    "\n",
    "# Full pipeline with all components\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=kb_dir, ext=[\".txt\"])\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", threshold=0.8, chunk_size=100)\n",
    "    .refine_with(\"overlap\", context_size=0.2, method=\"suffix\")\n",
    "    .refine_with(\"embeddings\", embedding_model=\"minishlab/potion-base-8M\")\n",
    "    .export_with(\"json\", file=\"./rag_chunks.json\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Processed {len(docs)} documents\")\n",
    "print(f\"\\nüìä Results:\")\n",
    "total_chunks = sum(len(doc.chunks) for doc in docs)\n",
    "print(f\"  Total chunks: {total_chunks}\")\n",
    "\n",
    "# Check first document\n",
    "if docs:\n",
    "    first_doc = docs[0]\n",
    "    print(f\"\\nüìÑ First Document:\")\n",
    "    print(f\"  Chunks: {len(first_doc.chunks)}\")\n",
    "    if first_doc.chunks:\n",
    "        chunk = first_doc.chunks[0]\n",
    "        print(f\"  First chunk: {chunk.text[:60]}...\")\n",
    "        print(f\"  Has context: {chunk.context is not None}\")\n",
    "        print(f\"  Has embedding: {chunk.embedding is not None}\")\n",
    "\n",
    "# Verify export\n",
    "if os.path.exists(\"./rag_chunks.json\"):\n",
    "    print(f\"\\n‚úÖ Exported to: rag_chunks.json\")\n",
    "\n",
    "print(\"\\n‚ú® Complete RAG pipeline executed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b8f0b",
   "metadata": {},
   "source": [
    "## 10. Semantic Search Pipeline\n",
    "\n",
    "Process documents with embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93f1f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Semantic Search Pipeline:\n",
      "\n",
      "‚úÖ Processed document with semantic chunking\n",
      "  Total chunks: 1\n",
      "\n",
      "üìä Chunk Analysis:\n",
      "\n",
      "  Chunk 1:\n",
      "    Text: Transformer architectures have revolutionized natural langua...\n",
      "    Tokens: 59\n",
      "    Embedding shape: (256,)\n",
      "\n",
      "‚ú® All chunks ready for semantic search!\n"
     ]
    }
   ],
   "source": [
    "research_text = \"\"\"Transformer architectures have revolutionized natural language processing. The attention mechanism allows models to focus on relevant parts of the input. BERT uses bidirectional training to understand context. GPT models use autoregressive generation for text creation. These models achieve state-of-the-art results across various NLP tasks.\"\"\"\n",
    "\n",
    "print(\"üîç Semantic Search Pipeline:\\n\")\n",
    "\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"semantic\", \n",
    "                threshold=0.8, \n",
    "                chunk_size=100, \n",
    "                similarity_window=3)\n",
    "    .refine_with(\"overlap\", context_size=0.2)\n",
    "    .refine_with(\"embeddings\", embedding_model=\"minishlab/potion-base-8M\")\n",
    "    .run(texts=research_text))\n",
    "\n",
    "print(f\"‚úÖ Processed document with semantic chunking\")\n",
    "print(f\"  Total chunks: {len(doc.chunks)}\")\n",
    "\n",
    "print(f\"\\nüìä Chunk Analysis:\")\n",
    "for i, chunk in enumerate(doc.chunks, 1):\n",
    "    print(f\"\\n  Chunk {i}:\")\n",
    "    print(f\"    Text: {chunk.text[:60]}...\")\n",
    "    print(f\"    Tokens: {chunk.token_count}\")\n",
    "    if chunk.context:\n",
    "        print(f\"    Context: {chunk.context[:40]}...\")\n",
    "    if chunk.embedding is not None:\n",
    "        print(f\"    Embedding shape: {chunk.embedding.shape}\")\n",
    "\n",
    "print(\"\\n‚ú® All chunks ready for semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab733217",
   "metadata": {},
   "source": [
    "## 11. Code Documentation Pipeline\n",
    "\n",
    "Process code files with specialized chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038d3f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Code Documentation Pipeline:\n",
      "\n",
      "‚úÖ Processed 2 Python files\n",
      "\n",
      "üìÑ File 1:\n",
      "  Chunks: 3\n",
      "  First chunk preview:\n",
      "    class User:\n",
      "    def __init__(self, name, email):\n",
      "        self.name = name\n",
      "      ...\n",
      "\n",
      "üìÑ File 2:\n",
      "  Chunks: 3\n",
      "  First chunk preview:\n",
      "    def calculate_sum(a, b):\n",
      "    '''Calculate sum of two numbers'''\n",
      "    return a + b...\n",
      "\n",
      "‚úÖ Code chunks exported to: code_chunks.json\n",
      "\n",
      "‚ú® Code documentation pipeline complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git-projects\\personal\\github.com\\OPENSEARCH_INTERMEDIATE_TUTORIAL\\7. BONUS_PROJECTS\\1. chunking\\.venv\\Lib\\site-packages\\chonkie\\chunker\\code.py:76: UserWarning: The language is set to `auto`. This would adversely affect the performance of the chunker. Consider setting the `language` parameter to a specific language to improve performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create sample Python files\n",
    "code_dir = tempfile.mkdtemp()\n",
    "code_files = {\n",
    "    \"utils.py\": \"\"\"def calculate_sum(a, b):\n",
    "    '''Calculate sum of two numbers'''\n",
    "    return a + b\n",
    "\n",
    "def calculate_product(a, b):\n",
    "    '''Calculate product of two numbers'''\n",
    "    return a * b\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "    \n",
    "    def add(self, value):\n",
    "        self.result += value\n",
    "        return self.result\n",
    "\"\"\",\n",
    "    \"models.py\": \"\"\"class User:\n",
    "    def __init__(self, name, email):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "    \n",
    "    def get_profile(self):\n",
    "        return {'name': self.name, 'email': self.email}\n",
    "\n",
    "class Post:\n",
    "    def __init__(self, title, content):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filename, content in code_files.items():\n",
    "    with open(os.path.join(code_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"üíª Code Documentation Pipeline:\\n\")\n",
    "\n",
    "# Process Python files with code chunker\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=code_dir, ext=[\".py\"])\n",
    "    .chunk_with(\"code\", chunk_size=150)\n",
    "    .export_with(\"json\", file=\"./code_chunks.json\")\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Processed {len(docs)} Python files\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nüìÑ File {i}:\")\n",
    "    print(f\"  Chunks: {len(doc.chunks)}\")\n",
    "    if doc.chunks:\n",
    "        print(f\"  First chunk preview:\")\n",
    "        print(f\"    {doc.chunks[0].text[:80]}...\")\n",
    "\n",
    "if os.path.exists(\"./code_chunks.json\"):\n",
    "    print(f\"\\n‚úÖ Code chunks exported to: code_chunks.json\")\n",
    "\n",
    "print(\"\\n‚ú® Code documentation pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0762f",
   "metadata": {},
   "source": [
    "## 12. Markdown Processing Pipeline\n",
    "\n",
    "Handle markdown with awareness of tables and code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb992cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Markdown Processing Pipeline:\n",
      "\n",
      "‚úÖ Processed markdown document\n",
      "  Total chunks: 4\n",
      "\n",
      "üìä Tables found: 1\n",
      "  Table 1: MarkdownTable(content='| Model | Accuracy | F1 Sco...\n",
      "\n",
      "üíª Code blocks found: 1\n",
      "  Code 1: MarkdownCode(content='def train_model(da...\n",
      "\n",
      "üìÑ Chunks preview:\n",
      "  1. # Project Documentation\n",
      "\n",
      "## Introduction\n",
      "This project demons...\n",
      "  2. \n",
      "## Features\n",
      "- Data preprocessing\n",
      "- Model training\n",
      "- Evaluat...\n",
      "  3. \n",
      "\n",
      "## Results Table\n",
      "...\n",
      "\n",
      "‚ú® Markdown processed with full structure awareness!\n"
     ]
    }
   ],
   "source": [
    "# Create sample markdown file\n",
    "md_content = \"\"\"# Project Documentation\n",
    "\n",
    "## Introduction\n",
    "This project demonstrates machine learning concepts.\n",
    "\n",
    "## Features\n",
    "- Data preprocessing\n",
    "- Model training\n",
    "- Evaluation metrics\n",
    "\n",
    "## Code Example\n",
    "```python\n",
    "def train_model(data):\n",
    "    model = LinearRegression()\n",
    "    model.fit(data.X, data.y)\n",
    "    return model\n",
    "```\n",
    "\n",
    "## Results Table\n",
    "| Model | Accuracy | F1 Score |\n",
    "|-------|----------|----------|\n",
    "| LR    | 0.85     | 0.82     |\n",
    "| RF    | 0.92     | 0.90     |\n",
    "\n",
    "## Conclusion\n",
    "Machine learning provides powerful tools for data analysis.\n",
    "\"\"\"\n",
    "\n",
    "md_file = os.path.join(demo_dir, \"project_doc.md\")\n",
    "with open(md_file, 'w') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(\"üìù Markdown Processing Pipeline:\\n\")\n",
    "\n",
    "# Process markdown with awareness\n",
    "doc = (Pipeline()\n",
    "    .fetch_from(\"file\", path=md_file)\n",
    "    .process_with(\"markdown\")\n",
    "    .chunk_with(\"recursive\", chunk_size=100)\n",
    "    .run())\n",
    "\n",
    "print(f\"‚úÖ Processed markdown document\")\n",
    "print(f\"  Total chunks: {len(doc.chunks)}\")\n",
    "\n",
    "# Access markdown metadata\n",
    "if hasattr(doc, 'tables') and doc.tables:\n",
    "    print(f\"\\nüìä Tables found: {len(doc.tables)}\")\n",
    "    for i, table in enumerate(doc.tables, 1):\n",
    "        # Convert table to string representation\n",
    "        table_str = str(table) if hasattr(table, '__str__') else repr(table)\n",
    "        print(f\"  Table {i}: {table_str[:50]}...\")\n",
    "\n",
    "if hasattr(doc, 'code') and doc.code:\n",
    "    print(f\"\\nüíª Code blocks found: {len(doc.code)}\")\n",
    "    for i, code in enumerate(doc.code, 1):\n",
    "        # Convert code to string representation\n",
    "        code_str = str(code) if hasattr(code, '__str__') else repr(code)\n",
    "        print(f\"  Code {i}: {code_str[:40]}...\")\n",
    "\n",
    "print(f\"\\nüìÑ Chunks preview:\")\n",
    "for i, chunk in enumerate(doc.chunks[:3], 1):\n",
    "    print(f\"  {i}. {chunk.text[:60]}...\")\n",
    "\n",
    "print(\"\\n‚ú® Markdown processed with full structure awareness!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d43d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Pipeline Validation & Patterns\n",
    "\n",
    "## 13. Pipeline Validation\n",
    "\n",
    "Pipelines validate configuration before execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00c310a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline Validation Examples:\n",
      "\n",
      "1Ô∏è‚É£ Valid Pipeline:\n",
      "   ‚úÖ Valid: Has chunker and fetcher\n",
      "\n",
      "2Ô∏è‚É£ Valid Pipeline (No Fetcher):\n",
      "   ‚úÖ Valid: Has chunker and text input\n",
      "\n",
      "3Ô∏è‚É£ Invalid Pipeline (No Chunker):\n",
      "   ‚úÖ Expected error caught: ValueError\n",
      "\n",
      "4Ô∏è‚É£ Invalid Pipeline (Multiple Chefs):\n",
      "   ‚úÖ Expected error caught: ValueError\n",
      "\n",
      "‚úÖ Pipeline validates configuration before execution\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Pipeline Validation Examples:\\n\")\n",
    "\n",
    "# Valid pipeline - has chunker and input\n",
    "print(\"1Ô∏è‚É£ Valid Pipeline:\")\n",
    "try:\n",
    "    doc = (Pipeline()\n",
    "        .fetch_from(\"file\", path=os.path.join(demo_dir, \"doc1.txt\"))\n",
    "        .chunk_with(\"recursive\", chunk_size=50)\n",
    "        .run())\n",
    "    print(\"   ‚úÖ Valid: Has chunker and fetcher\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Valid pipeline - text input, no fetcher needed\n",
    "print(\"\\n2Ô∏è‚É£ Valid Pipeline (No Fetcher):\")\n",
    "try:\n",
    "    doc = (Pipeline()\n",
    "        .chunk_with(\"recursive\", chunk_size=50)\n",
    "        .run(texts=\"Hello world\"))\n",
    "    print(\"   ‚úÖ Valid: Has chunker and text input\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Invalid pipeline - no chunker\n",
    "print(\"\\n3Ô∏è‚É£ Invalid Pipeline (No Chunker):\")\n",
    "try:\n",
    "    doc = (Pipeline()\n",
    "        .fetch_from(\"file\", path=os.path.join(demo_dir, \"doc1.txt\"))\n",
    "        .run())\n",
    "    print(\"   ‚ùå Should have failed!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Expected error caught: {type(e).__name__}\")\n",
    "\n",
    "# Invalid pipeline - multiple chefs\n",
    "print(\"\\n4Ô∏è‚É£ Invalid Pipeline (Multiple Chefs):\")\n",
    "try:\n",
    "    doc = (Pipeline()\n",
    "        .process_with(\"text\")\n",
    "        .process_with(\"markdown\")  # Second chef!\n",
    "        .chunk_with(\"recursive\", chunk_size=50)\n",
    "        .run(texts=\"test\"))\n",
    "    print(\"   ‚ùå Should have failed!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Expected error caught: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline validates configuration before execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc0938",
   "metadata": {},
   "source": [
    "## 14. Return Values\n",
    "\n",
    "Pipeline returns Document or list[Document] depending on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d03591e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Return Value Examples:\n",
      "\n",
      "1Ô∏è‚É£ Single File ‚Üí Document:\n",
      "   Type: Document\n",
      "   Is Document: True\n",
      "   Chunks: 6\n",
      "\n",
      "2Ô∏è‚É£ Directory ‚Üí list[Document]:\n",
      "   Type: list\n",
      "   Is list: True\n",
      "   Documents: 2\n",
      "\n",
      "3Ô∏è‚É£ Multiple Texts ‚Üí list[Document]:\n",
      "   Type: list\n",
      "   Is list: True\n",
      "   Documents: 3\n",
      "\n",
      "4Ô∏è‚É£ Single Text ‚Üí Document:\n",
      "   Type: Document\n",
      "   Is Document: True\n",
      "   Chunks: 1\n",
      "\n",
      "‚úÖ Return type depends on input: single ‚Üí Document, multiple ‚Üí list[Document]\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Return Value Examples:\\n\")\n",
    "\n",
    "# Single file ‚Üí Document\n",
    "print(\"1Ô∏è‚É£ Single File ‚Üí Document:\")\n",
    "result = (Pipeline()\n",
    "    .fetch_from(\"file\", path=os.path.join(demo_dir, \"doc1.txt\"))\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "print(f\"   Type: {type(result).__name__}\")\n",
    "print(f\"   Is Document: {isinstance(result, Document)}\")\n",
    "print(f\"   Chunks: {len(result.chunks)}\")\n",
    "\n",
    "# Directory ‚Üí list[Document]\n",
    "print(\"\\n2Ô∏è‚É£ Directory ‚Üí list[Document]:\")\n",
    "result = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=demo_dir, ext=[\".txt\"])\n",
    "    .chunk_with(\"recursive\", chunk_size=50)\n",
    "    .run())\n",
    "print(f\"   Type: {type(result).__name__}\")\n",
    "print(f\"   Is list: {isinstance(result, list)}\")\n",
    "print(f\"   Documents: {len(result)}\")\n",
    "\n",
    "# Multiple texts ‚Üí list[Document]\n",
    "print(\"\\n3Ô∏è‚É£ Multiple Texts ‚Üí list[Document]:\")\n",
    "result = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=30)\n",
    "    .run(texts=[\"Text 1\", \"Text 2\", \"Text 3\"]))\n",
    "print(f\"   Type: {type(result).__name__}\")\n",
    "print(f\"   Is list: {isinstance(result, list)}\")\n",
    "print(f\"   Documents: {len(result)}\")\n",
    "\n",
    "# Single text ‚Üí Document\n",
    "print(\"\\n4Ô∏è‚É£ Single Text ‚Üí Document:\")\n",
    "result = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=30)\n",
    "    .run(texts=\"Single text input\"))\n",
    "print(f\"   Type: {type(result).__name__}\")\n",
    "print(f\"   Is Document: {isinstance(result, Document)}\")\n",
    "print(f\"   Chunks: {len(result.chunks)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Return type depends on input: single ‚Üí Document, multiple ‚Üí list[Document]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ca5a3",
   "metadata": {},
   "source": [
    "## 15. Error Handling\n",
    "\n",
    "Pipelines provide clear error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37d9622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Error Handling Examples:\n",
      "\n",
      "1Ô∏è‚É£ File Not Found:\n",
      "   ‚úÖ Caught RuntimeError\n",
      "\n",
      "2Ô∏è‚É£ Configuration Error:\n",
      "   ‚úÖ Caught ValueError\n",
      "   Message: Pipeline must include a fetcher component (use fetch_from()) or provide text inp...\n",
      "\n",
      "3Ô∏è‚É£ Invalid Directory:\n",
      "   ‚úÖ Caught RuntimeError\n",
      "\n",
      "‚úÖ Pipelines provide clear, actionable error messages\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ°Ô∏è Error Handling Examples:\\n\")\n",
    "\n",
    "# File not found\n",
    "print(\"1Ô∏è‚É£ File Not Found:\")\n",
    "try:\n",
    "    doc = (Pipeline()\n",
    "        .fetch_from(\"file\", path=\"nonexistent.txt\")\n",
    "        .chunk_with(\"recursive\", chunk_size=50)\n",
    "        .run())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"   ‚úÖ Caught FileNotFoundError\")\n",
    "    print(f\"   Message: {str(e)[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Caught {type(e).__name__}\")\n",
    "\n",
    "# Configuration error (no input source)\n",
    "print(\"\\n2Ô∏è‚É£ Configuration Error:\")\n",
    "try:\n",
    "    doc = (Pipeline()\n",
    "        .chunk_with(\"recursive\", chunk_size=50)\n",
    "        .run())  # No texts or fetcher!\n",
    "except (ValueError, RuntimeError) as e:\n",
    "    print(f\"   ‚úÖ Caught {type(e).__name__}\")\n",
    "    print(f\"   Message: {str(e)[:80]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Caught {type(e).__name__}: {str(e)[:80]}...\")\n",
    "\n",
    "# Invalid directory\n",
    "print(\"\\n3Ô∏è‚É£ Invalid Directory:\")\n",
    "try:\n",
    "    docs = (Pipeline()\n",
    "        .fetch_from(\"file\", dir=\"/nonexistent/path\")\n",
    "        .chunk_with(\"recursive\", chunk_size=50)\n",
    "        .run())\n",
    "except (FileNotFoundError, ValueError, OSError) as e:\n",
    "    print(f\"   ‚úÖ Caught {type(e).__name__}\")\n",
    "    print(f\"   Message: {str(e)[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Caught {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipelines provide clear, actionable error messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc47bfe",
   "metadata": {},
   "source": [
    "## 16. Automatic Component Reordering\n",
    "\n",
    "Pipeline automatically reorders components to follow CHOMP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b07c32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Automatic Reordering Examples:\n",
      "\n",
      "1Ô∏è‚É£ Components Added Out of Order:\n",
      "   ‚úÖ Pipeline executed successfully!\n",
      "   Chunks: 3\n",
      "   Pipeline auto-reordered to: Chef ‚Üí Chunker ‚Üí Refinery\n",
      "\n",
      "2Ô∏è‚É£ Highly Mixed Order:\n",
      "   ‚úÖ Still works perfectly!\n",
      "   Chunks: 3\n",
      "   Auto-reordered to: Chef ‚Üí Chunker ‚Üí Refinery ‚Üí Porter\n",
      "\n",
      "‚úÖ Pipeline automatically follows CHOMP architecture\n",
      "   You can add components in any order!\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Automatic Reordering Examples:\\n\")\n",
    "\n",
    "sample_text = \"Machine learning enables computers to learn from data.\"\n",
    "\n",
    "# Components added in \"wrong\" order\n",
    "print(\"1Ô∏è‚É£ Components Added Out of Order:\")\n",
    "doc = (Pipeline()\n",
    "    .refine_with(\"overlap\", context_size=0.2)  # 4th in CHOMP\n",
    "    .chunk_with(\"token\", chunk_size=20)        # 3rd in CHOMP\n",
    "    .process_with(\"text\")                      # 2nd in CHOMP\n",
    "    .run(texts=sample_text))\n",
    "\n",
    "print(f\"   ‚úÖ Pipeline executed successfully!\")\n",
    "print(f\"   Chunks: {len(doc.chunks)}\")\n",
    "print(f\"   Pipeline auto-reordered to: Chef ‚Üí Chunker ‚Üí Refinery\")\n",
    "\n",
    "# Even more mixed up order\n",
    "print(\"\\n2Ô∏è‚É£ Highly Mixed Order:\")\n",
    "doc = (Pipeline()\n",
    "    .export_with(\"json\", file=\"./reorder_test.json\")  # 5th in CHOMP\n",
    "    .refine_with(\"overlap\", context_size=0.1)         # 4th in CHOMP\n",
    "    .process_with(\"text\")                             # 2nd in CHOMP\n",
    "    .chunk_with(\"token\", chunk_size=20)               # 3rd in CHOMP\n",
    "    .run(texts=sample_text))\n",
    "\n",
    "print(f\"   ‚úÖ Still works perfectly!\")\n",
    "print(f\"   Chunks: {len(doc.chunks)}\")\n",
    "print(f\"   Auto-reordered to: Chef ‚Üí Chunker ‚Üí Refinery ‚Üí Porter\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline automatically follows CHOMP architecture\")\n",
    "print(\"   You can add components in any order!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744424d6",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove temporary files created during demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8169d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up temporary files...\n",
      "\n",
      "  ‚úÖ Deleted directory: tmpjlgca46s\n",
      "  ‚úÖ Deleted directory: tmppy45esrg\n",
      "  ‚úÖ Deleted directory: tmpagk7mjst\n",
      "  ‚úÖ Deleted file: ./pipeline_chunks.json\n",
      "  ‚úÖ Deleted directory: pipeline_dataset\n",
      "  ‚úÖ Deleted file: ./rag_chunks.json\n",
      "  ‚úÖ Deleted file: ./code_chunks.json\n",
      "  ‚úÖ Deleted file: ./reorder_test.json\n",
      "\n",
      "‚úÖ Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# List of items to clean up\n",
    "cleanup_items = [\n",
    "    demo_dir,\n",
    "    kb_dir,\n",
    "    code_dir,\n",
    "    \"./pipeline_chunks.json\",\n",
    "    \"./pipeline_dataset\",\n",
    "    \"./rag_chunks.json\",\n",
    "    \"./code_chunks.json\",\n",
    "    \"./reorder_test.json\"\n",
    "]\n",
    "\n",
    "print(\"üßπ Cleaning up temporary files...\\n\")\n",
    "\n",
    "for item in cleanup_items:\n",
    "    try:\n",
    "        if os.path.isfile(item):\n",
    "            os.remove(item)\n",
    "            print(f\"  ‚úÖ Deleted file: {item}\")\n",
    "        elif os.path.isdir(item):\n",
    "            shutil.rmtree(item)\n",
    "            print(f\"  ‚úÖ Deleted directory: {os.path.basename(item)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ‚ÑπÔ∏è Not found: {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error deleting {item}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ced06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Pipeline API Complete Guide\n",
    "\n",
    "### CHOMP Architecture\n",
    "\n",
    "The Pipeline follows this order automatically:\n",
    "\n",
    "1. **Fetcher** (Optional) - Retrieve data from sources\n",
    "2. **Chef** (Optional) - Preprocess and transform data\n",
    "3. **Chunker** (Required) - Split into manageable chunks\n",
    "4. **Refinery** (Optional, Chainable) - Enhance chunks\n",
    "5. **Porter/Handshake** (Optional) - Export or store\n",
    "\n",
    "### Pipeline Methods\n",
    "\n",
    "| Method | Purpose | Required | Chainable |\n",
    "|--------|---------|----------|-----------|\n",
    "| `fetch_from()` | Fetch data from files/APIs | No* | No |\n",
    "| `process_with()` | Process with chef | No | No |\n",
    "| `chunk_with()` | Split into chunks | **Yes** | No |\n",
    "| `refine_with()` | Enhance chunks | No | **Yes** |\n",
    "| `export_with()` | Export to formats | No | Yes |\n",
    "| `store_in()` | Store in vector DB | No | Yes |\n",
    "| `run()` | Execute pipeline | **Yes** | No |\n",
    "\n",
    "*Required unless providing `texts` to `run()`\n",
    "\n",
    "### Key Features\n",
    "\n",
    "‚úÖ **Fluent API**: Chain methods naturally\n",
    "```python\n",
    "Pipeline().fetch_from(...).chunk_with(...).run()\n",
    "```\n",
    "\n",
    "‚úÖ **Auto-reordering**: Add components in any order\n",
    "```python\n",
    "# Works! Auto-reorders to CHOMP\n",
    "Pipeline().refine_with(...).chunk_with(...).process_with(...)\n",
    "```\n",
    "\n",
    "‚úÖ **Multiple refineries**: Chain as many as needed\n",
    "```python\n",
    ".refine_with(\"overlap\", ...).refine_with(\"embeddings\", ...)\n",
    "```\n",
    "\n",
    "‚úÖ **Flexible input**: File, directory, or direct text\n",
    "```python\n",
    ".fetch_from(\"file\", path=\"doc.txt\")  # Single file\n",
    ".fetch_from(\"file\", dir=\"./docs\")    # Directory\n",
    ".run(texts=\"Direct text\")             # No fetcher\n",
    "```\n",
    "\n",
    "‚úÖ **Smart returns**: Single ‚Üí Document, Multiple ‚Üí list[Document]\n",
    "```python\n",
    "doc = pipeline.run(texts=\"one\")      # Document\n",
    "docs = pipeline.run(texts=[\"1\",\"2\"]) # list[Document]\n",
    "```\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "**1. Simple Text Processing**:\n",
    "```python\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=512)\n",
    "    .run(texts=\"Your text here\"))\n",
    "```\n",
    "\n",
    "**2. File Processing**:\n",
    "```python\n",
    "doc = (Pipeline()\n",
    "    .fetch_from(\"file\", path=\"document.txt\")\n",
    "    .process_with(\"text\")\n",
    "    .chunk_with(\"semantic\", threshold=0.8)\n",
    "    .run())\n",
    "```\n",
    "\n",
    "**3. RAG Pipeline**:\n",
    "```python\n",
    "docs = (Pipeline()\n",
    "    .fetch_from(\"file\", dir=\"./docs\")\n",
    "    .chunk_with(\"semantic\", chunk_size=1024)\n",
    "    .refine_with(\"overlap\", context_size=100)\n",
    "    .refine_with(\"embeddings\", model=\"potion-base-8M\")\n",
    "    .run())\n",
    "```\n",
    "\n",
    "**4. Export Pipeline**:\n",
    "```python\n",
    "doc = (Pipeline()\n",
    "    .chunk_with(\"recursive\", chunk_size=512)\n",
    "    .export_with(\"json\", file=\"chunks.json\")\n",
    "    .run(texts=\"Text to export\"))\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Always specify chunk_size** - Required for most chunkers\n",
    "‚úÖ **Match chunkers to content** - Use `code` for code, `semantic` for varied content\n",
    "‚úÖ **Use refineries for RAG** - Add overlap and embeddings\n",
    "‚úÖ **Filter extensions** - Use `ext=[\".txt\", \".md\"]` in directory mode\n",
    "‚úÖ **Chain refineries** - Combine overlap, embeddings, etc.\n",
    "‚úÖ **Handle errors** - Use try/except for file operations\n",
    "\n",
    "### Validation Rules\n",
    "\n",
    "‚úÖ **Must have**: At least one chunker\n",
    "‚úÖ **Must have**: Fetcher OR text input via `run(texts=...)`\n",
    "‚ùå **Cannot have**: Multiple chefs (only one allowed)\n",
    "‚ùå **Cannot have**: Multiple chunkers (only one allowed)\n",
    "\n",
    "### Error Messages\n",
    "\n",
    "Pipelines provide clear, actionable errors:\n",
    "- `FileNotFoundError` - File or directory not found\n",
    "- `ValueError` - Invalid configuration or missing required components\n",
    "- `RuntimeError` - Pipeline execution failed\n",
    "\n",
    "### Pipeline Recipes\n",
    "\n",
    "Load pre-configured pipelines:\n",
    "```python\n",
    "# From Chonkie Hub\n",
    "pipeline = Pipeline.from_recipe(\"markdown\")\n",
    "\n",
    "# From local file\n",
    "pipeline = Pipeline.from_recipe(\"custom\", path=\"./recipe.json\")\n",
    "```\n",
    "\n",
    "### Return Values\n",
    "\n",
    "| Input | Return Type | Example |\n",
    "|-------|-------------|---------|\n",
    "| Single file | `Document` | `.fetch_from(\"file\", path=...)` |\n",
    "| Directory | `list[Document]` | `.fetch_from(\"file\", dir=...)` |\n",
    "| Single text | `Document` | `.run(texts=\"one\")` |\n",
    "| Multiple texts | `list[Document]` | `.run(texts=[\"1\", \"2\"])` |\n",
    "\n",
    "### Component Overview\n",
    "\n",
    "- **Fetchers**: file, API, database (see [Fetchers](https://docs.chonkie.ai/oss/fetchers/overview))\n",
    "- **Chefs**: text, markdown, table (see [Chefs](https://docs.chonkie.ai/oss/chefs/overview))\n",
    "- **Chunkers**: recursive, semantic, token, code (see [Chunkers](https://docs.chonkie.ai/oss/chunkers/overview))\n",
    "- **Refineries**: overlap, embeddings (see [Refineries](https://docs.chonkie.ai/oss/refinery/overview))\n",
    "- **Porters**: JSON, Datasets (see [Porters](https://docs.chonkie.ai/oss/porters/overview))\n",
    "- **Handshakes**: Chroma, Qdrant, Pinecone (see [Handshakes](https://docs.chonkie.ai/oss/handshakes/overview))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chonkie (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
