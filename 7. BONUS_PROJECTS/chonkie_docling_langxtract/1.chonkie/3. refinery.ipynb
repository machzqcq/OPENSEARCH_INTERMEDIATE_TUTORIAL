{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0fe31f",
   "metadata": {},
   "source": [
    "# Chonkie Refineries - Complete Guide\n",
    "\n",
    "This notebook demonstrates all Refinery types in Chonkie: **OverlapRefinery** and **EmbeddingsRefinery**.\n",
    "\n",
    "## What are Refineries?\n",
    "\n",
    "Refineries are post-processors that enhance chunks with additional information. Each Refinery adds different capabilities:\n",
    "\n",
    "- **OverlapRefinery**: Adds overlapping context from adjacent chunks (prefix or suffix)\n",
    "- **EmbeddingsRefinery**: Adds vector embeddings to chunks for semantic search\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ Enhance chunks after chunking process\n",
    "- ‚úÖ Maintain contextual continuity with overlap\n",
    "- ‚úÖ Enable semantic search with embeddings\n",
    "- ‚úÖ Configurable context size and methods\n",
    "- ‚úÖ Works with any chunker output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad1a69",
   "metadata": {},
   "source": [
    "## Visual Overview\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#ff6b6b','primaryTextColor':'#fff','primaryBorderColor':'#c92a2a','lineColor':'#339af0','secondaryColor':'#51cf66','tertiaryColor':'#ffd43b','background':'#f8f9fa','mainBkg':'#e3fafc','secondBkg':'#fff3bf','tertiaryBkg':'#ffe3e3','textColor':'#212529','fontSize':'16px'}}}%%\n",
    "\n",
    "graph TB\n",
    "    Start([üîß Refineries<br/>Chunk Enhancers]):::startClass\n",
    "    \n",
    "    Start --> RefineryType{Choose Refinery Type}:::decisionClass\n",
    "    \n",
    "    RefineryType -->|Add Context| OverlapRef[\"üìä OverlapRefinery<br/>Add adjacent context\"]:::overlapClass\n",
    "    RefineryType -->|Add Embeddings| EmbedRef[\"üß¨ EmbeddingsRefinery<br/>Add vector embeddings\"]:::embedClass\n",
    "    \n",
    "    OverlapRef --> OverlapConfig{Configuration}:::decisionClass\n",
    "    EmbedRef --> EmbedConfig{Configuration}:::decisionClass\n",
    "    \n",
    "    OverlapConfig -->|Method| MethodChoice[\"method='suffix' or 'prefix'\"]:::paramClass\n",
    "    OverlapConfig -->|Context Size| SizeChoice[\"context_size=0.25 or int\"]:::paramClass\n",
    "    OverlapConfig -->|Mode| ModeChoice[\"mode='token' or 'recursive'\"]:::paramClass\n",
    "    OverlapConfig -->|Merge| MergeChoice[\"merge=True or False\"]:::paramClass\n",
    "    \n",
    "    EmbedConfig -->|Model| ModelChoice[\"embedding_model=str or instance\"]:::paramClass\n",
    "    \n",
    "    MethodChoice --> OverlapProcess[\"Process Chunks\"]:::processClass\n",
    "    SizeChoice --> OverlapProcess\n",
    "    ModeChoice --> OverlapProcess\n",
    "    MergeChoice --> OverlapProcess\n",
    "    \n",
    "    ModelChoice --> EmbedProcess[\"Process Chunks\"]:::processClass\n",
    "    \n",
    "    OverlapProcess --> OverlapOutput[\"üì¶ Chunks with Context<br/>text + context_before/after\"]:::outputClass\n",
    "    EmbedProcess --> EmbedOutput[\"üì¶ Chunks with Embeddings<br/>text + embedding vector\"]:::embedOutputClass\n",
    "    \n",
    "    OverlapOutput --> UseCases{Use Cases}:::decisionClass\n",
    "    EmbedOutput --> UseCases\n",
    "    \n",
    "    UseCases -->|Context| QA[\"‚ùì Question Answering<br/>Summarization\"]:::useClass\n",
    "    UseCases -->|Search| Semantic[\"üîç Semantic Search<br/>Similarity\"]:::useClass\n",
    "    UseCases -->|Storage| VectorDB[\"üíæ Vector Database<br/>Retrieval\"]:::useClass\n",
    "    \n",
    "    classDef startClass fill:#4c6ef5,stroke:#364fc7,stroke-width:3px,color:#fff\n",
    "    classDef decisionClass fill:#7950f2,stroke:#5f3dc4,stroke-width:2px,color:#fff\n",
    "    classDef overlapClass fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:#fff\n",
    "    classDef embedClass fill:#20c997,stroke:#087f5b,stroke-width:2px,color:#fff\n",
    "    classDef paramClass fill:#748ffc,stroke:#4c6ef5,stroke-width:2px,color:#fff\n",
    "    classDef processClass fill:#ffd43b,stroke:#fab005,stroke-width:2px,color:#333\n",
    "    classDef outputClass fill:#51cf66,stroke:#37b24d,stroke-width:2px,color:#fff\n",
    "    classDef embedOutputClass fill:#69db7c,stroke:#40c057,stroke-width:2px,color:#fff\n",
    "    classDef useClass fill:#ff922b,stroke:#e8590c,stroke-width:2px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add31cf",
   "metadata": {},
   "source": [
    "## Setup - Create Test Content\n",
    "\n",
    "First, we'll create test content to demonstrate each Refinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ab3d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test content created:\n",
      "  üìù short: 140 characters\n",
      "  üìù medium: 349 characters\n",
      "  üìù long: 779 characters\n"
     ]
    }
   ],
   "source": [
    "# Test strings for demonstrations\n",
    "test_strings = {\n",
    "    \"short\": \"This is the first sentence. This is the second sentence, providing context. This is the third sentence, which needs context from the second.\",\n",
    "    \n",
    "    \"medium\": \"\"\"Machine learning has revolutionized technology. Deep learning models can recognize patterns. \n",
    "Neural networks are inspired by the human brain. They consist of interconnected layers of nodes. \n",
    "Training these models requires large datasets. The data is processed through multiple iterations. \n",
    "Eventually, the model learns to make accurate predictions.\"\"\",\n",
    "    \n",
    "    \"long\": \"\"\"Artificial intelligence is transforming industries worldwide. From healthcare to finance, AI applications are becoming ubiquitous.\n",
    "\n",
    "Machine learning algorithms can analyze vast amounts of data. They identify patterns that humans might miss. This capability has led to breakthroughs in various fields.\n",
    "\n",
    "Natural language processing enables computers to understand human language. Chatbots and virtual assistants rely on NLP technology. They can answer questions and provide information.\n",
    "\n",
    "Computer vision allows machines to interpret visual information. Self-driving cars use computer vision to navigate roads. Medical imaging benefits from AI-powered diagnosis tools.\n",
    "\n",
    "The future of AI holds immense potential. Researchers continue to push boundaries and discover new applications.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Test content created:\")\n",
    "for name, content in test_strings.items():\n",
    "    print(f\"  üìù {name}: {len(content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb9ae2",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Chonkie to use refineries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b631059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All Refineries imported successfully!\n",
      "  üìä OverlapRefinery: <class 'chonkie.refinery.overlap.OverlapRefinery'>\n",
      "  üß¨ EmbeddingsRefinery: <class 'chonkie.refinery.embedding.EmbeddingsRefinery'>\n",
      "  ‚úÇÔ∏è TokenChunker: <class 'chonkie.chunker.token.TokenChunker'>\n"
     ]
    }
   ],
   "source": [
    "# Install chonkie\n",
    "# !pip install chonkie\n",
    "\n",
    "from chonkie import OverlapRefinery, EmbeddingsRefinery, TokenChunker\n",
    "\n",
    "print(\"‚úÖ All Refineries imported successfully!\")\n",
    "print(f\"  üìä OverlapRefinery: {OverlapRefinery}\")\n",
    "print(f\"  üß¨ EmbeddingsRefinery: {EmbeddingsRefinery}\")\n",
    "print(f\"  ‚úÇÔ∏è TokenChunker: {TokenChunker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716caac0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: OverlapRefinery\n",
    "\n",
    "## 1. OverlapRefinery - Basic Initialization\n",
    "\n",
    "Initialize OverlapRefinery with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1570d6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä OverlapRefinery Initialization Options:\n",
      "\n",
      "  1. Default: OverlapRefinery(tokenizer=<chonkie.tokenizer.AutoTokenizer object at 0x000002A3FFEA8830>, context_size=0.25, mode=token, method=suffix, merge=True, inplace=True)\n",
      "  2. Suffix Method (50% context): OverlapRefinery(tokenizer=<chonkie.tokenizer.AutoTokenizer object at 0x000002A3CF3BDE50>, context_size=0.5, mode=token, method=suffix, merge=True, inplace=True)\n",
      "  3. Prefix Method (30 chars): OverlapRefinery(tokenizer=<chonkie.tokenizer.AutoTokenizer object at 0x000002A380045FD0>, context_size=30, mode=token, method=prefix, merge=False, inplace=True)\n",
      "\n",
      "‚úÖ All initialization options work!\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Default initialization (character tokenizer, 25% context)\n",
    "overlap_default = OverlapRefinery()\n",
    "print(\"üìä OverlapRefinery Initialization Options:\\n\")\n",
    "print(f\"  1. Default: {overlap_default}\")\n",
    "\n",
    "# Option 2: With suffix method (adds context from NEXT chunk)\n",
    "overlap_suffix = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=0.5,  # 50% of chunk size\n",
    "    method=\"suffix\",\n",
    "    merge=True\n",
    ")\n",
    "print(f\"  2. Suffix Method (50% context): {overlap_suffix}\")\n",
    "\n",
    "# Option 3: With prefix method (adds context from PREVIOUS chunk)\n",
    "overlap_prefix = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=30,  # Absolute number of characters\n",
    "    method=\"prefix\",\n",
    "    merge=False\n",
    ")\n",
    "print(f\"  3. Prefix Method (30 chars): {overlap_prefix}\")\n",
    "\n",
    "print(\"\\n‚úÖ All initialization options work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4fd10",
   "metadata": {},
   "source": [
    "## 2. OverlapRefinery - Suffix Method\n",
    "\n",
    "Add context from the NEXT chunk to the end of the current chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98e1707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original Chunks (3 chunks):\n",
      "\n",
      "  Chunk 1: This is the first sentence. This is the second sen\n",
      "  Length: 50 chars\n",
      "\n",
      "  Chunk 2: tence, providing context. This is the third senten\n",
      "  Length: 50 chars\n",
      "\n",
      "  Chunk 3: ce, which needs context from the second.\n",
      "  Length: 40 chars\n",
      "\n",
      "\n",
      "üìä Refined Chunks with SUFFIX Context (3 chunks):\n",
      "\n",
      "  Chunk 1: This is the first sentence. This is the second sentence, providing context.\n",
      "  Length: 75 chars\n",
      "\n",
      "  Chunk 2: tence, providing context. This is the third sentence, which needs context f\n",
      "  Length: 75 chars\n",
      "\n",
      "  Chunk 3: ce, which needs context from the second.\n",
      "  Length: 40 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker\n",
    "\n",
    "# Step 1: Chunk the text\n",
    "chunker = TokenChunker(chunk_size=50)\n",
    "chunks = chunker(test_strings[\"short\"])\n",
    "\n",
    "print(f\"üìÑ Original Chunks ({len(chunks)} chunks):\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text}\")\n",
    "    print(f\"  Length: {len(chunk.text)} chars\\n\")\n",
    "\n",
    "# Step 2: Add suffix overlap (context from next chunk)\n",
    "overlap_refinery = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=0.5,  # 50% of chunk size as overlap\n",
    "    method=\"suffix\",\n",
    "    merge=True\n",
    ")\n",
    "\n",
    "refined_chunks = overlap_refinery(chunks)\n",
    "\n",
    "print(f\"\\nüìä Refined Chunks with SUFFIX Context ({len(refined_chunks)} chunks):\\n\")\n",
    "for i, chunk in enumerate(refined_chunks, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text}\")\n",
    "    print(f\"  Length: {len(chunk.text)} chars\")\n",
    "    if hasattr(chunk, 'context_after'):\n",
    "        print(f\"  Context after: {chunk.context_after}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6690aa",
   "metadata": {},
   "source": [
    "## 3. OverlapRefinery - Prefix Method\n",
    "\n",
    "Add context from the PREVIOUS chunk to the beginning of the current chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51befc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Refined Chunks with PREFIX Context (3 chunks):\n",
      "\n",
      "  Chunk 1: This is the first sentence. This is the second sen\n",
      "  Length: 50 chars\n",
      "\n",
      "  Chunk 2:  the second sentence, providing context. This is the third senten\n",
      "  Length: 65 chars\n",
      "\n",
      "  Chunk 3: he third sentence, which needs context from the second.\n",
      "  Length: 55 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Chunk the text\n",
    "chunker = TokenChunker(chunk_size=50)\n",
    "chunks = chunker(test_strings[\"short\"])\n",
    "\n",
    "# Step 2: Add prefix overlap (context from previous chunk)\n",
    "overlap_refinery = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=0.3,  # 30% of chunk size as overlap\n",
    "    method=\"prefix\",\n",
    "    merge=True\n",
    ")\n",
    "\n",
    "refined_chunks = overlap_refinery(chunks)\n",
    "\n",
    "print(f\"üìä Refined Chunks with PREFIX Context ({len(refined_chunks)} chunks):\\n\")\n",
    "for i, chunk in enumerate(refined_chunks, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text}\")\n",
    "    print(f\"  Length: {len(chunk.text)} chars\")\n",
    "    if hasattr(chunk, 'context_before'):\n",
    "        print(f\"  Context before: {chunk.context_before}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84119b28",
   "metadata": {},
   "source": [
    "## 4. OverlapRefinery - Merge vs No Merge\n",
    "\n",
    "- Compare merged context (added to text) vs separate context fields.\n",
    "- If merge=True, the calculated context is directly prepended (for prefix) or appended (for suffix) to the chunk.text. If False, the context is stored in chunk.context attribute without modifying chunk.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e45c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Testing with 9 chunks\n",
      "\n",
      "üìä Option 1: MERGE = TRUE (context added to text)\n",
      "\n",
      "  Chunk 1:\n",
      "  Text: Machine learning has revolutionized technology. Deep learnin...\n",
      "  Length: 60 chars\n",
      "\n",
      " Context: nology. Deep learnin..\n",
      "  Chunk 2:\n",
      "  Text: nology. Deep learning models can recogni...\n",
      "  Length: 40 chars\n",
      "\n",
      " Context: None..\n",
      "\n",
      "üìä Option 2: MERGE = FALSE (context separate)\n",
      "\n",
      "  Chunk 1:\n",
      "  Text: Machine learning has revolutionized technology. Deep learnin...\n",
      "  Length: 60 chars\n",
      " Context: nology. Deep learnin..\n",
      "\n",
      "  Chunk 2:\n",
      "  Text: nology. Deep learning models can recogni...\n",
      "  Length: 40 chars\n",
      " Context: None..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chunk the text\n",
    "chunker = TokenChunker(chunk_size=40)\n",
    "chunks = chunker(test_strings[\"medium\"])\n",
    "\n",
    "print(f\"üìÑ Testing with {len(chunks)} chunks\\n\")\n",
    "\n",
    "# Option 1: Merge=True (context merged into text)\n",
    "print(\"üìä Option 1: MERGE = TRUE (context added to text)\\n\")\n",
    "overlap_merged = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=20,\n",
    "    method=\"suffix\",\n",
    "    merge=True\n",
    ")\n",
    "refined_merged = overlap_merged(chunks[:2])  # Just first 2 chunks for demo\n",
    "\n",
    "for i, chunk in enumerate(refined_merged, 1):\n",
    "    print(f\"  Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text[:100]}...\")\n",
    "    print(f\"  Length: {len(chunk.text)} chars\\n\")\n",
    "    print(f\" Context: {chunk.context}..\")\n",
    "\n",
    "# Option 2: Merge=False (context in separate field)\n",
    "print(\"\\nüìä Option 2: MERGE = FALSE (context separate)\\n\")\n",
    "overlap_separate = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=20,\n",
    "    method=\"suffix\",\n",
    "    merge=False\n",
    ")\n",
    "refined_separate = overlap_separate(chunks[:2])\n",
    "\n",
    "for i, chunk in enumerate(refined_separate, 1):\n",
    "    print(f\"  Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text[:80]}...\")\n",
    "    print(f\"  Length: {len(chunk.text)} chars\")\n",
    "    print(f\" Context: {chunk.context}..\")\n",
    "    if hasattr(chunk, 'context_after'):\n",
    "        print(f\"  Context (separate): {chunk.context_after}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fd8ba",
   "metadata": {},
   "source": [
    "## 5. OverlapRefinery - Context Size Comparison\n",
    "\n",
    "Compare different context sizes (fraction vs absolute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8572f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Context Size Comparison\n",
      "\n",
      "Testing with 3 chunks\n",
      "\n",
      "1Ô∏è‚É£ Context Size = 0.25 (25% of chunk)\n",
      "   Avg length: 70 chars\n",
      "\n",
      "2Ô∏è‚É£ Context Size = 0.5 (50% of chunk)\n",
      "   Avg length: 95 chars\n",
      "\n",
      "3Ô∏è‚É£ Context Size = 30 (absolute chars)\n",
      "   Avg length: 115 chars\n",
      "\n",
      "‚úÖ Different context sizes demonstrated!\n"
     ]
    }
   ],
   "source": [
    "# Chunk the text\n",
    "chunker = TokenChunker(chunk_size=60)\n",
    "chunks = chunker(test_strings[\"medium\"])\n",
    "test_chunks = chunks[:3]  # Use first 3 chunks\n",
    "\n",
    "print(\"üìä Context Size Comparison\\n\")\n",
    "print(f\"Testing with {len(test_chunks)} chunks\\n\")\n",
    "\n",
    "# Test 1: 25% context (fraction)\n",
    "print(\"1Ô∏è‚É£ Context Size = 0.25 (25% of chunk)\")\n",
    "refinery_25 = OverlapRefinery(context_size=0.25, method=\"suffix\", merge=True)\n",
    "refined_25 = refinery_25(test_chunks)\n",
    "print(f\"   Avg length: {sum(len(c.text) for c in refined_25) / len(refined_25):.0f} chars\\n\")\n",
    "\n",
    "# Test 2: 50% context (fraction)\n",
    "print(\"2Ô∏è‚É£ Context Size = 0.5 (50% of chunk)\")\n",
    "refinery_50 = OverlapRefinery(context_size=0.5, method=\"suffix\", merge=True)\n",
    "refined_50 = refinery_50(test_chunks)\n",
    "print(f\"   Avg length: {sum(len(c.text) for c in refined_50) / len(refined_50):.0f} chars\\n\")\n",
    "\n",
    "# Test 3: 30 chars absolute\n",
    "print(\"3Ô∏è‚É£ Context Size = 30 (absolute chars)\")\n",
    "refinery_abs = OverlapRefinery(context_size=30, method=\"suffix\", merge=True)\n",
    "refined_abs = refinery_abs(test_chunks)\n",
    "print(f\"   Avg length: {sum(len(c.text) for c in refined_abs) / len(refined_abs):.0f} chars\\n\")\n",
    "\n",
    "print(\"‚úÖ Different context sizes demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb136a",
   "metadata": {},
   "source": [
    "## 6. OverlapRefinery - Use Case: Question Answering\n",
    "\n",
    "Demonstrate how overlap helps maintain context for QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cb47fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå WITHOUT Overlap Refinery:\n",
      "\n",
      "  Chunk 1: Python was created by Guido van Rossum in 1991. It\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 2:  emphasizes code readability and simplicity.\n",
      "The l\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 3: anguage supports multiple programming paradigms. T\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 4: hese include procedural, object-oriented, and func\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 5: tional programming.\n",
      "Python's extensive standard li\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 6: brary is one of its greatest strengths. It provide\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 7: s modules for various tasks.\n",
      "Many companies use Py\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 8: thon for web development, data science, and automa\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "  Chunk 9: tion.\n",
      "  ‚Üí Context: Limited to chunk only\n",
      "\n",
      "\n",
      "‚úÖ WITH Overlap Refinery:\n",
      "\n",
      "  Chunk 1: Python was created by Guido van Rossum in 1991. It\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 2: n Rossum in 1991. It emphasizes code readability and simplicity.\n",
      "The l\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 3: nd simplicity.\n",
      "The language supports multiple programming paradigms. T\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 4: ramming paradigms. These include procedural, object-oriented, and func\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 5: t-oriented, and functional programming.\n",
      "Python's extensive standard li\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 6: xtensive standard library is one of its greatest strengths. It provide\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 7: trengths. It provides modules for various tasks.\n",
      "Many companies use Py\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 8: any companies use Python for web development, data science, and automa\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "  Chunk 9:  science, and automation.\n",
      "  ‚Üí Context: Includes previous chunk context\n",
      "\n",
      "üí° With overlap, each chunk has more context for answering questions!\n"
     ]
    }
   ],
   "source": [
    "qa_text = \"\"\"Python was created by Guido van Rossum in 1991. It emphasizes code readability and simplicity.\n",
    "The language supports multiple programming paradigms. These include procedural, object-oriented, and functional programming.\n",
    "Python's extensive standard library is one of its greatest strengths. It provides modules for various tasks.\n",
    "Many companies use Python for web development, data science, and automation.\"\"\"\n",
    "\n",
    "# WITHOUT overlap\n",
    "print(\"‚ùå WITHOUT Overlap Refinery:\\n\")\n",
    "chunker = TokenChunker(chunk_size=50)\n",
    "chunks_no_overlap = chunker(qa_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks_no_overlap, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text}\")\n",
    "    print(f\"  ‚Üí Context: Limited to chunk only\\n\")\n",
    "\n",
    "# WITH overlap\n",
    "print(\"\\n‚úÖ WITH Overlap Refinery:\\n\")\n",
    "overlap_refinery = OverlapRefinery(\n",
    "    context_size=0.4,\n",
    "    method=\"prefix\",\n",
    "    merge=True\n",
    ")\n",
    "chunks_with_overlap = overlap_refinery(chunks_no_overlap)\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_overlap, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text}\")\n",
    "    print(f\"  ‚Üí Context: Includes previous chunk context\\n\")\n",
    "\n",
    "print(\"üí° With overlap, each chunk has more context for answering questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630a381",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: EmbeddingsRefinery\n",
    "\n",
    "## 7. EmbeddingsRefinery - Basic Initialization\n",
    "\n",
    "Initialize EmbeddingsRefinery with an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4737b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ EmbeddingsRefinery Initialization:\n",
      "\n",
      "  Model: minishlab/potion-base-32M\n",
      "  Refinery: EmbeddingsRefinery(embedding_model=Model2VecEmbeddings(model=minishlab/potion-base-32M))\n",
      "\n",
      "‚úÖ EmbeddingsRefinery ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize with model string identifier\n",
    "em_refinery = EmbeddingsRefinery(\n",
    "    embedding_model=\"minishlab/potion-base-32M\"\n",
    ")\n",
    "\n",
    "print(\"üß¨ EmbeddingsRefinery Initialization:\\n\")\n",
    "print(f\"  Model: minishlab/potion-base-32M\")\n",
    "print(f\"  Refinery: {em_refinery}\")\n",
    "print(\"\\n‚úÖ EmbeddingsRefinery ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c747a",
   "metadata": {},
   "source": [
    "## 8. EmbeddingsRefinery - Add Embeddings to Chunks\n",
    "\n",
    "Add vector embeddings to chunks for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed216225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original Chunks (6 chunks):\n",
      "\n",
      "  Chunk 1: Machine learning has revolutionized technology. Deep learnin...\n",
      "  Has embedding: True\n",
      "\n",
      "  Chunk 2: g models can recognize patterns. \n",
      "Neural networks are inspir...\n",
      "  Has embedding: True\n",
      "\n",
      "  Chunk 3: ed by the human brain. They consist of interconnected layers...\n",
      "  Has embedding: True\n",
      "\n",
      "  Chunk 4:  of nodes. \n",
      "Training these models requires large datasets. T...\n",
      "  Has embedding: True\n",
      "\n",
      "  Chunk 5: he data is processed through multiple iterations. \n",
      "Eventuall...\n",
      "  Has embedding: True\n",
      "\n",
      "  Chunk 6: y, the model learns to make accurate predictions....\n",
      "  Has embedding: True\n",
      "\n",
      "\n",
      "üß¨ Chunks with Embeddings (6 chunks):\n",
      "\n",
      "  Chunk 1: Machine learning has revolutionized technology. Deep learnin...\n",
      "  ‚úÖ Has embedding: shape (512,), dtype float32\n",
      "  First 5 values: [ 0.00299574  0.18857926 -0.17724113  0.06512574 -0.02891888]\n",
      "\n",
      "  Chunk 2: g models can recognize patterns. \n",
      "Neural networks are inspir...\n",
      "  ‚úÖ Has embedding: shape (512,), dtype float32\n",
      "  First 5 values: [ 0.25840315  0.07568364 -0.12907434 -0.04131316 -0.19902714]\n",
      "\n",
      "  Chunk 3: ed by the human brain. They consist of interconnected layers...\n",
      "  ‚úÖ Has embedding: shape (512,), dtype float32\n",
      "  First 5 values: [-0.09304962  0.09901723 -0.17905122 -0.03884055 -0.1590328 ]\n",
      "\n",
      "  Chunk 4:  of nodes. \n",
      "Training these models requires large datasets. T...\n",
      "  ‚úÖ Has embedding: shape (512,), dtype float32\n",
      "  First 5 values: [ 0.26874962  0.0171121   0.06310201  0.02878202 -0.20861174]\n",
      "\n",
      "  Chunk 5: he data is processed through multiple iterations. \n",
      "Eventuall...\n",
      "  ‚úÖ Has embedding: shape (512,), dtype float32\n",
      "  First 5 values: [-0.00502743  0.21667583 -0.09161084 -0.00176129 -0.03346768]\n",
      "\n",
      "  Chunk 6: y, the model learns to make accurate predictions....\n",
      "  ‚úÖ Has embedding: shape (512,), dtype float32\n",
      "  First 5 values: [ 0.00624713  0.05315128 -0.09968819  0.05879216 -0.0095893 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Chunk the text\n",
    "test_text = test_strings[\"medium\"]\n",
    "chunker = TokenChunker(chunk_size=60)\n",
    "chunks = chunker(test_text)\n",
    "\n",
    "print(f\"üìÑ Original Chunks ({len(chunks)} chunks):\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text[:60]}...\")\n",
    "    print(f\"  Has embedding: {hasattr(chunk, 'embedding')}\\n\")\n",
    "\n",
    "# Step 2: Add embeddings\n",
    "em_refinery = EmbeddingsRefinery(\n",
    "    embedding_model=\"minishlab/potion-base-32M\"\n",
    ")\n",
    "\n",
    "chunks_with_embeddings = em_refinery(chunks)\n",
    "\n",
    "print(f\"\\nüß¨ Chunks with Embeddings ({len(chunks_with_embeddings)} chunks):\\n\")\n",
    "for i, chunk in enumerate(chunks_with_embeddings, 1):\n",
    "    print(f\"  Chunk {i}: {chunk.text[:60]}...\")\n",
    "    if hasattr(chunk, 'embedding'):\n",
    "        print(f\"  ‚úÖ Has embedding: shape {chunk.embedding.shape}, dtype {chunk.embedding.dtype}\")\n",
    "        print(f\"  First 5 values: {chunk.embedding[:5]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e35d64",
   "metadata": {},
   "source": [
    "## 9. EmbeddingsRefinery - Embedding Properties\n",
    "\n",
    "Explore the properties of generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e4b0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Embedding Analysis:\n",
      "\n",
      "Chunk 1: \"Machine learning is \"\n",
      "  Shape: (512,)\n",
      "  Dtype: float32\n",
      "  Min value: -0.1618\n",
      "  Max value: 0.1756\n",
      "  Mean: -0.0005\n",
      "  Std: 0.0442\n",
      "\n",
      "Chunk 2: \"powerful. Deep learn\"\n",
      "  Shape: (512,)\n",
      "  Dtype: float32\n",
      "  Min value: -0.1402\n",
      "  Max value: 0.1917\n",
      "  Mean: 0.0004\n",
      "  Std: 0.0442\n",
      "\n",
      "Chunk 3: \"ing is a subset. Neu\"\n",
      "  Shape: (512,)\n",
      "  Dtype: float32\n",
      "  Min value: -0.1765\n",
      "  Max value: 0.1641\n",
      "  Mean: 0.0005\n",
      "  Std: 0.0442\n",
      "\n",
      "Chunk 4: \"ral networks drive A\"\n",
      "  Shape: (512,)\n",
      "  Dtype: float32\n",
      "  Min value: -0.1694\n",
      "  Max value: 0.1540\n",
      "  Mean: 0.0010\n",
      "  Std: 0.0442\n",
      "\n",
      "Chunk 5: \"I.\"\n",
      "  Shape: (512,)\n",
      "  Dtype: float32\n",
      "  Min value: -0.1266\n",
      "  Max value: 0.1258\n",
      "  Mean: -0.0018\n",
      "  Std: 0.0442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create chunks and add embeddings\n",
    "test_text = \"Machine learning is powerful. Deep learning is a subset. Neural networks drive AI.\"\n",
    "chunker = TokenChunker(chunk_size=20)\n",
    "chunks = chunker(test_text)\n",
    "\n",
    "em_refinery = EmbeddingsRefinery(embedding_model=\"minishlab/potion-base-32M\")\n",
    "chunks_with_embeddings = em_refinery(chunks)\n",
    "\n",
    "print(\"üß¨ Embedding Analysis:\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_embeddings, 1):\n",
    "    if hasattr(chunk, 'embedding'):\n",
    "        emb = chunk.embedding\n",
    "        print(f\"Chunk {i}: \\\"{chunk.text}\\\"\")\n",
    "        print(f\"  Shape: {emb.shape}\")\n",
    "        print(f\"  Dtype: {emb.dtype}\")\n",
    "        print(f\"  Min value: {emb.min():.4f}\")\n",
    "        print(f\"  Max value: {emb.max():.4f}\")\n",
    "        print(f\"  Mean: {emb.mean():.4f}\")\n",
    "        print(f\"  Std: {emb.std():.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8818796",
   "metadata": {},
   "source": [
    "## 10. EmbeddingsRefinery - Semantic Similarity\n",
    "\n",
    "Calculate similarity between chunk embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a24ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Semantic Similarity Analysis:\n",
      "\n",
      "Text 1: \"Python is a programming language\"\n",
      "Text 2: \"Java is also a programming language\"\n",
      "Similarity: 0.7655\n",
      "Interpretation: üü° Somewhat Similar\n",
      "\n",
      "Text 1: \"Python is a programming language\"\n",
      "Text 2: \"I love eating pizza for dinner\"\n",
      "Similarity: 0.2002\n",
      "Interpretation: üî¥ Not Similar\n",
      "\n",
      "Text 1: \"Java is also a programming language\"\n",
      "Text 2: \"I love eating pizza for dinner\"\n",
      "Similarity: 0.0951\n",
      "Interpretation: üî¥ Not Similar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(emb1, emb2):\n",
    "    \"\"\"Calculate cosine similarity between two embeddings.\"\"\"\n",
    "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "\n",
    "# Test different texts\n",
    "texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Java is also a programming language\",\n",
    "    \"I love eating pizza for dinner\"\n",
    "]\n",
    "\n",
    "# Create chunks and add embeddings\n",
    "chunker = TokenChunker(chunk_size=100)  # Large enough for full sentences\n",
    "all_chunks = []\n",
    "for text in texts:\n",
    "    chunks = chunker(text)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "em_refinery = EmbeddingsRefinery(embedding_model=\"minishlab/potion-base-32M\")\n",
    "embedded_chunks = em_refinery(all_chunks)\n",
    "\n",
    "print(\"üîç Semantic Similarity Analysis:\\n\")\n",
    "\n",
    "# Compare all pairs\n",
    "for i in range(len(embedded_chunks)):\n",
    "    for j in range(i+1, len(embedded_chunks)):\n",
    "        if hasattr(embedded_chunks[i], 'embedding') and hasattr(embedded_chunks[j], 'embedding'):\n",
    "            sim = cosine_similarity(embedded_chunks[i].embedding, embedded_chunks[j].embedding)\n",
    "            print(f\"Text 1: \\\"{embedded_chunks[i].text}\\\"\")\n",
    "            print(f\"Text 2: \\\"{embedded_chunks[j].text}\\\"\")\n",
    "            print(f\"Similarity: {sim:.4f}\")\n",
    "            print(f\"Interpretation: {'üü¢ Very Similar' if sim > 0.8 else 'üü° Somewhat Similar' if sim > 0.5 else 'üî¥ Not Similar'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f35186",
   "metadata": {},
   "source": [
    "## 11. EmbeddingsRefinery - Use Case: Vector Search\n",
    "\n",
    "Demonstrate semantic search using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36c90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Semantic Search Results\n",
      "\n",
      "Query: \"What programming language is best for AI?\"\n",
      "\n",
      "Top 3 Most Relevant Results:\n",
      "\n",
      "1. Score: 0.4881\n",
      "   Text: JavaScript is the primary language for web development.\n",
      "\n",
      "2. Score: 0.4169\n",
      "   Text: Python is excellent for data science and machine learning projects.\n",
      "\n",
      "3. Score: 0.3884\n",
      "   Text: Web browsers execute JavaScript code on the client side.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a mini document corpus\n",
    "documents = [\n",
    "    \"Python is excellent for data science and machine learning projects.\",\n",
    "    \"JavaScript is the primary language for web development.\",\n",
    "    \"Machine learning models require large amounts of training data.\",\n",
    "    \"Web browsers execute JavaScript code on the client side.\",\n",
    "    \"Neural networks are inspired by biological brain structures.\"\n",
    "]\n",
    "\n",
    "# Chunk and embed documents\n",
    "chunker = TokenChunker(chunk_size=150)\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "em_refinery = EmbeddingsRefinery(embedding_model=\"minishlab/potion-base-32M\")\n",
    "embedded_chunks = em_refinery(all_chunks)\n",
    "\n",
    "# Search query\n",
    "query = \"What programming language is best for AI?\"\n",
    "query_chunks = chunker(query)\n",
    "query_embedded = em_refinery(query_chunks)\n",
    "query_embedding = query_embedded[0].embedding\n",
    "\n",
    "print(f\"üîç Semantic Search Results\\n\")\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "# Calculate similarities\n",
    "results = []\n",
    "for chunk in embedded_chunks:\n",
    "    if hasattr(chunk, 'embedding'):\n",
    "        sim = cosine_similarity(query_embedding, chunk.embedding)\n",
    "        results.append((chunk.text, sim))\n",
    "\n",
    "# Sort by similarity\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 3 Most Relevant Results:\\n\")\n",
    "for i, (text, score) in enumerate(results[:3], 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Text: {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee612e35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Combined Refineries\n",
    "\n",
    "## 12. Using Both Refineries Together\n",
    "\n",
    "Combine OverlapRefinery and EmbeddingsRefinery for maximum enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4483cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Starting with 10 chunks\n",
      "\n",
      "‚úÖ Step 1: Added overlap context\n",
      "‚úÖ Step 2: Added embeddings\n",
      "\n",
      "üéØ Fully Refined Chunks (10 chunks):\n",
      "\n",
      "Chunk 1:\n",
      "  Text: Artificial intelligence is transforming industries worldwide. From hea...\n",
      "  Has overlap context: True\n",
      "  Has embedding: True\n",
      "  Embedding shape: (512,)\n",
      "\n",
      "Chunk 2:\n",
      "  Text:  finance, AI applications are becoming ubiquitous.\n",
      "\n",
      "Machine learning a...\n",
      "  Has overlap context: True\n",
      "  Has embedding: True\n",
      "  Embedding shape: (512,)\n",
      "\n",
      "Chunk 3:\n",
      "  Text: can analyze vast amounts of data. They identify patterns that humans m...\n",
      "  Has overlap context: True\n",
      "  Has embedding: True\n",
      "  Embedding shape: (512,)\n",
      "\n",
      "‚ú® Chunks now have both contextual overlap AND semantic embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Chunk the text\n",
    "test_text = test_strings[\"long\"]\n",
    "chunker = TokenChunker(chunk_size=80)\n",
    "chunks = chunker(test_text)\n",
    "\n",
    "print(f\"üìÑ Starting with {len(chunks)} chunks\\n\")\n",
    "\n",
    "# Step 2: Add overlap context\n",
    "overlap_refinery = OverlapRefinery(\n",
    "    context_size=0.3,\n",
    "    method=\"suffix\",\n",
    "    merge=True\n",
    ")\n",
    "chunks_with_overlap = overlap_refinery(chunks)\n",
    "print(f\"‚úÖ Step 1: Added overlap context\")\n",
    "\n",
    "# Step 3: Add embeddings\n",
    "em_refinery = EmbeddingsRefinery(\n",
    "    embedding_model=\"minishlab/potion-base-32M\"\n",
    ")\n",
    "fully_refined_chunks = em_refinery(chunks_with_overlap)\n",
    "print(f\"‚úÖ Step 2: Added embeddings\\n\")\n",
    "\n",
    "# Display results\n",
    "print(f\"üéØ Fully Refined Chunks ({len(fully_refined_chunks)} chunks):\\n\")\n",
    "for i, chunk in enumerate(fully_refined_chunks[:3], 1):  # Show first 3\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text[:70]}...\")\n",
    "    print(f\"  Has overlap context: {len(chunk.text) > 80}\")\n",
    "    print(f\"  Has embedding: {hasattr(chunk, 'embedding')}\")\n",
    "    if hasattr(chunk, 'embedding'):\n",
    "        print(f\"  Embedding shape: {chunk.embedding.shape}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚ú® Chunks now have both contextual overlap AND semantic embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8367e49",
   "metadata": {},
   "source": [
    "## 13. Real-World Pipeline: Document Processing\n",
    "\n",
    "Complete pipeline: Chunk ‚Üí Add Context ‚Üí Add Embeddings ‚Üí Ready for Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16c56bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing Document for Vector Database\n",
      "\n",
      "Document length: 779 characters\n",
      "\n",
      "üìù Step 1: Created 8 chunks\n",
      "üìä Step 2: Added overlap context\n",
      "üß¨ Step 3: Added embeddings\n",
      "\n",
      "‚úÖ Document processed! Ready for vector database insertion\n",
      "\n",
      "üì¶ Output Summary:\n",
      "  Total chunks: 8\n",
      "  Each chunk has:\n",
      "    - Text content with overlap context\n",
      "    - Vector embedding for semantic search\n",
      "    - Metadata (start_index, end_index, token_count)\n",
      "\n",
      "üìã Sample Chunk Structure:\n",
      "  text: Artificial intelligence is transforming industries worldwide...\n",
      "  embedding: array of shape (512,)\n",
      "  start_index: 0\n",
      "  end_index: 100\n",
      "  token_count: 125\n"
     ]
    }
   ],
   "source": [
    "def process_document_for_vectordb(text, chunk_size=100, context_size=0.25):\n",
    "    \"\"\"Process document with chunking, overlap, and embeddings.\"\"\"\n",
    "    # Step 1: Chunk\n",
    "    chunker = TokenChunker(chunk_size=chunk_size)\n",
    "    chunks = chunker(text)\n",
    "    print(f\"üìù Step 1: Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Step 2: Add overlap\n",
    "    overlap_refinery = OverlapRefinery(\n",
    "        context_size=context_size,\n",
    "        method=\"suffix\",\n",
    "        merge=True\n",
    "    )\n",
    "    chunks = overlap_refinery(chunks)\n",
    "    print(f\"üìä Step 2: Added overlap context\")\n",
    "    \n",
    "    # Step 3: Add embeddings\n",
    "    em_refinery = EmbeddingsRefinery(\n",
    "        embedding_model=\"minishlab/potion-base-32M\"\n",
    "    )\n",
    "    chunks = em_refinery(chunks)\n",
    "    print(f\"üß¨ Step 3: Added embeddings\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process a document\n",
    "document = test_strings[\"long\"]\n",
    "print(\"üöÄ Processing Document for Vector Database\\n\")\n",
    "print(f\"Document length: {len(document)} characters\\n\")\n",
    "\n",
    "processed_chunks = process_document_for_vectordb(document)\n",
    "\n",
    "print(f\"\\n‚úÖ Document processed! Ready for vector database insertion\")\n",
    "print(f\"\\nüì¶ Output Summary:\")\n",
    "print(f\"  Total chunks: {len(processed_chunks)}\")\n",
    "print(f\"  Each chunk has:\")\n",
    "print(f\"    - Text content with overlap context\")\n",
    "print(f\"    - Vector embedding for semantic search\")\n",
    "print(f\"    - Metadata (start_index, end_index, token_count)\")\n",
    "\n",
    "# Show sample chunk structure\n",
    "print(f\"\\nüìã Sample Chunk Structure:\")\n",
    "sample = processed_chunks[0]\n",
    "print(f\"  text: {sample.text[:60]}...\")\n",
    "if hasattr(sample, 'embedding'):\n",
    "    print(f\"  embedding: array of shape {sample.embedding.shape}\")\n",
    "print(f\"  start_index: {sample.start_index}\")\n",
    "print(f\"  end_index: {sample.end_index}\")\n",
    "print(f\"  token_count: {sample.token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c93285",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: All Refinery Types and Capabilities\n",
    "\n",
    "### Refinery Comparison Table\n",
    "\n",
    "| Refinery | Purpose | Key Parameters | Output | Use Cases |\n",
    "|----------|---------|----------------|--------|-----------||\n",
    "| **OverlapRefinery** | Add context from adjacent chunks | `context_size`, `method`, `merge` | Chunks with overlap context | QA, Summarization, Context preservation |\n",
    "| **EmbeddingsRefinery** | Add vector embeddings | `embedding_model` | Chunks with embeddings | Semantic search, Vector DB, Similarity |\n",
    "\n",
    "### OverlapRefinery Parameters\n",
    "\n",
    "- **tokenizer**: `\"character\"`, `\"word\"`, `\"gpt2\"`, or custom (default: `\"character\"`)\n",
    "- **context_size**: `float` (0-1 as fraction) or `int` (absolute tokens) (default: `0.25`)\n",
    "- **method**: `\"suffix\"` (next chunk) or `\"prefix\"` (previous chunk) (default: `\"suffix\"`)\n",
    "- **mode**: `\"token\"` or `\"recursive\"` (default: `\"token\"`)\n",
    "- **merge**: `True` (merge into text) or `False` (separate field) (default: `True`)\n",
    "\n",
    "### EmbeddingsRefinery Parameters\n",
    "\n",
    "- **embedding_model**: Model string identifier or `BaseEmbeddings` instance (required)\n",
    "\n",
    "### Methods Available\n",
    "\n",
    "All refineries support:\n",
    "- `refine(chunks)` - Refine a list of chunks\n",
    "- `__call__(chunks)` - Callable interface for refining\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **OverlapRefinery**:\n",
    "- Use `suffix` for forward-looking context (next chunk)\n",
    "- Use `prefix` for backward-looking context (previous chunk)\n",
    "- Set `context_size` to 0.25-0.5 for good balance\n",
    "- Use `merge=True` for simpler data structure\n",
    "- Useful for QA and summarization tasks\n",
    "\n",
    "‚úÖ **EmbeddingsRefinery**:\n",
    "- Choose appropriate embedding model for your domain\n",
    "- Smaller models (`32M`) are faster, larger models more accurate\n",
    "- Essential for semantic search and vector databases\n",
    "- Calculate cosine similarity for relevance scoring\n",
    "\n",
    "‚úÖ **Combined Usage**:\n",
    "- Apply OverlapRefinery first, then EmbeddingsRefinery\n",
    "- Overlap provides context, embeddings enable search\n",
    "- Perfect for production RAG (Retrieval-Augmented Generation) systems\n",
    "- Ready for vector database insertion (Pinecone, Weaviate, ChromaDB, etc.)\n",
    "\n",
    "### Integration Pattern\n",
    "\n",
    "```python\n",
    "# Standard refinement pipeline\n",
    "chunks = chunker(text)\n",
    "chunks = overlap_refinery(chunks)     # Add context\n",
    "chunks = embeddings_refinery(chunks)  # Add embeddings\n",
    "# ‚Üí Ready for vector database!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chonkie (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
