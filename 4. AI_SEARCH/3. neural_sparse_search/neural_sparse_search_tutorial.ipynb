{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395cbf71",
   "metadata": {},
   "source": [
    "# Neural Sparse Search Tutorial\n",
    "![Course](../../static_images/ai_ml_search_opensearch_intermediate.jpeg)\n",
    "\n",
    "## Overview\n",
    "Semantic search relies on dense retrieval that is based on text embedding models. However, dense methods use k-NN search, which consumes a large amount of memory and CPU resources. An alternative to semantic search, neural sparse search is implemented using an inverted index and is thus as efficient as BM25.\n",
    "\n",
    "Neural Sparse Search combines the efficiency of sparse retrieval with the relevance of neural embeddings. This notebook demonstrates various use cases and implementation techniques.\n",
    "\n",
    "To further boost search relevance, you can combine neural sparse search with dense semantic search using a hybrid query\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[\"üß† Neural Sparse Search Workflow\"] --> B{\"Search Mode\"}\n",
    "    B -->|\"Doc-Only Mode\"| C[\"üìÑ Sparse Encoding at Ingestion\"]\n",
    "    B -->|\"Bi-Encoder Mode\"| D[\"üîÑ Encoding at Both Stages\"]\n",
    "    B -->|\"ANN Mode\"| E[\"‚ö° Approximate Nearest Neighbor\"]\n",
    "    \n",
    "    C --> F[\"üîë Token-Weight Pairs\"]\n",
    "    D --> G[\"üîç Query Text Encoding\"]\n",
    "    E --> H[\"üìä Clustered Inverted Index\"]\n",
    "    \n",
    "    F --> I[\"‚úÖ Fast Retrieval\"]\n",
    "    G --> J[\"üéØ High Relevance\"]\n",
    "    H --> K[\"‚öôÔ∏è Scalable Performance\"]\n",
    "    \n",
    "    I --> L[\"üéÅ Search Results\"]\n",
    "    J --> L\n",
    "    K --> L\n",
    "    \n",
    "    M[\"üîß Ingest Pipeline\"] --> C\n",
    "    N[\"üóÇÔ∏è Rank Features Index\"] --> C\n",
    "    O[\"üé® Semantic Field\"] --> C\n",
    "    \n",
    "    style A fill:#FF6B6B,stroke:#333,stroke-width:3px,color:#fff\n",
    "    style C fill:#4ECDC4,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style D fill:#45B7D1,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style E fill:#96CEB4,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style L fill:#FFEAA7,stroke:#333,stroke-width:3px,color:#333\n",
    "    style I fill:#DDA15E,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style J fill:#BC6C25,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style K fill:#8E44AD,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style M fill:#E74C3C,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style N fill:#3498DB,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style O fill:#9B59B6,stroke:#333,stroke-width:2px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd56d12",
   "metadata": {},
   "source": [
    "## Prerequisites & Setup\n",
    "\n",
    "This notebook requires:\n",
    "- OpenSearch cluster (local or remote)\n",
    "- Python 3.8+\n",
    "- Required libraries: opensearch-py, numpy, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297a16c",
   "metadata": {},
   "source": [
    "## üê≥ Docker Setup\n",
    "- **If docker compose up fails , start it manually from shell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0024af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fully optimized OpenSearch cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network 4ai_search_opensearch-net Creating \n",
      " Network 4ai_search_opensearch-net Created \n",
      " Volume 4ai_search_opensearch-optimized-data2 Creating \n",
      " Volume 4ai_search_opensearch-optimized-data2 Created \n",
      " Volume 4ai_search_opensearch-optimized-data1 Creating \n",
      " Volume 4ai_search_opensearch-optimized-data1 Created \n",
      " Container opensearch-optimized-node1 Creating \n",
      " Container opensearch-optimized-dashboards Creating \n",
      " Container opensearch-optimized-node2 Creating \n",
      " Container opensearch-optimized-dashboards Created \n",
      " Container opensearch-optimized-node2 Created \n",
      " Container opensearch-optimized-node1 Created \n",
      " Container opensearch-optimized-node1 Starting \n",
      " Container opensearch-optimized-node2 Starting \n",
      " Container opensearch-optimized-dashboards Starting \n",
      " Container opensearch-optimized-node1 Started \n",
      " Container opensearch-optimized-node2 Started \n",
      " Container opensearch-optimized-dashboards Started \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for cluster to initialize...\n",
      "üè• Checking cluster health...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   550  100   550    0     0   2170      0 --:--:-- --:--:-- --:--:--  2173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cluster_name\" : \"opensearch-optimized-cluster\",\n",
      "  \"status\" : \"green\",\n",
      "  \"timed_out\" : false,\n",
      "  \"number_of_nodes\" : 2,\n",
      "  \"number_of_data_nodes\" : 2,\n",
      "  \"discovered_master\" : true,\n",
      "  \"discovered_cluster_manager\" : true,\n",
      "  \"active_primary_shards\" : 4,\n",
      "  \"active_shards\" : 8,\n",
      "  \"relocating_shards\" : 0,\n",
      "  \"initializing_shards\" : 0,\n",
      "  \"unassigned_shards\" : 0,\n",
      "  \"delayed_unassigned_shards\" : 0,\n",
      "  \"number_of_pending_tasks\" : 0,\n",
      "  \"number_of_in_flight_fetch\" : 0,\n",
      "  \"task_max_waiting_in_queue_millis\" : 0,\n",
      "  \"active_shards_percent_as_number\" : 100.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../\n",
    "echo \"üöÄ Starting fully optimized OpenSearch cluster...\"\n",
    "\n",
    "# Start the optimized cluster\n",
    "docker compose -f docker-compose-fully-optimized.yml down -v\n",
    "docker compose -f docker-compose-fully-optimized.yml up -d\n",
    "\n",
    "# Wait for startup\n",
    "echo \"‚è≥ Waiting for cluster to initialize...\"\n",
    "sleep 60\n",
    "\n",
    "# Check cluster health\n",
    "echo \"üè• Checking cluster health...\"\n",
    "curl -k -u admin:Developer@123 https://localhost:9200/_cluster/health?pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "469a0e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from opensearchpy import OpenSearch\n",
    "import sys, os\n",
    "from opensearchpy.helpers import bulk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311167a9",
   "metadata": {},
   "source": [
    "## Step 1: Configure OpenSearch Client\n",
    "\n",
    "Initialize connection to OpenSearch cluster with proper authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b977872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper module loaded from: /data/OPENSEARCH_INTERMEDIATE_TUTORIAL\n",
      "üìç Data directory: /data/OPENSEARCH_INTERMEDIATE_TUTORIAL/0. DATA\n",
      "üîå Connecting to OpenSearch at localhost:9200\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory of the notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(current_dir, '../../0. DATA'))\n",
    "\n",
    "# Construct the path to the directory levels up\n",
    "module_paths = [os.path.abspath(os.path.join(current_dir, '../../')),]\n",
    "\n",
    "# Add the module path to sys.path if it's not already there\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    import helpers as hp\n",
    "    print(f\"‚úÖ Helper module loaded from: {module_paths[0]}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Helper module not found: {e}\")\n",
    "\n",
    "# Configuration\n",
    "IS_AUTH = True\n",
    "HOST = 'localhost'\n",
    "\n",
    "print(f\"üìç Data directory: {DATA_DIR}\")\n",
    "print(f\"üîå Connecting to OpenSearch at {HOST}:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b50a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to opensearch v3.3.0\n",
      "üìä Cluster Status: green\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenSearch client\n",
    "if IS_AUTH:\n",
    "    client = OpenSearch(\n",
    "        hosts=[{'host': HOST, 'port': 9200}],\n",
    "        http_auth=('admin', 'Developer@123'),  # Replace with your credentials\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "else:\n",
    "    client = OpenSearch(\n",
    "        hosts=[{'host': HOST, 'port': 9200}],\n",
    "        use_ssl=False,\n",
    "        verify_certs=False,\n",
    "        ssl_assert_hostname=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"‚úÖ Connected to {info['version']['distribution']} v{info['version']['number']}\")\n",
    "    print(f\"üìä Cluster Status: {client.cluster.health()['status']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bd83a",
   "metadata": {},
   "source": [
    "## Step 2: Register and Deploy Sparse Encoding Model\n",
    "\n",
    "### Overview\n",
    "Neural sparse search requires a sparse encoding model to generate sparse vector embeddings from text. This model converts text into token-weight pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77f3c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Registering model group: neural_sparse_models_1767453012\n",
      "‚úÖ Model group registered with ID: X0JohJsBgGjK7mcRsg8u\n"
     ]
    }
   ],
   "source": [
    "# Step 2a: Register a model group\n",
    "model_group_name = f\"neural_sparse_models_{int(time.time())}\"\n",
    "print(f\"üì¶ Registering model group: {model_group_name}\")\n",
    "\n",
    "try:\n",
    "    model_group_response = client.transport.perform_request(\n",
    "        method='POST',\n",
    "        url='/_plugins/_ml/model_groups/_register',\n",
    "        body={\n",
    "            \"name\": model_group_name,\n",
    "            \"description\": \"Model group for neural sparse search examples\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model_group_id = model_group_response['model_group_id']\n",
    "    print(f\"‚úÖ Model group registered with ID: {model_group_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Model group registration error: {e}\")\n",
    "    model_group_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d674b2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Registering sparse encoding model...\n",
      "   Model: amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v3-distill\n",
      "üìã Model registration task ID: YEJohJsBgGjK7mcR4A-V\n"
     ]
    }
   ],
   "source": [
    "# Step 2b: Register sparse encoding model (doc-only mode)\n",
    "print(\"\\nüß† Registering sparse encoding model...\")\n",
    "print(\"   Model: amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v3-distill\")\n",
    "\n",
    "try:\n",
    "    register_response = client.transport.perform_request(\n",
    "        method='POST',\n",
    "        url='/_plugins/_ml/models/_register?deploy=true',\n",
    "        body={\n",
    "            \"name\": \"amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v3-distill\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"model_format\": \"TORCH_SCRIPT\",\n",
    "            \"model_group_id\": model_group_id if model_group_id else None\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    register_task_id = register_response['task_id']\n",
    "    print(f\"üìã Model registration task ID: {register_task_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model registration error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f426f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Waiting for model registration to complete...\n",
      "   Attempt 7/36: Status = COMPLETED\n",
      "‚úÖ Model registered successfully! Model ID: YUJohJsBgGjK7mcR5A-o\n"
     ]
    }
   ],
   "source": [
    "# Step 2c: Wait for model registration to complete\n",
    "print(\"\\n‚è≥ Waiting for model registration to complete...\")\n",
    "max_attempts = 36  # e.g., 3 minutes with 5 seconds interval\n",
    "attempt = 0\n",
    "model_id = None\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    try:\n",
    "        task_status = client.transport.perform_request(\n",
    "            method='GET',\n",
    "            url=f'/_plugins/_ml/tasks/{register_task_id}'\n",
    "        )\n",
    "        \n",
    "        state = task_status.get('state', 'UNKNOWN')\n",
    "        print(f\"   Attempt {attempt + 1}/{max_attempts}: Status = {state}\", end='\\r')\n",
    "        \n",
    "        if state == 'COMPLETED':\n",
    "            model_id = task_status.get('model_id')\n",
    "            print(f\"\\n‚úÖ Model registered successfully! Model ID: {model_id}\")\n",
    "            break\n",
    "        elif state == 'FAILED':\n",
    "            print(f\"\\n‚ùå Model registration failed: {task_status.get('error', 'Unknown error')}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "        attempt += 1\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error checking task status: {e}\")\n",
    "        time.sleep(5)\n",
    "        attempt += 1\n",
    "\n",
    "if not model_id:\n",
    "    print(\"‚ö†Ô∏è Model registration did not complete within timeout. Please verify manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbc55f",
   "metadata": {},
   "source": [
    "## Step 3: Create Ingest Pipeline\n",
    "\n",
    "### Overview\n",
    "The ingest pipeline automatically generates sparse embeddings when documents are indexed. The `sparse_encoding` processor uses the registered model to convert text into token-weight pairs.\n",
    "\n",
    "### Pruning Configuration: `prune_type` and `prune_ratio`\n",
    "\n",
    "When generating sparse embeddings, the model produces many token-weight pairs. **Pruning** removes less important tokens to optimize storage and query performance.\n",
    "\n",
    "#### `prune_type`\n",
    "Defines the strategy for selecting which tokens to keep:\n",
    "- **`max_ratio`** (Recommended): Keeps only the top tokens by weight ratio\n",
    "  - For example, if you set `prune_ratio: 0.1`, it keeps only the 10% of tokens with the highest weights\n",
    "  - Eliminates tokens with very low importance scores\n",
    "  - Results in smaller, more efficient sparse vectors\n",
    "  \n",
    "- Other types include `threshold` (keeps tokens above a weight threshold), but `max_ratio` is typically preferred for neural sparse search\n",
    "\n",
    "#### `prune_ratio`\n",
    "Specifies the fraction of tokens to retain (as a decimal between 0 and 1):\n",
    "- **`0.1` (10%)**: Aggressive pruning - keeps top 10% of tokens\n",
    "  - ‚úÖ Pros: Minimal index size, faster queries, lower memory usage\n",
    "  - ‚ö†Ô∏è Cons: May lose some semantic information\n",
    "  \n",
    "- **`0.2` (20%)**: Moderate pruning - keeps top 20% of tokens\n",
    "  - Good balance between efficiency and relevance\n",
    "  \n",
    "- **`0.3` (30%)**: Conservative pruning - keeps top 30% of tokens\n",
    "  - ‚úÖ Pros: Better semantic preservation\n",
    "  - ‚ö†Ô∏è Cons: Larger index, slightly slower queries\n",
    "\n",
    "#### Example Impact\n",
    "For a document producing 500 tokens:\n",
    "- `prune_ratio: 0.1` ‚Üí keeps ~50 tokens\n",
    "- `prune_ratio: 0.2` ‚Üí keeps ~100 tokens  \n",
    "- `prune_ratio: 0.3` ‚Üí keeps ~150 tokens\n",
    "\n",
    "#### Recommendation\n",
    "Start with `prune_ratio: 0.1` for most production use cases. It provides excellent query performance while maintaining sufficient semantic relevance. Adjust based on your specific relevance vs. performance requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a117cf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating ingest pipeline: neural_sparse_ingest_pipeline\n",
      "‚úÖ Ingest pipeline created successfully!\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Create ingest pipeline for sparse encoding\n",
    "    pipeline_name = \"neural_sparse_ingest_pipeline\"\n",
    "    print(f\"üîß Creating ingest pipeline: {pipeline_name}\")\n",
    "    \n",
    "    try:\n",
    "        pipeline_body = {\n",
    "            \"description\": \"Ingest pipeline for neural sparse search with automatic embedding generation\",\n",
    "            \"processors\": [\n",
    "                {\n",
    "                    \"sparse_encoding\": {\n",
    "                        \"model_id\": model_id,\n",
    "                        \"prune_type\": \"max_ratio\",\n",
    "                        \"prune_ratio\": 0.1,\n",
    "                        \"field_map\": {\n",
    "                            \"text\": \"text_embedding\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        client.transport.perform_request(\n",
    "            method='PUT',\n",
    "            url=f'/_ingest/pipeline/{pipeline_name}',\n",
    "            body=pipeline_body\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Ingest pipeline created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating ingest pipeline: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping pipeline creation - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bdd57",
   "metadata": {},
   "source": [
    "## Step 4: Create Index with Rank Features\n",
    "\n",
    "### Overview\n",
    "The index uses the `rank_features` field type to store sparse embeddings. This allows efficient sparse vector search using an inverted index.\n",
    "\n",
    "### Understanding `rank_features` Field Type\n",
    "\n",
    "The `rank_features` field type is specifically designed for storing sparse vectors (token-weight pairs) efficiently. It's the optimal choice for neural sparse search.\n",
    "\n",
    "#### What are Rank Features?\n",
    "- **Sparse vectors**: Collections of `{token: weight}` key-value pairs\n",
    "- **Example**: `{\"hello\": 6.7, \"world\": 4.7, \"greeting\": 0.97}`\n",
    "- Only non-zero values are stored (extreme sparsity = efficiency)\n",
    "- Tokens are typically words or subword units (e.g., \"##world\" for BERT tokenization)\n",
    "- Weights represent token importance/relevance for that document\n",
    "\n",
    "#### Why Use `rank_features` Instead of Other Types?\n",
    "\n",
    "| Aspect | `rank_features` | `dense_vector` | `text` field |\n",
    "|--------|-----------------|---|---|\n",
    "| **Storage** | Ultra-efficient (sparse) | Large (dense) | Not designed for vectors |\n",
    "| **Query Speed** | ‚ö° Very fast (inverted index) | üü° Medium (approximate) | ‚ùå Not suitable for embeddings |\n",
    "| **Memory** | üíæ Low | üìä Very high | üíæ Medium |\n",
    "| **Use Case** | Sparse embeddings | Dense embeddings | Full-text search |\n",
    "| **Scoring** | BM25-like ranking | Similarity scoring | Text matching |\n",
    "\n",
    "#### Key Characteristics of `rank_features`\n",
    "\n",
    "1. **Inverted Index Structure**\n",
    "   - OpenSearch builds an inverted index internally\n",
    "   - Maps each token ‚Üí list of documents containing it\n",
    "   - Enables fast retrieval without scanning all documents\n",
    "   - Similar to traditional full-text search efficiency\n",
    "\n",
    "2. **Immutable After Indexing**\n",
    "   - Cannot be updated in-place\n",
    "   - Documents must be reindexed to change embeddings\n",
    "   - Trade-off for query performance gains\n",
    "\n",
    "3. **No Retrieval of Raw Embeddings**\n",
    "   - If you set `\"excludes\": [\"text_embedding\"]` in `_source` (for disk space savings)\n",
    "   - The embedding data is permanently discarded\n",
    "   - Cannot be recovered even from stored data\n",
    "   - Decision must be made at index creation time\n",
    "\n",
    "4. **Automatic Tokenization**\n",
    "   - The sparse encoding model already produces tokens\n",
    "   - `rank_features` stores them as-is (no additional processing)\n",
    "   - Each token must be a non-negative number or string identifier\n",
    "\n",
    "#### Field Mapping Example\n",
    "\n",
    "```json\n",
    "\"mappings\": {\n",
    "  \"properties\": {\n",
    "    \"text\": {\n",
    "      \"type\": \"text\"              # Original text (full-text searchable)\n",
    "    },\n",
    "    \"text_embedding\": {\n",
    "      \"type\": \"rank_features\"     # Sparse vectors (efficient neural search)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Optimizing Disk Space with `rank_features`\n",
    "\n",
    "You can exclude embeddings from the `_source` to save 50-60% disk space:\n",
    "\n",
    "```json\n",
    "\"mappings\": {\n",
    "  \"_source\": {\n",
    "    \"excludes\": [\"text_embedding\"]  # Don't store embedding in _source\n",
    "  },\n",
    "  \"properties\": {\n",
    "    \"text\": { \"type\": \"text\" },\n",
    "    \"text_embedding\": { \"type\": \"rank_features\" }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Important**: Once excluded from `_source`, embeddings cannot be retrieved. Use this only if you don't need to inspect the embeddings later.\n",
    "\n",
    "#### Querying with `rank_features`\n",
    "\n",
    "Neural sparse queries automatically search the `rank_features` field using the inverted index:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural_sparse\": {\n",
    "      \"text_embedding\": {           # Query against rank_features field\n",
    "        \"query_text\": \"hello world\",\n",
    "        \"analyzer\": \"bert-uncased\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The search engine:\n",
    "1. Tokenizes the query text using the specified analyzer\n",
    "2. Looks up each token in the inverted index\n",
    "3. Retrieves documents containing those tokens\n",
    "4. Ranks by combined token weights\n",
    "5. Returns top matching documents\n",
    "\n",
    "#### Summary\n",
    "\n",
    "`rank_features` is the optimal field type for neural sparse search because it:\n",
    "- ‚úÖ Stores sparse embeddings efficiently\n",
    "- ‚úÖ Leverages inverted index for fast queries\n",
    "- ‚úÖ Minimizes memory overhead\n",
    "- ‚úÖ Provides BM25-style ranking with semantic understanding\n",
    "- ‚úÖ Scales to billions of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6b4c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìá Creating index: neural_sparse_demo\n",
      "‚úÖ Index created successfully!\n",
      "   - Field mapping: text -> text_embedding (rank_features)\n",
      "   - Default pipeline: neural_sparse_ingest_pipeline\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Create index for neural sparse search\n",
    "    index_name = \"neural_sparse_demo\"\n",
    "    print(f\"üìá Creating index: {index_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Delete index if it exists\n",
    "        try:\n",
    "            client.indices.delete(index=index_name)\n",
    "            print(f\"   Removed existing index\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"default_pipeline\": \"neural_sparse_ingest_pipeline\",\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    \"text_embedding\": {\n",
    "                        \"type\": \"rank_features\"\n",
    "                    },\n",
    "                    \"category\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"timestamp\": {\n",
    "                        \"type\": \"date\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        client.indices.create(index=index_name, body=index_body)\n",
    "        print(f\"‚úÖ Index created successfully!\")\n",
    "        print(f\"   - Field mapping: text -> text_embedding (rank_features)\")\n",
    "        print(f\"   - Default pipeline: neural_sparse_ingest_pipeline\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating index: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping index creation - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cc33e",
   "metadata": {},
   "source": [
    "## Step 5: Use Case 1 - E-Commerce Product Search\n",
    "\n",
    "### Scenario\n",
    "Search product catalog efficiently using sparse vectors while maintaining semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5378bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõçÔ∏è USE CASE 1: E-Commerce Product Search\n",
      "==================================================\n",
      "\n",
      "üì§ Ingesting 20 products...\n",
      "‚úÖ Successfully ingested 20 products\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüõçÔ∏è USE CASE 1: E-Commerce Product Search\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sample e-commerce products (20 stratified across categories)\n",
    "    products = [\n",
    "        # Electronics - Audio & Headphones (5 products)\n",
    "        {\n",
    "            \"id\": \"prod_001\",\n",
    "            \"text\": \"High-performance wireless Bluetooth headphones with active noise cancellation and 30-hour battery life\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_002\",\n",
    "            \"text\": \"Premium studio-grade over-ear headphones with noise isolation for professional audio engineers\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_003\",\n",
    "            \"text\": \"Compact wireless earbuds with touch controls and dual microphone noise cancellation\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_004\",\n",
    "            \"text\": \"Gaming headset with surround sound 7.1 audio and comfortable memory foam ear cups\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_005\",\n",
    "            \"text\": \"Portable Bluetooth speaker with waterproof design and 360-degree sound output\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Electronics - Mobile Devices (4 products)\n",
    "        {\n",
    "            \"id\": \"prod_006\",\n",
    "            \"text\": \"Ultra-lightweight smartphone with 5G connectivity and exceptional camera system featuring 48MP sensor\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_007\",\n",
    "            \"text\": \"Flagship tablet with large OLED display supporting stylus input for creative professionals\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_008\",\n",
    "            \"text\": \"Rugged smartphone with military-grade durability and exceptional battery endurance in extreme conditions\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_009\",\n",
    "            \"text\": \"Budget-friendly smartphone with excellent processor performance and clean software experience\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Computer Peripherals (4 products)\n",
    "        {\n",
    "            \"id\": \"prod_010\",\n",
    "            \"text\": \"Ergonomic mechanical keyboard with customizable RGB lighting and mechanical Cherry MX switches\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_011\",\n",
    "            \"text\": \"Precision gaming mouse with adjustable DPI settings and programmable side buttons\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_012\",\n",
    "            \"text\": \"Ultra-wide curved monitor with 144Hz refresh rate ideal for immersive gaming and content creation\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_013\",\n",
    "            \"text\": \"Professional external SSD with Thunderbolt connection offering blazing fast data transfer speeds\",\n",
    "            \"category\": \"Electronics\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Appliances (3 products)\n",
    "        {\n",
    "            \"id\": \"prod_014\",\n",
    "            \"text\": \"Premium stainless steel coffee maker with programmable brewing and thermal carafe temperature control\",\n",
    "            \"category\": \"Appliances\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_015\",\n",
    "            \"text\": \"High-capacity air fryer with smart presets for healthy cooking of various cuisines\",\n",
    "            \"category\": \"Appliances\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_016\",\n",
    "            \"text\": \"Energy-efficient dishwasher with soil sensors and quiet operation under 42 decibels\",\n",
    "            \"category\": \"Appliances\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Wearables (3 products)\n",
    "        {\n",
    "            \"id\": \"prod_017\",\n",
    "            \"text\": \"Durable waterproof smartwatch with fitness tracking heart rate monitoring and sleep analysis\",\n",
    "            \"category\": \"Wearables\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_018\",\n",
    "            \"text\": \"Advanced fitness band with blood oxygen monitoring and multi-sport tracking capabilities\",\n",
    "            \"category\": \"Wearables\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"prod_019\",\n",
    "            \"text\": \"Smart ring with continuous health metrics tracking and contactless payment support\",\n",
    "            \"category\": \"Wearables\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Accessories (1 product)\n",
    "        {\n",
    "            \"id\": \"prod_020\",\n",
    "            \"text\": \"Premium carrying case with shock absorption foam and weather-resistant protective design\",\n",
    "            \"category\": \"Accessories\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Ingest products\n",
    "    print(\"\\nüì§ Ingesting 20 products...\")\n",
    "    try:\n",
    "        for product in products:\n",
    "            client.index(\n",
    "                index=index_name,\n",
    "                id=product['id'],\n",
    "                body=product\n",
    "            )\n",
    "        \n",
    "        # Refresh index\n",
    "        client.indices.refresh(index=index_name)\n",
    "        print(f\"‚úÖ Successfully ingested {len(products)} products\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error ingesting products: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f08298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'wireless audio headphones'\n",
      "\n",
      "üìä Found 9 results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 9.9664\n",
      "   ID: prod_001\n",
      "   Category: Electronics\n",
      "   Text: High-performance wireless Bluetooth headphones with active noise cancellation an...\n",
      "\n",
      "2. Score: 9.6712\n",
      "   ID: prod_002\n",
      "   Category: Electronics\n",
      "   Text: Premium studio-grade over-ear headphones with noise isolation for professional a...\n",
      "\n",
      "3. Score: 8.8853\n",
      "   ID: prod_004\n",
      "   Category: Electronics\n",
      "   Text: Gaming headset with surround sound 7.1 audio and comfortable memory foam ear cup...\n",
      "\n",
      "4. Score: 8.4226\n",
      "   ID: prod_003\n",
      "   Category: Electronics\n",
      "   Text: Compact wireless earbuds with touch controls and dual microphone noise cancellat...\n",
      "\n",
      "5. Score: 8.0468\n",
      "   ID: prod_005\n",
      "   Category: Electronics\n",
      "   Text: Portable Bluetooth speaker with waterproof design and 360-degree sound output...\n",
      "\n",
      "6. Score: 2.2424\n",
      "   ID: prod_006\n",
      "   Category: Electronics\n",
      "   Text: Ultra-lightweight smartphone with 5G connectivity and exceptional camera system ...\n",
      "\n",
      "7. Score: 1.3165\n",
      "   ID: prod_008\n",
      "   Category: Electronics\n",
      "   Text: Rugged smartphone with military-grade durability and exceptional battery enduran...\n",
      "\n",
      "8. Score: 0.7015\n",
      "   ID: prod_009\n",
      "   Category: Electronics\n",
      "   Text: Budget-friendly smartphone with excellent processor performance and clean softwa...\n",
      "\n",
      "9. Score: 0.5497\n",
      "   ID: prod_019\n",
      "   Category: Wearables\n",
      "   Text: Smart ring with continuous health metrics tracking and contactless payment suppo...\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Search for products\n",
    "    print(\"\\nüîç Searching for: 'wireless audio headphones'\")\n",
    "    \n",
    "    try:\n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"neural_sparse\": {\n",
    "                    \"text_embedding\": {\n",
    "                        \"query_text\": \"wireless audio headphones\",\n",
    "                        \"analyzer\": \"bert-uncased\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"id\", \"text\", \"category\"],\n",
    "            \"size\": 10\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=index_name, body=search_query)\n",
    "        \n",
    "        print(f\"\\nüìä Found {response['hits']['total']['value']} results:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, hit in enumerate(response['hits']['hits'], 1):\n",
    "            print(f\"\\n{i}. Score: {hit['_score']:.4f}\")\n",
    "            print(f\"   ID: {hit['_source']['id']}\")\n",
    "            print(f\"   Category: {hit['_source']['category']}\")\n",
    "            print(f\"   Text: {hit['_source']['text'][:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ddb74",
   "metadata": {},
   "source": [
    "## Step 6: Use Case 2 - Customer Support Ticket Search\n",
    "\n",
    "### Scenario\n",
    "Find similar support tickets efficiently using neural sparse embeddings to help support agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ba82460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé´ USE CASE 2: Customer Support Ticket Search\n",
      "==================================================\n",
      "‚úÖ Support tickets index created\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüé´ USE CASE 2: Customer Support Ticket Search\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create separate index for support tickets\n",
    "    support_index = \"neural_sparse_support_tickets\"\n",
    "    \n",
    "    try:\n",
    "        # Delete index if it exists\n",
    "        try:\n",
    "            client.indices.delete(index=support_index)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create index\n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"default_pipeline\": \"neural_sparse_ingest_pipeline\",\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"ticket_id\": {\"type\": \"keyword\"},\n",
    "                    \"text\": {\"type\": \"text\"},\n",
    "                    \"text_embedding\": {\"type\": \"rank_features\"},\n",
    "                    \"priority\": {\"type\": \"keyword\"},\n",
    "                    \"status\": {\"type\": \"keyword\"},\n",
    "                    \"resolved_at\": {\"type\": \"date\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        client.indices.create(index=support_index, body=index_body)\n",
    "        print(f\"‚úÖ Support tickets index created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating support index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eca4f52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ Ingesting 20 support tickets...\n",
      "‚úÖ Successfully ingested 20 support tickets\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Sample support tickets (20 stratified across categories)\n",
    "    tickets = [\n",
    "        # Authentication & Login Issues (5 tickets)\n",
    "        {\n",
    "            \"ticket_id\": \"TKT001\",\n",
    "            \"text\": \"Cannot login to my account. Getting 'invalid credentials' error. I have reset password twice.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT002\",\n",
    "            \"text\": \"Authentication token expired, can't access API. Please refresh my session.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT003\",\n",
    "            \"text\": \"Two-factor authentication not working. Can't receive verification codes via SMS or email.\",\n",
    "            \"priority\": \"critical\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT004\",\n",
    "            \"text\": \"Account locked after multiple failed login attempts. Need to unlock immediately.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT005\",\n",
    "            \"text\": \"Forgot password link expired before I could reset. Requesting new reset email.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        \n",
    "        # Payment & Billing Issues (5 tickets)\n",
    "        {\n",
    "            \"ticket_id\": \"TKT006\",\n",
    "            \"text\": \"Payment processing is stuck. My order shows pending for 48 hours. Need immediate assistance.\",\n",
    "            \"priority\": \"critical\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT007\",\n",
    "            \"text\": \"Charged twice for the same purchase. Please refund the duplicate transaction.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT008\",\n",
    "            \"text\": \"Credit card declined but still charged. Payment gateway showing error message.\",\n",
    "            \"priority\": \"critical\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT009\",\n",
    "            \"text\": \"Invoice missing from my account. Need receipt for tax documentation.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT010\",\n",
    "            \"text\": \"Unable to update payment method. System keeps rejecting new card information.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        \n",
    "        # Shipping & Delivery Issues (5 tickets)\n",
    "        {\n",
    "            \"ticket_id\": \"TKT011\",\n",
    "            \"text\": \"Shipping address verification failed. System won't accept my apartment number.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT012\",\n",
    "            \"text\": \"Order tracking number invalid. Can't track my package anywhere.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT013\",\n",
    "            \"text\": \"Package delivered to wrong address. Received neighbor's order instead of mine.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT014\",\n",
    "            \"text\": \"Shipping method changed after purchase without my consent. Want refund for expedited shipping.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT015\",\n",
    "            \"text\": \"Package damaged upon arrival. Items are broken and unusable.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        \n",
    "        # Technical & Performance Issues (5 tickets)\n",
    "        {\n",
    "            \"ticket_id\": \"TKT016\",\n",
    "            \"text\": \"Website very slow, pages taking 30 seconds to load. Tried different browsers.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT017\",\n",
    "            \"text\": \"Mobile app crashes when accessing product catalog. Error on startup.\",\n",
    "            \"priority\": \"high\",\n",
    "            \"status\": \"resolved\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT018\",\n",
    "            \"text\": \"Search functionality not working. Results page shows error and won't load.\",\n",
    "            \"priority\": \"critical\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT019\",\n",
    "            \"text\": \"Checkout page stuck on payment processing screen. Unable to complete purchase.\",\n",
    "            \"priority\": \"critical\",\n",
    "            \"status\": \"open\"\n",
    "        },\n",
    "        {\n",
    "            \"ticket_id\": \"TKT020\",\n",
    "            \"text\": \"Session timeout too quick. Getting logged out while browsing products.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"status\": \"resolved\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Ingest tickets\n",
    "    print(\"\\nüì§ Ingesting 20 support tickets...\")\n",
    "    try:\n",
    "        for ticket in tickets:\n",
    "            client.index(\n",
    "                index=support_index,\n",
    "                id=ticket['ticket_id'],\n",
    "                body=ticket\n",
    "            )\n",
    "        \n",
    "        client.indices.refresh(index=support_index)\n",
    "        print(f\"‚úÖ Successfully ingested {len(tickets)} support tickets\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error ingesting tickets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e6f2354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç New Ticket: 'Account login authentication failed'\n",
      "   Finding similar tickets...\n",
      "\n",
      "üìä Found 12 similar tickets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Relevance Score: 8.9597\n",
      "   Ticket ID: TKT004\n",
      "   Priority: high | Status: open\n",
      "   Issue: Account locked after multiple failed login attempts. Need to unlock immedia...\n",
      "\n",
      "2. Relevance Score: 7.8385\n",
      "   Ticket ID: TKT001\n",
      "   Priority: high | Status: resolved\n",
      "   Issue: Cannot login to my account. Getting 'invalid credentials' error. I have res...\n",
      "\n",
      "3. Relevance Score: 5.9660\n",
      "   Ticket ID: TKT011\n",
      "   Priority: medium | Status: resolved\n",
      "   Issue: Shipping address verification failed. System won't accept my apartment numb...\n",
      "\n",
      "4. Relevance Score: 5.3457\n",
      "   Ticket ID: TKT002\n",
      "   Priority: high | Status: resolved\n",
      "   Issue: Authentication token expired, can't access API. Please refresh my session....\n",
      "\n",
      "5. Relevance Score: 4.5373\n",
      "   Ticket ID: TKT003\n",
      "   Priority: critical | Status: open\n",
      "   Issue: Two-factor authentication not working. Can't receive verification codes via...\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Search support tickets\n",
    "    print(\"\\nüîç New Ticket: 'Account login authentication failed'\")\n",
    "    print(\"   Finding similar tickets...\\n\")\n",
    "    \n",
    "    try:\n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"neural_sparse\": {\n",
    "                    \"text_embedding\": {\n",
    "                        \"query_text\": \"login authentication failed\",\n",
    "                        \"analyzer\": \"bert-uncased\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"ticket_id\", \"text\", \"priority\", \"status\"],\n",
    "            \"size\": 5\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=support_index, body=search_query)\n",
    "        \n",
    "        print(f\"üìä Found {response['hits']['total']['value']} similar tickets:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, hit in enumerate(response['hits']['hits'], 1):\n",
    "            print(f\"\\n{i}. Relevance Score: {hit['_score']:.4f}\")\n",
    "            print(f\"   Ticket ID: {hit['_source']['ticket_id']}\")\n",
    "            print(f\"   Priority: {hit['_source']['priority']} | Status: {hit['_source']['status']}\")\n",
    "            print(f\"   Issue: {hit['_source']['text'][:75]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching support tickets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8606d7c",
   "metadata": {},
   "source": [
    "## Step 7: Use Case 3 - Document Repository Search\n",
    "\n",
    "### Scenario\n",
    "Search large document repositories (technical docs, research papers, etc.) with semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc8a24fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö USE CASE 3: Document Repository Search\n",
      "==================================================\n",
      "‚úÖ Documents index created\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüìö USE CASE 3: Document Repository Search\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create index for documents\n",
    "    docs_index = \"neural_sparse_documents\"\n",
    "    \n",
    "    try:\n",
    "        # Delete index if it exists\n",
    "        try:\n",
    "            client.indices.delete(index=docs_index)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create index\n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"default_pipeline\": \"neural_sparse_ingest_pipeline\",\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"doc_id\": {\"type\": \"keyword\"},\n",
    "                    \"text\": {\"type\": \"text\"},\n",
    "                    \"text_embedding\": {\"type\": \"rank_features\"},\n",
    "                    \"doc_type\": {\"type\": \"keyword\"},\n",
    "                    \"author\": {\"type\": \"keyword\"},\n",
    "                    \"created_date\": {\"type\": \"date\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        client.indices.create(index=docs_index, body=index_body)\n",
    "        print(f\"‚úÖ Documents index created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating documents index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6860b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ Ingesting 20 technical documents...\n",
      "‚úÖ Successfully ingested 20 documents\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Sample documents (20 stratified across technical topics)\n",
    "    documents = [\n",
    "        # Machine Learning Fundamentals (4 documents)\n",
    "        {\n",
    "            \"doc_id\": \"DOC001\",\n",
    "            \"text\": \"Machine learning models require careful preprocessing of input data including normalization, tokenization, and feature scaling to achieve optimal performance.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Smith\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC002\",\n",
    "            \"text\": \"Supervised learning algorithms learn from labeled training data to predict outcomes. Classification and regression are two primary supervised learning tasks.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Smith\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC003\",\n",
    "            \"text\": \"Unsupervised learning discovers hidden patterns in unlabeled data through clustering and dimensionality reduction techniques for exploratory data analysis.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Anderson\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC004\",\n",
    "            \"text\": \"Model evaluation metrics like precision, recall, and F1-score help assess classifier performance and guide hyperparameter optimization decisions.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Wilson\"\n",
    "        },\n",
    "        \n",
    "        # Deep Learning & Neural Networks (4 documents)\n",
    "        {\n",
    "            \"doc_id\": \"DOC005\",\n",
    "            \"text\": \"Neural networks consist of interconnected layers of neurons that process information through non-linear activation functions enabling complex pattern recognition.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Johnson\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC006\",\n",
    "            \"text\": \"Convolutional neural networks excel at image processing tasks by using convolutional filters to extract spatial features from visual data efficiently.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Garcia\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC007\",\n",
    "            \"text\": \"Recurrent neural networks process sequential data through hidden state mechanisms enabling them to capture temporal dependencies in time series.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Martinez\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC008\",\n",
    "            \"text\": \"Transformer architectures revolutionized NLP by replacing recurrence with self-attention mechanisms achieving state-of-the-art performance on language tasks.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Lee\"\n",
    "        },\n",
    "        \n",
    "        # Natural Language Processing (4 documents)\n",
    "        {\n",
    "            \"doc_id\": \"DOC009\",\n",
    "            \"text\": \"Natural language processing leverages transformer models with attention mechanisms to understand contextual relationships between words in sequential text data.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Dr. Lee\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC010\",\n",
    "            \"text\": \"Word embeddings like Word2Vec and GloVe represent words as dense vectors capturing semantic relationships enabling effective text analysis and similarity computation.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Prof. Chen\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC011\",\n",
    "            \"text\": \"Named entity recognition identifies and classifies named entities like persons, organizations, and locations from unstructured text documents.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Kumar\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC012\",\n",
    "            \"text\": \"Sentiment analysis determines emotional tone and opinion polarity in text using lexicon-based and machine learning approaches for customer feedback analysis.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Patel\"\n",
    "        },\n",
    "        \n",
    "        # Distributed Systems & Infrastructure (4 documents)\n",
    "        {\n",
    "            \"doc_id\": \"DOC013\",\n",
    "            \"text\": \"Distributed computing systems utilize clustering algorithms and consensus protocols to maintain data consistency across multiple nodes in fault-tolerant architectures.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Brown\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC014\",\n",
    "            \"text\": \"MapReduce framework enables processing large datasets across distributed clusters by dividing work into map and reduce phases for parallel computation.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Davis\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC015\",\n",
    "            \"text\": \"NoSQL databases provide scalable storage for unstructured data using document models, key-value stores, and column families replacing traditional relational databases.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Taylor\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC016\",\n",
    "            \"text\": \"Microservices architecture decomposes applications into independently deployable services improving scalability and enabling continuous deployment practices.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Prof. Harris\"\n",
    "        },\n",
    "        \n",
    "        # Search & Information Retrieval (4 documents)\n",
    "        {\n",
    "            \"doc_id\": \"DOC017\",\n",
    "            \"text\": \"Search engines employ inverted indexing and ranking algorithms with query expansion techniques to retrieve relevant documents from large-scale text collections efficiently.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Prof. Chen\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC018\",\n",
    "            \"text\": \"Information retrieval systems combine keyword matching with semantic understanding using vector embeddings for improved ranking and relevance assessment.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. Robinson\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC019\",\n",
    "            \"text\": \"BM25 ranking function balances term frequency and document length normalization for robust full-text search performance in document retrieval systems.\",\n",
    "            \"doc_type\": \"technical_guide\",\n",
    "            \"author\": \"Dr. Clark\"\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"DOC020\",\n",
    "            \"text\": \"Semantic search using sparse neural embeddings combines efficiency with relevance by identifying token-weight relationships for improved retrieval accuracy.\",\n",
    "            \"doc_type\": \"research_paper\",\n",
    "            \"author\": \"Prof. White\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Ingest documents\n",
    "    print(\"\\nüì§ Ingesting 20 technical documents...\")\n",
    "    try:\n",
    "        for doc in documents:\n",
    "            client.index(\n",
    "                index=docs_index,\n",
    "                id=doc['doc_id'],\n",
    "                body=doc\n",
    "            )\n",
    "        \n",
    "        client.indices.refresh(index=docs_index)\n",
    "        print(f\"‚úÖ Successfully ingested {len(documents)} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error ingesting documents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "069ab2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'deep learning neural networks training'\n",
      "\n",
      "üìä Found 11 relevant documents:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Relevance Score: 9.9394\n",
      "   Doc ID: DOC005\n",
      "   Type: research_paper | Author: Prof. Johnson\n",
      "   Summary: Neural networks consist of interconnected layers of neurons that process informa...\n",
      "\n",
      "2. Relevance Score: 8.4649\n",
      "   Doc ID: DOC007\n",
      "   Type: technical_guide | Author: Dr. Martinez\n",
      "   Summary: Recurrent neural networks process sequential data through hidden state mechanism...\n",
      "\n",
      "3. Relevance Score: 7.4427\n",
      "   Doc ID: DOC006\n",
      "   Type: research_paper | Author: Prof. Garcia\n",
      "   Summary: Convolutional neural networks excel at image processing tasks by using convoluti...\n",
      "\n",
      "4. Relevance Score: 7.2702\n",
      "   Doc ID: DOC001\n",
      "   Type: technical_guide | Author: Dr. Smith\n",
      "   Summary: Machine learning models require careful preprocessing of input data including no...\n",
      "\n",
      "5. Relevance Score: 7.1659\n",
      "   Doc ID: DOC002\n",
      "   Type: technical_guide | Author: Dr. Smith\n",
      "   Summary: Supervised learning algorithms learn from labeled training data to predict outco...\n",
      "\n",
      "6. Relevance Score: 5.8839\n",
      "   Doc ID: DOC003\n",
      "   Type: research_paper | Author: Prof. Anderson\n",
      "   Summary: Unsupervised learning discovers hidden patterns in unlabeled data through cluste...\n",
      "\n",
      "7. Relevance Score: 5.2507\n",
      "   Doc ID: DOC020\n",
      "   Type: research_paper | Author: Prof. White\n",
      "   Summary: Semantic search using sparse neural embeddings combines efficiency with relevanc...\n",
      "\n",
      "8. Relevance Score: 2.0773\n",
      "   Doc ID: DOC012\n",
      "   Type: research_paper | Author: Prof. Patel\n",
      "   Summary: Sentiment analysis determines emotional tone and opinion polarity in text using ...\n",
      "\n",
      "9. Relevance Score: 1.5395\n",
      "   Doc ID: DOC009\n",
      "   Type: research_paper | Author: Dr. Lee\n",
      "   Summary: Natural language processing leverages transformer models with attention mechanis...\n",
      "\n",
      "10. Relevance Score: 1.0413\n",
      "   Doc ID: DOC018\n",
      "   Type: research_paper | Author: Prof. Robinson\n",
      "   Summary: Information retrieval systems combine keyword matching with semantic understandi...\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Search documents\n",
    "    print(\"\\nüîç Searching for: 'deep learning neural networks training'\\n\")\n",
    "    \n",
    "    try:\n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"neural_sparse\": {\n",
    "                    \"text_embedding\": {\n",
    "                        \"query_text\": \"deep learning neural networks training\",\n",
    "                        \"analyzer\": \"bert-uncased\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"doc_id\", \"text\", \"doc_type\", \"author\"],\n",
    "            \"size\": 10\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=docs_index, body=search_query)\n",
    "        \n",
    "        print(f\"üìä Found {response['hits']['total']['value']} relevant documents:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, hit in enumerate(response['hits']['hits'], 1):\n",
    "            print(f\"\\n{i}. Relevance Score: {hit['_score']:.4f}\")\n",
    "            print(f\"   Doc ID: {hit['_source']['doc_id']}\")\n",
    "            print(f\"   Type: {hit['_source']['doc_type']} | Author: {hit['_source']['author']}\")\n",
    "            print(f\"   Summary: {hit['_source']['text'][:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05febdaa",
   "metadata": {},
   "source": [
    "## Step 8: Performance Comparison & Analysis\n",
    "\n",
    "### Key Metrics\n",
    "Compare neural sparse search with traditional BM25 and dense semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "717ee5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PERFORMANCE COMPARISON ANALYSIS\n",
      "================================================================================\n",
      "                    Method Indexing Speed  Query Speed Memory Usage   Relevance Semantic Understanding Scalability\n",
      "BM25 (Traditional Lexical)    Very Fast ‚ö°  Very Fast ‚ö°        Low üíæ    Medium üü°                 None ‚ùå Excellent ‚úÖ\n",
      "     Dense Semantic Search         Slow üê¢     Medium üü° Very High üìäüìä Excellent ‚úÖ            Excellent ‚úÖ  Limited ‚ö†Ô∏è\n",
      "      Neural Sparse Search        Fast ‚ö°‚ö° Very Fast ‚ö°‚ö° Low-Medium üíæ Excellent ‚úÖ            Excellent ‚úÖ Excellent ‚úÖ\n",
      "\n",
      "‚ú® Key Advantages of Neural Sparse Search:\n",
      "  1. Combines efficiency of sparse retrieval with semantic relevance\n",
      "  2. Low memory footprint compared to dense embeddings\n",
      "  3. Fast indexing and query performance at scale\n",
      "  4. Interpretable token-weight pairs provide explainability\n",
      "  5. Hybrid approach balances performance and accuracy\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüìä PERFORMANCE COMPARISON ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison_data = {\n",
    "        \"Method\": [\"BM25 (Traditional Lexical)\", \"Dense Semantic Search\", \"Neural Sparse Search\"],\n",
    "        \"Indexing Speed\": [\"Very Fast ‚ö°\", \"Slow üê¢\", \"Fast ‚ö°‚ö°\"],\n",
    "        \"Query Speed\": [\"Very Fast ‚ö°\", \"Medium üü°\", \"Very Fast ‚ö°‚ö°\"],\n",
    "        \"Memory Usage\": [\"Low üíæ\", \"Very High üìäüìä\", \"Low-Medium üíæ\"],\n",
    "        \"Relevance\": [\"Medium üü°\", \"Excellent ‚úÖ\", \"Excellent ‚úÖ\"],\n",
    "        \"Semantic Understanding\": [\"None ‚ùå\", \"Excellent ‚úÖ\", \"Excellent ‚úÖ\"],\n",
    "        \"Scalability\": [\"Excellent ‚úÖ\", \"Limited ‚ö†Ô∏è\", \"Excellent ‚úÖ\"]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n‚ú® Key Advantages of Neural Sparse Search:\")\n",
    "    print(\"  1. Combines efficiency of sparse retrieval with semantic relevance\")\n",
    "    print(\"  2. Low memory footprint compared to dense embeddings\")\n",
    "    print(\"  3. Fast indexing and query performance at scale\")\n",
    "    print(\"  4. Interpretable token-weight pairs provide explainability\")\n",
    "    print(\"  5. Hybrid approach balances performance and accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ff2f3",
   "metadata": {},
   "source": [
    "## Step 9: Advanced Configuration - ANN Search Mode\n",
    "\n",
    "### Overview\n",
    "Neural Sparse ANN (Approximate Nearest Neighbor) search provides optimized query performance for large-scale datasets using clustering and approximate search techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa5c8599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° NEURAL SPARSE ANN SEARCH MODE\n",
      "================================================================================\n",
      "\n",
      "üéØ When to use Neural Sparse ANN Search:\n",
      "  ‚Ä¢ Datasets with millions to billions of documents\n",
      "  ‚Ä¢ High-throughput query scenarios requiring sub-100ms response times\n",
      "  ‚Ä¢ Need >90% recall with minimal latency trade-off\n",
      "\n",
      "üìã Configuration for ANN Search:\n",
      "\n",
      "{\n",
      "  \"settings\": {\n",
      "    \"index.sparse\": true\n",
      "  },\n",
      "  \"mappings\": {\n",
      "    \"properties\": {\n",
      "      \"sparse_embedding\": {\n",
      "        \"type\": \"sparse_vector\",\n",
      "        \"method\": {\n",
      "          \"name\": \"seismic\",\n",
      "          \"parameters\": {\n",
      "            \"n_postings\": 4000,          # Top docs per token\n",
      "            \"cluster_ratio\": 0.1,        # Clustering granularity\n",
      "            \"summary_prune_ratio\": 0.4,  # Summary vector pruning\n",
      "            \"approximate_threshold\": 1000000  # ANN activation threshold\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "üîß Query Parameters for ANN Search:\n",
      "  Parameter                         Purpose Typical Value\n",
      "          k Number of top results to return        10-100\n",
      "      top_n      Top query tokens to retain         10-50\n",
      "heap_factor Recall vs performance trade-off       1.0-2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚ö° NEURAL SPARSE ANN SEARCH MODE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéØ When to use Neural Sparse ANN Search:\")\n",
    "print(\"  ‚Ä¢ Datasets with millions to billions of documents\")\n",
    "print(\"  ‚Ä¢ High-throughput query scenarios requiring sub-100ms response times\")\n",
    "print(\"  ‚Ä¢ Need >90% recall with minimal latency trade-off\")\n",
    "\n",
    "print(\"\\nüìã Configuration for ANN Search:\")\n",
    "print(\"\"\"\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.sparse\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"sparse_embedding\": {\n",
    "        \"type\": \"sparse_vector\",\n",
    "        \"method\": {\n",
    "          \"name\": \"seismic\",\n",
    "          \"parameters\": {\n",
    "            \"n_postings\": 4000,          # Top docs per token\n",
    "            \"cluster_ratio\": 0.1,        # Clustering granularity\n",
    "            \"summary_prune_ratio\": 0.4,  # Summary vector pruning\n",
    "            \"approximate_threshold\": 1000000  # ANN activation threshold\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüîß Query Parameters for ANN Search:\")\n",
    "query_params = {\n",
    "    \"Parameter\": [\"k\", \"top_n\", \"heap_factor\"],\n",
    "    \"Purpose\": [\"Number of top results to return\", \"Top query tokens to retain\", \"Recall vs performance trade-off\"],\n",
    "    \"Typical Value\": [\"10-100\", \"10-50\", \"1.0-2.0\"]\n",
    "}\n",
    "params_df = pd.DataFrame(query_params)\n",
    "print(params_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c01f80",
   "metadata": {},
   "source": [
    "## Create ANN index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e3f9fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìá Creating index: neural_sparse_demo_ann\n",
      "‚úÖ Index created successfully!\n",
      "   - Field mapping: text -> text_embedding (rank_features)\n",
      "   - Default pipeline: neural_sparse_ingest_pipeline\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    # Create index for neural sparse search\n",
    "    index_name = \"neural_sparse_demo_ann\"\n",
    "    print(f\"üìá Creating index: {index_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Delete index if it exists\n",
    "        try:\n",
    "            client.indices.delete(index=index_name)\n",
    "            print(f\"   Removed existing index\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"default_pipeline\": \"neural_sparse_ingest_pipeline\",\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0,\n",
    "                \"index.sparse\": True\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                          \"sparse_embedding\": {\n",
    "        \"type\": \"sparse_vector\",\n",
    "        \"method\": {\n",
    "          \"name\": \"seismic\",\n",
    "          \"parameters\": {\n",
    "            \"n_postings\": 4000,          # Top docs per token\n",
    "            \"cluster_ratio\": 0.1,        # Clustering granularity\n",
    "            \"summary_prune_ratio\": 0.4,  # Summary vector pruning\n",
    "            \"approximate_threshold\": 1000000  # ANN activation threshold\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "                    \"category\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"timestamp\": {\n",
    "                        \"type\": \"date\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        client.indices.create(index=index_name, body=index_body)\n",
    "        print(f\"‚úÖ Index created successfully!\")\n",
    "        print(f\"   - Field mapping: text -> text_embedding (rank_features)\")\n",
    "        print(f\"   - Default pipeline: neural_sparse_ingest_pipeline\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating index: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping index creation - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf8ca4",
   "metadata": {},
   "source": [
    "## Ingest ANN sparse documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb48614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ INGESTING DOCUMENTS INTO NEURAL SPARSE ANN INDEX\n",
      "================================================================================\n",
      "\n",
      "üîß Creating ANN-specific ingest pipeline: neural_sparse_ann_ingest_pipeline\n",
      "‚úÖ ANN ingest pipeline created successfully!\n",
      "   Field mapping: text -> sparse_embedding\n",
      "‚úÖ Index 'neural_sparse_demo_ann' updated to use ANN pipeline\n",
      "\n",
      "üìù Preparing 30 documents across diverse topics...\n",
      "\n",
      "üì§ Ingesting 30 documents...\n",
      "   Using pipeline: neural_sparse_ann_ingest_pipeline\n",
      "\n",
      "‚úÖ Successfully ingested 30/30 documents\n",
      "   ‚Ä¢ 8 Technology documents\n",
      "   ‚Ä¢ 7 Healthcare documents\n",
      "   ‚Ä¢ 7 Finance documents\n",
      "   ‚Ä¢ 8 Environment documents\n",
      "\n",
      "üìä Total documents in index: 30\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüì§ INGESTING DOCUMENTS INTO NEURAL SPARSE ANN INDEX\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Note: The index has a field mapping issue - the pipeline maps \"text\" -> \"text_embedding\"\n",
    "    # but the ANN index expects \"sparse_embedding\". We need to either:\n",
    "    # 1. Update the pipeline to map to \"sparse_embedding\", or\n",
    "    # 2. Manually provide sparse embeddings, or\n",
    "    # 3. Create a new pipeline for this index\n",
    "    \n",
    "    # For this demo, let's create a new pipeline specific to this ANN index\n",
    "    ann_pipeline_name = \"neural_sparse_ann_ingest_pipeline\"\n",
    "    print(f\"\\nüîß Creating ANN-specific ingest pipeline: {ann_pipeline_name}\")\n",
    "    \n",
    "    try:\n",
    "        ann_pipeline_body = {\n",
    "            \"description\": \"Ingest pipeline for neural sparse ANN search with sparse_embedding field\",\n",
    "            \"processors\": [\n",
    "                {\n",
    "                    \"sparse_encoding\": {\n",
    "                        \"model_id\": model_id,\n",
    "                        \"prune_type\": \"max_ratio\",\n",
    "                        \"prune_ratio\": 0.1,\n",
    "                        \"field_map\": {\n",
    "                            \"text\": \"sparse_embedding\"  # Map to sparse_embedding instead of text_embedding\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        client.transport.perform_request(\n",
    "            method='PUT',\n",
    "            url=f'/_ingest/pipeline/{ann_pipeline_name}',\n",
    "            body=ann_pipeline_body\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ ANN ingest pipeline created successfully!\")\n",
    "        print(f\"   Field mapping: text -> sparse_embedding\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating ANN pipeline: {e}\")\n",
    "    \n",
    "    # Update the index settings to use the new pipeline\n",
    "    try:\n",
    "        client.indices.put_settings(\n",
    "            index=index_name,\n",
    "            body={\"index\": {\"default_pipeline\": ann_pipeline_name}}\n",
    "        )\n",
    "        print(f\"‚úÖ Index '{index_name}' updated to use ANN pipeline\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not update index settings: {e}\")\n",
    "    \n",
    "    # Sample documents for Neural Sparse ANN (larger dataset for ANN benefits)\n",
    "    print(\"\\nüìù Preparing 30 documents across diverse topics...\")\n",
    "    \n",
    "    ann_documents = [\n",
    "        # Technology & AI (8 documents)\n",
    "        {\n",
    "            \"id\": \"ann_001\",\n",
    "            \"text\": \"Artificial intelligence systems using deep learning neural networks demonstrate remarkable performance in computer vision tasks including image classification and object detection\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_002\",\n",
    "            \"text\": \"Natural language processing models leverage transformer architecture with attention mechanisms to understand semantic context and generate human-like text responses\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_003\",\n",
    "            \"text\": \"Machine learning algorithms require extensive training data preprocessing including normalization feature engineering and hyperparameter tuning for optimal model performance\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_004\",\n",
    "            \"text\": \"Cloud computing platforms provide scalable infrastructure services including virtual machines containerization and serverless computing for distributed applications\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_005\",\n",
    "            \"text\": \"Blockchain technology enables decentralized distributed ledger systems with cryptographic security ensuring transparent immutable transaction records\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_006\",\n",
    "            \"text\": \"Quantum computing leverages quantum mechanics principles including superposition and entanglement for exponentially faster computation on specific problem domains\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_007\",\n",
    "            \"text\": \"Edge computing brings data processing closer to IoT devices reducing latency and bandwidth consumption for real-time applications in smart cities\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_008\",\n",
    "            \"text\": \"Cybersecurity frameworks implement defense mechanisms including encryption firewalls intrusion detection systems and zero-trust architecture for enterprise protection\",\n",
    "            \"category\": \"Technology\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Healthcare & Medicine (7 documents)\n",
    "        {\n",
    "            \"id\": \"ann_009\",\n",
    "            \"text\": \"Medical diagnostic imaging systems utilize advanced computational algorithms for disease detection in radiology including MRI CT scans and ultrasound analysis\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_010\",\n",
    "            \"text\": \"Genomic sequencing technologies enable personalized medicine approaches by analyzing DNA variations for targeted therapeutic interventions and drug development\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_011\",\n",
    "            \"text\": \"Telemedicine platforms facilitate remote patient consultations using video conferencing and digital health monitoring devices for accessible healthcare delivery\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_012\",\n",
    "            \"text\": \"Pharmaceutical research employs high-throughput screening and computational drug design to discover novel therapeutic compounds for disease treatment\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_013\",\n",
    "            \"text\": \"Surgical robotics enhance precision in minimally invasive procedures using computer-assisted navigation and haptic feedback for improved patient outcomes\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_014\",\n",
    "            \"text\": \"Electronic health records integrate patient medical history laboratory results and treatment plans for coordinated care across healthcare providers\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_015\",\n",
    "            \"text\": \"Immunotherapy treatments harness the immune system to target cancer cells using checkpoint inhibitors and CAR T-cell therapy for oncology patients\",\n",
    "            \"category\": \"Healthcare\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Finance & Economics (7 documents)\n",
    "        {\n",
    "            \"id\": \"ann_016\",\n",
    "            \"text\": \"Algorithmic trading systems execute high-frequency trades using quantitative models and real-time market data analysis for optimized investment strategies\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_017\",\n",
    "            \"text\": \"Cryptocurrency markets employ blockchain-based digital assets with decentralized exchange platforms for peer-to-peer financial transactions\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_018\",\n",
    "            \"text\": \"Risk management frameworks utilize statistical modeling and stress testing to assess portfolio vulnerability and regulatory compliance in banking institutions\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_019\",\n",
    "            \"text\": \"Financial technology innovations include mobile payment solutions digital wallets and robo-advisors for automated investment portfolio management\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_020\",\n",
    "            \"text\": \"Credit scoring algorithms analyze consumer financial behavior using machine learning for loan approval decisions and interest rate determination\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_021\",\n",
    "            \"text\": \"Macroeconomic indicators including GDP inflation rates and employment statistics guide monetary policy decisions by central banking authorities\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_022\",\n",
    "            \"text\": \"Derivatives markets trade complex financial instruments including options futures and swaps for hedging strategies and speculative investment positions\",\n",
    "            \"category\": \"Finance\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \n",
    "        # Environmental Science (8 documents)\n",
    "        {\n",
    "            \"id\": \"ann_023\",\n",
    "            \"text\": \"Climate modeling systems simulate atmospheric dynamics and ocean currents using supercomputers to predict global temperature changes and weather patterns\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_024\",\n",
    "            \"text\": \"Renewable energy technologies harness solar photovoltaic panels wind turbines and hydroelectric systems for sustainable electricity generation\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_025\",\n",
    "            \"text\": \"Environmental monitoring networks deploy sensor arrays to measure air quality water contamination and ecosystem biodiversity for conservation efforts\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_026\",\n",
    "            \"text\": \"Carbon capture and storage technologies mitigate greenhouse gas emissions by capturing CO2 from industrial sources for underground sequestration\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_027\",\n",
    "            \"text\": \"Precision agriculture employs GPS-guided machinery and satellite imagery for optimized crop management reducing pesticide use and water consumption\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_028\",\n",
    "            \"text\": \"Ocean acidification studies monitor pH changes in marine ecosystems examining impacts on coral reefs and shellfish populations from carbon dioxide absorption\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_029\",\n",
    "            \"text\": \"Waste management systems integrate recycling programs composting facilities and waste-to-energy conversion for circular economy sustainability goals\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ann_030\",\n",
    "            \"text\": \"Biodiversity conservation strategies protect endangered species through habitat restoration wildlife corridors and anti-poaching enforcement measures\",\n",
    "            \"category\": \"Environment\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Ingest documents using the ANN pipeline\n",
    "    print(f\"\\nüì§ Ingesting {len(ann_documents)} documents...\")\n",
    "    print(f\"   Using pipeline: {ann_pipeline_name}\")\n",
    "    \n",
    "    try:\n",
    "        success_count = 0\n",
    "        for doc in ann_documents:\n",
    "            try:\n",
    "                client.index(\n",
    "                    index=index_name,\n",
    "                    id=doc['id'],\n",
    "                    body=doc,\n",
    "                    pipeline=ann_pipeline_name  # Explicitly use the ANN pipeline\n",
    "                )\n",
    "                success_count += 1\n",
    "            except Exception as doc_error:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to index {doc['id']}: {doc_error}\")\n",
    "        \n",
    "        # Refresh index to make documents searchable\n",
    "        client.indices.refresh(index=index_name)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully ingested {success_count}/{len(ann_documents)} documents\")\n",
    "        print(f\"   ‚Ä¢ 8 Technology documents\")\n",
    "        print(f\"   ‚Ä¢ 7 Healthcare documents\")\n",
    "        print(f\"   ‚Ä¢ 7 Finance documents\")\n",
    "        print(f\"   ‚Ä¢ 8 Environment documents\")\n",
    "        \n",
    "        # Verify ingestion\n",
    "        count = client.count(index=index_name)\n",
    "        print(f\"\\nüìä Total documents in index: {count['count']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during bulk ingestion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping document ingestion - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a96b30",
   "metadata": {},
   "source": [
    "## ANN Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4a5df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç NEURAL SPARSE ANN SEARCH QUERY EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "Searching documents using Neural Sparse ANN mode...\n",
      "Query: 'machine learning artificial intelligence deep learning'\n",
      "\n",
      "üìä ANN Search Results (28 matches):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Relevance Score: 4.5185\n",
      "   Document ID: ann_001\n",
      "   Category: Technology\n",
      "   Content: Artificial intelligence systems using deep learning neural networks demonstrate ...\n",
      "\n",
      "2. Relevance Score: 2.1186\n",
      "   Document ID: ann_003\n",
      "   Category: Technology\n",
      "   Content: Machine learning algorithms require extensive training data preprocessing includ...\n",
      "\n",
      "3. Relevance Score: 1.0815\n",
      "   Document ID: ann_006\n",
      "   Category: Technology\n",
      "   Content: Quantum computing leverages quantum mechanics principles including superposition...\n",
      "\n",
      "4. Relevance Score: 1.0130\n",
      "   Document ID: ann_020\n",
      "   Category: Finance\n",
      "   Content: Credit scoring algorithms analyze consumer financial behavior using machine lear...\n",
      "\n",
      "5. Relevance Score: 0.8795\n",
      "   Document ID: ann_013\n",
      "   Category: Healthcare\n",
      "   Content: Surgical robotics enhance precision in minimally invasive procedures using compu...\n",
      "\n",
      "6. Relevance Score: 0.8210\n",
      "   Document ID: ann_007\n",
      "   Category: Technology\n",
      "   Content: Edge computing brings data processing closer to IoT devices reducing latency and...\n",
      "\n",
      "7. Relevance Score: 0.7395\n",
      "   Document ID: ann_004\n",
      "   Category: Technology\n",
      "   Content: Cloud computing platforms provide scalable infrastructure services including vir...\n",
      "\n",
      "8. Relevance Score: 0.6559\n",
      "   Document ID: ann_009\n",
      "   Category: Healthcare\n",
      "   Content: Medical diagnostic imaging systems utilize advanced computational algorithms for...\n",
      "\n",
      "9. Relevance Score: 0.6121\n",
      "   Document ID: ann_002\n",
      "   Category: Technology\n",
      "   Content: Natural language processing models leverage transformer architecture with attent...\n",
      "\n",
      "10. Relevance Score: 0.6059\n",
      "   Document ID: ann_016\n",
      "   Category: Finance\n",
      "   Content: Algorithmic trading systems execute high-frequency trades using quantitative mod...\n",
      "\n",
      "üí° Key ANN Query Features Used:\n",
      "   ‚Ä¢ method_parameters.k=5 - Returns top 5 documents\n",
      "   ‚Ä¢ method_parameters.top_n=10 - Uses top 10 query tokens\n",
      "   ‚Ä¢ method_parameters.heap_factor=1.0 - Exact recall mode\n",
      "   ‚Ä¢ Query text automatically tokenized by sparse encoding model\n",
      "   ‚Ä¢ SEISMIC algorithm for clustered inverted index search\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüîç NEURAL SPARSE ANN SEARCH QUERY EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Example ANN search query using official documentation format\n",
    "    print(\"\\nSearching documents using Neural Sparse ANN mode...\")\n",
    "    print(\"Query: 'machine learning artificial intelligence deep learning'\")\n",
    "    \n",
    "    try:\n",
    "        ann_search_query = {\n",
    "            \"query\": {\n",
    "                \"neural_sparse\": {\n",
    "                    \"sparse_embedding\": {\n",
    "                        \"query_text\": \"machine learning artificial intelligence deep learning\",\n",
    "                        \"model_id\": model_id,\n",
    "                        \"method_parameters\": {\n",
    "                            \"k\": 5,          # Number of top results to return\n",
    "                            \"top_n\": 10,     # Top query tokens to retain\n",
    "                            \"heap_factor\": 1.0  # Recall vs performance trade-off\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"id\", \"text\", \"category\"]\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=index_name, body=ann_search_query)\n",
    "        \n",
    "        print(f\"\\nüìä ANN Search Results ({response['hits']['total']['value']} matches):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, hit in enumerate(response['hits']['hits'], 1):\n",
    "            print(f\"\\n{i}. Relevance Score: {hit['_score']:.4f}\")\n",
    "            print(f\"   Document ID: {hit['_source']['id']}\")\n",
    "            print(f\"   Category: {hit['_source']['category']}\")\n",
    "            print(f\"   Content: {hit['_source']['text'][:80]}...\")\n",
    "        \n",
    "        print(\"\\nüí° Key ANN Query Features Used:\")\n",
    "        print(\"   ‚Ä¢ method_parameters.k=5 - Returns top 5 documents\")\n",
    "        print(\"   ‚Ä¢ method_parameters.top_n=10 - Uses top 10 query tokens\")\n",
    "        print(\"   ‚Ä¢ method_parameters.heap_factor=1.0 - Exact recall mode\")\n",
    "        print(\"   ‚Ä¢ Query text automatically tokenized by sparse encoding model\")\n",
    "        print(\"   ‚Ä¢ SEISMIC algorithm for clustered inverted index search\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing ANN search: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping ANN search example - model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2cc6f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ ANN SEARCH QUERY EXAMPLES WITH DIFFERENT PARAMETERS\n",
      "================================================================================\n",
      "\n",
      "üìå Example 1: Varying 'k' - Number of Results to Return\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Query with k=3 (return top 3 results):\n",
      "   Results returned: 3\n",
      "      1. ann_009 (score: 4.5111)\n",
      "      2. ann_014 (score: 3.4370)\n",
      "      3. ann_011 (score: 2.4365)\n",
      "\n",
      "üîç Query with k=5 (return top 5 results):\n",
      "   Results returned: 5\n",
      "      1. ann_009 (score: 4.5111)\n",
      "      2. ann_014 (score: 3.4370)\n",
      "      3. ann_011 (score: 2.4365)\n",
      "      4. ann_012 (score: 1.1346)\n",
      "      5. ann_010 (score: 0.9982)\n",
      "\n",
      "üîç Query with k=10 (return top 10 results):\n",
      "   Results returned: 10\n",
      "      1. ann_009 (score: 4.5111)\n",
      "      2. ann_014 (score: 3.4370)\n",
      "      3. ann_011 (score: 2.4365)\n",
      "      4. ann_012 (score: 1.1346)\n",
      "      5. ann_010 (score: 0.9982)\n",
      "      6. ann_013 (score: 0.9100)\n",
      "      7. ann_015 (score: 0.7955)\n",
      "      8. ann_001 (score: 0.5540)\n",
      "      9. ann_027 (score: 0.2705)\n",
      "      10. ann_018 (score: 0.1539)\n",
      "\n",
      "\n",
      "üìå Example 2: Varying 'top_n' - Number of Query Tokens to Retain\n",
      "--------------------------------------------------------------------------------\n",
      "   top_n controls how many highest-weighted query tokens are used for search\n",
      "\n",
      "üîç Query with top_n=5 tokens:\n",
      "   Matched 29 documents:\n",
      "      1. ann_017 - Finance (score: 4.5799)\n",
      "      2. ann_005 - Technology (score: 4.5783)\n",
      "      3. ann_019 - Finance (score: 3.4978)\n",
      "      4. ann_018 - Finance (score: 1.7748)\n",
      "      5. ann_020 - Finance (score: 1.7230)\n",
      "      6. ann_008 - Technology (score: 1.4398)\n",
      "      7. ann_016 - Finance (score: 1.3278)\n",
      "      8. ann_022 - Finance (score: 1.1576)\n",
      "      9. ann_004 - Technology (score: 1.0472)\n",
      "      10. ann_021 - Finance (score: 1.0082)\n",
      "\n",
      "üîç Query with top_n=10 tokens:\n",
      "   Matched 29 documents:\n",
      "      1. ann_017 - Finance (score: 4.5799)\n",
      "      2. ann_005 - Technology (score: 4.5783)\n",
      "      3. ann_019 - Finance (score: 3.4978)\n",
      "      4. ann_018 - Finance (score: 1.7748)\n",
      "      5. ann_020 - Finance (score: 1.7230)\n",
      "      6. ann_008 - Technology (score: 1.4398)\n",
      "      7. ann_016 - Finance (score: 1.3278)\n",
      "      8. ann_022 - Finance (score: 1.1576)\n",
      "      9. ann_004 - Technology (score: 1.0472)\n",
      "      10. ann_021 - Finance (score: 1.0082)\n",
      "\n",
      "üîç Query with top_n=15 tokens:\n",
      "   Matched 29 documents:\n",
      "      1. ann_017 - Finance (score: 4.5799)\n",
      "      2. ann_005 - Technology (score: 4.5783)\n",
      "      3. ann_019 - Finance (score: 3.4978)\n",
      "      4. ann_018 - Finance (score: 1.7748)\n",
      "      5. ann_020 - Finance (score: 1.7230)\n",
      "      6. ann_008 - Technology (score: 1.4398)\n",
      "      7. ann_016 - Finance (score: 1.3278)\n",
      "      8. ann_022 - Finance (score: 1.1576)\n",
      "      9. ann_004 - Technology (score: 1.0472)\n",
      "      10. ann_021 - Finance (score: 1.0082)\n",
      "\n",
      "\n",
      "üìå Example 3: Varying 'heap_factor' - Recall vs Performance Trade-off\n",
      "--------------------------------------------------------------------------------\n",
      "   heap_factor: Controls the balance between search quality and speed\n",
      "   ‚Ä¢ 1.0 = Exact recall, slower (100% accuracy)\n",
      "   ‚Ä¢ 1.2 = High recall, slightly faster (98-99% accuracy)\n",
      "   ‚Ä¢ 1.5 = Good recall, balanced (95-98% accuracy)\n",
      "   ‚Ä¢ 2.0 = Acceptable recall, much faster (90-95% accuracy)\n",
      "\n",
      "üîç Same Query with Different heap_factor Values:\n",
      "\n",
      "   heap_factor: 1.0\n",
      "      Total hits: 12\n",
      "      1. ann_024 (score: 5.4758)\n",
      "      2. ann_029 (score: 0.7594)\n",
      "      3. ann_026 (score: 0.7508)\n",
      "\n",
      "   heap_factor: 1.2\n",
      "      Total hits: 12\n",
      "      1. ann_024 (score: 5.4758)\n",
      "      2. ann_029 (score: 0.7594)\n",
      "      3. ann_026 (score: 0.7508)\n",
      "\n",
      "   heap_factor: 1.5\n",
      "      Total hits: 12\n",
      "      1. ann_024 (score: 5.4758)\n",
      "      2. ann_029 (score: 0.7594)\n",
      "      3. ann_026 (score: 0.7508)\n",
      "\n",
      "   heap_factor: 2.0\n",
      "      Total hits: 12\n",
      "      1. ann_024 (score: 5.4758)\n",
      "      2. ann_029 (score: 0.7594)\n",
      "      3. ann_026 (score: 0.7508)\n",
      "\n",
      "\n",
      "üìå Example 4: Production Query with Filtering\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Production Query Configuration:\n",
      "   - k=10 (return top 10 results)\n",
      "   - top_n=15 (use top 15 query tokens)\n",
      "   - heap_factor=1.2 (balanced recall/speed)\n",
      "   - Category filter for 'Technology' documents\n",
      "\n",
      "   ‚úÖ Query Results: 8 documents returned\n",
      "   Total matches: 8\n",
      "\n",
      "   Top Results:\n",
      "      1. ann_001 - Score: 5.4062\n",
      "         Artificial intelligence systems using deep learning neural networks de...\n",
      "      2. ann_003 - Score: 1.8480\n",
      "         Machine learning algorithms require extensive training data preprocess...\n",
      "      3. ann_002 - Score: 1.2447\n",
      "         Natural language processing models leverage transformer architecture w...\n",
      "      4. ann_006 - Score: 0.6126\n",
      "         Quantum computing leverages quantum mechanics principles including sup...\n",
      "      5. ann_004 - Score: 0.5579\n",
      "         Cloud computing platforms provide scalable infrastructure services inc...\n",
      "\n",
      "\n",
      "üí° Parameter Guidelines (per OpenSearch docs):\n",
      "   ‚Ä¢ k: Number of top results to return (typically 5-100)\n",
      "   ‚Ä¢ top_n: Number of query tokens to retain (typically 5-50)\n",
      "   ‚Ä¢ heap_factor: Controls recall vs speed trade-off\n",
      "     - 1.0: Exact recall (slowest, highest accuracy)\n",
      "     - 1.2-1.5: Balanced (recommended for production)\n",
      "     - 2.0+: Fast approximate search (lower recall)\n",
      "   ‚Ä¢ Use filters for pre-filtering or post-filtering scenarios\n"
     ]
    }
   ],
   "source": [
    "if model_id:\n",
    "    print(\"\\nüéØ ANN SEARCH QUERY EXAMPLES WITH DIFFERENT PARAMETERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Example 1: Using k parameter (number of results)\n",
    "    print(\"\\nüìå Example 1: Varying 'k' - Number of Results to Return\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for k_value in [3, 5, 10]:\n",
    "        print(f\"\\nüîç Query with k={k_value} (return top {k_value} results):\")\n",
    "        try:\n",
    "            query_k = {\n",
    "                \"size\": k_value,  # limit returned hits; default is 10 if omitted\n",
    "                \"query\": {\n",
    "                    \"neural_sparse\": {\n",
    "                        \"sparse_embedding\": {\n",
    "                            \"query_text\": \"healthcare medical diagnostic imaging\",\n",
    "                            \"model_id\": model_id,\n",
    "                            \"method_parameters\": {\n",
    "                                \"k\": k_value,\n",
    "                                \"top_n\": 8,\n",
    "                                \"heap_factor\": 1.0\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"_source\": [\"id\", \"text\", \"category\"]\n",
    "            }\n",
    "            \n",
    "            response = client.search(index=index_name, body=query_k)\n",
    "            print(f\"   Results returned: {len(response['hits']['hits'])}\")\n",
    "            for i, hit in enumerate(response['hits']['hits'], 1):\n",
    "                print(f\"      {i}. {hit['_source']['id']} (score: {hit['_score']:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Example 2: Using top_n parameter (query tokens)\n",
    "    print(\"\\n\\nüìå Example 2: Varying 'top_n' - Number of Query Tokens to Retain\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"   top_n controls how many highest-weighted query tokens are used for search\")\n",
    "    \n",
    "    query_text = \"blockchain cryptocurrency decentralized financial technology systems\"\n",
    "    \n",
    "    for top_n_value in [5, 10, 15]:\n",
    "        print(f\"\\nüîç Query with top_n={top_n_value} tokens:\")\n",
    "        try:\n",
    "            query_topn = {\n",
    "                \"query\": {\n",
    "                    \"neural_sparse\": {\n",
    "                        \"sparse_embedding\": {\n",
    "                            \"query_text\": query_text,\n",
    "                            \"model_id\": model_id,\n",
    "                            \"method_parameters\": {\n",
    "                                \"k\": 3,\n",
    "                                \"top_n\": top_n_value,\n",
    "                                \"heap_factor\": 1.0\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"_source\": [\"id\", \"category\"]\n",
    "            }\n",
    "            \n",
    "            response = client.search(index=index_name, body=query_topn)\n",
    "            print(f\"   Matched {response['hits']['total']['value']} documents:\")\n",
    "            for i, hit in enumerate(response['hits']['hits'], 1):\n",
    "                print(f\"      {i}. {hit['_source']['id']} - {hit['_source']['category']} (score: {hit['_score']:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Example 3: Using heap_factor for recall/performance trade-off\n",
    "    print(\"\\n\\nüìå Example 3: Varying 'heap_factor' - Recall vs Performance Trade-off\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"   heap_factor: Controls the balance between search quality and speed\")\n",
    "    print(\"   ‚Ä¢ 1.0 = Exact recall, slower (100% accuracy)\")\n",
    "    print(\"   ‚Ä¢ 1.2 = High recall, slightly faster (98-99% accuracy)\")\n",
    "    print(\"   ‚Ä¢ 1.5 = Good recall, balanced (95-98% accuracy)\")\n",
    "    print(\"   ‚Ä¢ 2.0 = Acceptable recall, much faster (90-95% accuracy)\")\n",
    "    \n",
    "    print(\"\\nüîç Same Query with Different heap_factor Values:\")\n",
    "    query_text = \"renewable energy solar wind hydroelectric power generation\"\n",
    "    \n",
    "    for heap_factor_value in [1.0, 1.2, 1.5, 2.0]:\n",
    "        print(f\"\\n   heap_factor: {heap_factor_value}\")\n",
    "        try:\n",
    "            query_heap = {\n",
    "                \"query\": {\n",
    "                    \"neural_sparse\": {\n",
    "                        \"sparse_embedding\": {\n",
    "                            \"query_text\": query_text,\n",
    "                            \"model_id\": model_id,\n",
    "                            \"method_parameters\": {\n",
    "                                \"k\": 5,\n",
    "                                \"top_n\": 10,\n",
    "                                \"heap_factor\": heap_factor_value\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"_source\": [\"id\", \"text\"]\n",
    "            }\n",
    "            \n",
    "            response = client.search(index=index_name, body=query_heap)\n",
    "            print(f\"      Total hits: {response['hits']['total']['value']}\")\n",
    "            for i, hit in enumerate(response['hits']['hits'][:3], 1):\n",
    "                print(f\"      {i}. {hit['_source']['id']} (score: {hit['_score']:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Example 4: Combined parameters with filtering\n",
    "    print(\"\\n\\nüìå Example 4: Production Query with Filtering\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nüîç Production Query Configuration:\")\n",
    "    print(\"   - k=10 (return top 10 results)\")\n",
    "    print(\"   - top_n=15 (use top 15 query tokens)\")\n",
    "    print(\"   - heap_factor=1.2 (balanced recall/speed)\")\n",
    "    print(\"   - Category filter for 'Technology' documents\")\n",
    "    \n",
    "    try:\n",
    "        production_query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"neural_sparse\": {\n",
    "                                \"sparse_embedding\": {\n",
    "                                    \"query_text\": \"artificial intelligence neural networks deep learning models\",\n",
    "                                    \"model_id\": model_id,\n",
    "                                    \"method_parameters\": {\n",
    "                                        \"k\": 10,\n",
    "                                        \"top_n\": 15,\n",
    "                                        \"heap_factor\": 1.2\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"filter\": [\n",
    "                        {\"term\": {\"category\": \"Technology\"}}\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"id\", \"text\", \"category\"]\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=index_name, body=production_query)\n",
    "        print(f\"\\n   ‚úÖ Query Results: {len(response['hits']['hits'])} documents returned\")\n",
    "        print(f\"   Total matches: {response['hits']['total']['value']}\")\n",
    "        print(f\"\\n   Top Results:\")\n",
    "        for i, hit in enumerate(response['hits']['hits'][:5], 1):\n",
    "            print(f\"      {i}. {hit['_source']['id']} - Score: {hit['_score']:.4f}\")\n",
    "            print(f\"         {hit['_source']['text'][:70]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\\nüí° Parameter Guidelines (per OpenSearch docs):\")\n",
    "    print(\"   ‚Ä¢ k: Number of top results to return (typically 5-100)\")\n",
    "    print(\"   ‚Ä¢ top_n: Number of query tokens to retain (typically 5-50)\")\n",
    "    print(\"   ‚Ä¢ heap_factor: Controls recall vs speed trade-off\")\n",
    "    print(\"     - 1.0: Exact recall (slowest, highest accuracy)\")\n",
    "    print(\"     - 1.2-1.5: Balanced (recommended for production)\")\n",
    "    print(\"     - 2.0+: Fast approximate search (lower recall)\")\n",
    "    print(\"   ‚Ä¢ Use filters for pre-filtering or post-filtering scenarios\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping parameter examples - model not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b874250",
   "metadata": {},
   "source": [
    "## Step 10: Best Practices & Optimization Tips\n",
    "\n",
    "### Recommendations for Production Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c969fe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ BEST PRACTICES FOR NEURAL SPARSE SEARCH\n",
      "================================================================================\n",
      "       Category                                            Best Practice                         Impact\n",
      "Model Selection      Use doc-only mode for most use cases (best balance)          ‚ö° 2-3x faster queries\n",
      "Model Selection Use bi-encoder mode only for high-relevance requirements üìä +5-10% relevance improvement\n",
      "      Ingestion            Apply text chunking for documents >512 tokens       üíæ Prevents memory issues\n",
      "      Ingestion   Configure pruning ratio (0.1-0.3) to reduce index size         üíæ 30-40% space savings\n",
      "      Ingestion  Use ingest pipelines for automatic embedding generation          üîÑ Simplified workflow\n",
      "       Indexing         Set appropriate number_of_shards for parallelism       ‚ö° Better parallelization\n",
      "       Indexing       Exclude embeddings from _source to save disk space          üíæ 50-60% disk savings\n",
      "         Search  Always specify compatible analyzer for doc-only queries      ‚úÖ Consistent tokenization\n",
      "         Search         Combine with filters for better relevance tuning         üéØ More precise results\n",
      "    Maintenance            Monitor model deployment and update as needed         üìà Improved performance\n",
      "\n",
      "\n",
      "üí° Pro Tips:\n",
      "  1. Always test with your actual data before production deployment\n",
      "  2. Monitor query latency and adjust parameters based on SLA requirements\n",
      "  3. Use hybrid queries combining sparse + dense search for optimal results\n",
      "  4. Implement result caching for frequently searched queries\n",
      "  5. Set up alerts for model deployment failures\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ BEST PRACTICES FOR NEURAL SPARSE SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_practices = {\n",
    "    \"Category\": [\n",
    "        \"Model Selection\",\n",
    "        \"Model Selection\",\n",
    "        \"Ingestion\",\n",
    "        \"Ingestion\",\n",
    "        \"Ingestion\",\n",
    "        \"Indexing\",\n",
    "        \"Indexing\",\n",
    "        \"Search\",\n",
    "        \"Search\",\n",
    "        \"Maintenance\"\n",
    "    ],\n",
    "    \"Best Practice\": [\n",
    "        \"Use doc-only mode for most use cases (best balance)\",\n",
    "        \"Use bi-encoder mode only for high-relevance requirements\",\n",
    "        \"Apply text chunking for documents >512 tokens\",\n",
    "        \"Configure pruning ratio (0.1-0.3) to reduce index size\",\n",
    "        \"Use ingest pipelines for automatic embedding generation\",\n",
    "        \"Set appropriate number_of_shards for parallelism\",\n",
    "        \"Exclude embeddings from _source to save disk space\",\n",
    "        \"Always specify compatible analyzer for doc-only queries\",\n",
    "        \"Combine with filters for better relevance tuning\",\n",
    "        \"Monitor model deployment and update as needed\"\n",
    "    ],\n",
    "    \"Impact\": [\n",
    "        \"‚ö° 2-3x faster queries\",\n",
    "        \"üìä +5-10% relevance improvement\",\n",
    "        \"üíæ Prevents memory issues\",\n",
    "        \"üíæ 30-40% space savings\",\n",
    "        \"üîÑ Simplified workflow\",\n",
    "        \"‚ö° Better parallelization\",\n",
    "        \"üíæ 50-60% disk savings\",\n",
    "        \"‚úÖ Consistent tokenization\",\n",
    "        \"üéØ More precise results\",\n",
    "        \"üìà Improved performance\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "practices_df = pd.DataFrame(best_practices)\n",
    "print(practices_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüí° Pro Tips:\")\n",
    "print(\"  1. Always test with your actual data before production deployment\")\n",
    "print(\"  2. Monitor query latency and adjust parameters based on SLA requirements\")\n",
    "print(\"  3. Use hybrid queries combining sparse + dense search for optimal results\")\n",
    "print(\"  4. Implement result caching for frequently searched queries\")\n",
    "print(\"  5. Set up alerts for model deployment failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ace56",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Covered\n",
    "1. **Neural Sparse Search Fundamentals** - How sparse embeddings work\n",
    "2. **Model Registration & Deployment** - Setting up sparse encoding models\n",
    "3. **Ingest Pipeline Configuration** - Automatic embedding generation\n",
    "4. **Rank Features Index** - Efficient sparse vector storage\n",
    "5. **Real-World Use Cases**:\n",
    "   - E-Commerce Product Search\n",
    "   - Customer Support Ticket Search\n",
    "   - Document Repository Search\n",
    "6. **Performance Optimization** - Neural Sparse ANN for large-scale search\n",
    "7. **Best Practices** - Production deployment recommendations\n",
    "\n",
    "### Key Takeaways\n",
    "‚úÖ Neural Sparse Search achieves the best balance between:\n",
    "- **Speed** (2-10x faster than dense embeddings)\n",
    "- **Relevance** (comparable to semantic search)\n",
    "- **Scalability** (handles millions of documents efficiently)\n",
    "- **Memory** (low footprint compared to dense methods)\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different sparse encoding models for your domain\n",
    "- Combine with hybrid queries for further relevance improvements\n",
    "- Deploy ANN search mode for production at scale\n",
    "- Monitor and tune based on your specific requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Neural Sparse Search Tutorial Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìö Resources:\")\n",
    "print(\"  ‚Ä¢ OpenSearch Docs: https://docs.opensearch.org/latest/vector-search/ai-search/\")\n",
    "print(\"  ‚Ä¢ Model Registry: https://docs.opensearch.org/latest/ml-commons-plugin/\")\n",
    "print(\"  ‚Ä¢ Performance Tuning: https://docs.opensearch.org/latest/vector-search/performance-tuning-sparse/\")\n",
    "print(\"\\nüéì Happy Learning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-search-with-opensearch-intermediate (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
