# RAG Conversational Flow Agent with Memory

## Overview
This script implements a complete **Retrieval-Augmented Generation (RAG)** conversational agent with memory capabilities using OpenSearch and OpenAI. The agent maintains context across conversations and provides data-driven responses based on vector similarity search.

---

## Architecture Flow

```mermaid
graph TD
    A["ğŸŒ Initialize OpenSearch Client"] -->|Configure Credentials| B["âš™ï¸ Setup Cluster Settings"]
    B -->|Configure ML Commons| C["ğŸ“¦ Register Embedding Model"]
    
    C -->|Deploy Model| D["â³ Monitor Task Status"]
    D -->|Model Ready| E["ğŸ”Œ Create Ingest Pipeline"]
    E -->|Configure Embedding| F["ğŸ“‡ Create Vector Index"]
    
    F -->|Index Created| G["ğŸ“Š Load Sample Data"]
    G -->|Bulk Insert| H["âœ… Data Indexed with Embeddings"]
    
    H -->|Setup LLM| I["ğŸ¤ Setup OpenAI Connector"]
    I -->|Register Connector| J["ğŸ”‘ Register OpenAI Model"]
    
    J -->|Deploy Model| K["ğŸ§ª Test OpenAI Model"]
    K -->|Test Complete| L["ğŸ¤– Create RAG Agent"]
    
    L -->|Register Agent| M["ğŸ’¬ Agent with Memory Enabled"]
    M -->|Execute Query| N["ğŸ” Vector Search for Relevant Docs"]
    N -->|Retrieve Context| O["ğŸ§  Generate Response with GPT-3.5"]
    
    O -->|Return Answer| P["ğŸ“ Store in Conversation Memory"]
    P -->|Follow-up Query| Q["ğŸ”„ Continue with Context Preserved"]
    
    Q -->|Explore Memory| R["ğŸ“š View Conversation History"]
    R -->|Traces| S["ğŸ” Analyze Agent Traces & Details"]
    
    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
    style F fill:#03a9f4
    style G fill:#039be5
    style H fill:#0288d1
    style I fill:#e1bee7
    style J fill:#ce93d8
    style K fill:#ba68c8
    style L fill:#ab47bc
    style M fill:#9c27b0
    style N fill:#ffe082
    style O fill:#ffd54f
    style P fill:#ffca28
    style Q fill:#fbc02d
    style R fill:#f57f17
    style S fill:#ff6f00
```

---

## Detailed Component Breakdown

### 1. **Cluster Configuration**
```mermaid
sequenceDiagram
    participant Script
    participant OpenSearch
    
    Script ->> OpenSearch: Put Cluster Settings
    activate OpenSearch
    OpenSearch ->> OpenSearch: plugins.ml_commons.only_run_on_ml_node = false
    OpenSearch ->> OpenSearch: plugins.ml_commons.memory_feature_enabled = true
    deactivate OpenSearch
    OpenSearch -->> Script: Settings Configured
```

### 2. **Model Registration & Deployment Pipeline**
```mermaid
graph LR
    A["Register Embedding Model"] -->|Task ID| B["Monitor Status"]
    B -->|COMPLETED| C["Extract Model ID"]
    C -->|Ready| D["Deploy Model"]
    D -->|Deploy Task| E["Monitor Deployment"]
    E -->|COMPLETED| F["âœ… Model Deployed"]
    
    style A fill:#4fc3f7
    style B fill:#29b6f6
    style C fill:#03a9f4
    style D fill:#039be5
    style E fill:#0288d1
    style F fill:#01579b,color:#fff
```

### 3. **Data Ingestion Pipeline**
```mermaid
graph TD
    A["Create Ingest Pipeline"] -->|Add Processor| B["Text Embedding Processor"]
    B -->|Maps text field| C["embedding field"]
    
    D["Create Index"] -->|Configure Mappings| E["text field: text"]
    D -->|Configure Mappings| F["embedding field: knn_vector"]
    D -->|Configure Settings| G["Set default_pipeline"]
    
    H["Population Data"] -->|Sample Docs| I["Bulk Insert"]
    I -->|Via Ingest Pipeline| J["Auto-embed on Index"]
    J -->|384-dim vectors| K["âœ… Indexed Data with Embeddings"]
    
    style A fill:#81d4fa
    style B fill:#4fc3f7
    style C fill:#29b6f6
    style D fill:#29b6f6
    style E fill:#0288d1
    style F fill:#0288d1
    style G fill:#0277bd
    style K fill:#01579b,color:#fff
```

### 4. **LLM Integration**
```mermaid
graph TD
    A["Register Model Group"] -->|Generate ID| B["Model Group Created"]
    B -->|Model Group ID| C["Create Connector"]
    
    C -->|Connector Config| D["OpenAI Endpoint"]
    D -->|Bearer Token| E["Setup Auth"]
    E -->|Credentials| F["Register OpenAI Model"]
    
    F -->|Deploy=true| G["Monitor Deployment"]
    G -->|COMPLETED| H["âœ… LLM Ready"]
    
    I["Test Model"] -->|Sample Prompt| J["Send to OpenAI API"]
    J -->|Response| K["Verify Integration"]
    
    style A fill:#ce93d8
    style B fill:#ba68c8
    style C fill:#ab47bc
    style D fill:#9c27b0
    style E fill:#8e24aa
    style F fill:#7b1fa2
    style G fill:#6a1b9a
    style H fill:#4a148c,color:#fff
```

### 5. **RAG Agent Creation**
```mermaid
graph TD
    A["Agent Configuration"] -->|Type| B["conversational_flow"]
    A -->|App Type| C["rag"]
    A -->|Memory| D["conversation_index"]
    
    E["VectorDBTool"] -->|Config| F["model_id: Embedding Model"]
    E -->|Config| G["index: my_test_data"]
    E -->|Config| H["search_fields: embedding"]
    
    I["MLModelTool"] -->|Config| J["model_id: OpenAI GPT-3.5"]
    I -->|Config| K["System Prompt: Data Analyst"]
    I -->|Config| L["Context from Vector Search"]
    
    E -->|Tool 1| M["Register Agent"]
    I -->|Tool 2| M
    M -->|Register| N["âœ… Agent Registered"]
    
    style A fill:#ffca28
    style B fill:#fbc02d
    style C fill:#f57f17
    style D fill:#ff6f00
    style E fill:#ffe082
    style I fill:#ffd54f
    style N fill:#e65100,color:#fff
```

### 6. **Conversation Flow with Memory**
```mermaid
graph TD
    A["User Query 1"] -->|Text| B["Execute Agent"]
    B -->|Vector Search| C["Find Similar Docs"]
    C -->|Top Results| D["Extract Context"]
    
    D -->|Context + Query| E["Send to LLM"]
    E -->|Generate Response| F["Get Answer"]
    F -->|Memory ID| G["Store in Conversation Index"]
    
    G -->|memory_id| H["User Query 2 - Follow-up"]
    H -->|With memory_id| I["Execute Agent"]
    I -->|Load Previous Context| J["Conversation History"]
    J -->|Vector Search| K["Find New Docs"]
    K -->|New Context + History| L["Send to LLM"]
    L -->|Generate Response| M["Get Answer with Context"]
    
    M -->|Store| N["Update Conversation Index"]
    
    style A fill:#c8e6c9
    style B fill:#a5d6a7
    style C fill:#81c784
    style D fill:#66bb6a
    style E fill:#4caf50
    style F fill:#388e3c
    style G fill:#2e7d32
    style H fill:#81d4fa
    style I fill:#4fc3f7
    style J fill:#29b6f6
    style K fill:#1976d2
    style L fill:#1565c0
    style M fill:#0d47a1,color:#fff
    style N fill:#1565c0
```

### 7. **Memory Exploration**
```mermaid
graph LR
    A["Conversation Memory"] -->|memory_id| B["Get Memory Details"]
    B -->|Inspect| C["Get All Messages"]
    C -->|Per Message| D["Get Message Details"]
    
    E["Message Traces"] -->|message_id| F["Get Trace Info"]
    F -->|Analyze| G["Tool Execution Details"]
    G -->|Steps| H["Document Search Results"]
    
    style A fill:#ffccbc
    style B fill:#ffab91
    style C fill:#ff8a65
    style D fill:#ff7043
    style E fill:#ffccbc
    style F fill:#ffab91
    style G fill:#ff8a65
    style H fill:#d84315,color:#fff
```

---

## Key Features

### ğŸ¯ **Core Capabilities**
- **Vector Search**: Uses sentence-transformers for semantic search with 384-dimensional embeddings
- **LLM Integration**: Connects to OpenAI GPT-3.5-turbo for response generation
- **Conversation Memory**: Maintains context across multiple turns using conversation indices
- **Ingest Pipeline**: Automatically embeds documents on ingestion
- **Multi-turn Dialogue**: Supports follow-up questions with preserved context

### ğŸ”„ **Main Functions**
1. `setup_cluster_settings()` - Configures OpenSearch for ML operations
2. `register_embedding_model()` - Registers and deploys HuggingFace model
3. `create_ingest_pipeline()` - Sets up automatic embedding on data ingestion
4. `create_vector_index()` - Creates KNN index with vector field
5. `load_sample_data()` - Inserts population data with auto-embedding
6. `setup_openai_connector()` - Registers OpenAI model with credentials
7. `create_rag_agent()` - Creates agent with vector search and LLM tools
8. `test_conversation()` - Tests multi-turn conversation with memory
9. `explore_memory()` - Retrieves and analyzes conversation history

---

## Data Flow Summary

```
User Query
    â†“
Vector Search (Embedding Model)
    â†“
Retrieve Top K Documents
    â†“
Send to LLM with Context
    â†“
Generate Response
    â†“
Store in Conversation Memory
    â†“
(Optional) Load Memory for Follow-up
    â†“
Response to User
```

---

## Technologies Used
- ğŸ” **OpenSearch**: Vector database and agent orchestration
- ğŸ¤– **Sentence Transformers**: All-MiniLM-L12-v2 for embeddings (384-dim)
- ğŸ§  **OpenAI GPT-3.5-turbo**: Large Language Model for responses
- ğŸ’¾ **Conversation Index**: Memory storage for multi-turn dialogue
- ğŸ“Š **HuggingFace Models**: Pre-trained embedding model

---

## Configuration Parameters
- **Embedding Dimension**: 384 (all-MiniLM-L12-v2)
- **Vector Method**: HNSW (Hierarchical Navigable Small World)
- **Space Type**: L2 (Euclidean distance)
- **Engine**: Lucene
- **LLM Model**: gpt-3.5-turbo
- **Memory Type**: conversation_index
