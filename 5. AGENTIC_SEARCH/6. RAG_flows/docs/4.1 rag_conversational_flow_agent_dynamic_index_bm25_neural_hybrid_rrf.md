# RAG Conversational Agent with Dynamic Index & Hybrid Search + RRF (Reciprocal Rank Fusion)

## Overview
This script extends hybrid search with **Reciprocal Rank Fusion (RRF)**, an advanced ranking algorithm that combines rankings from multiple search methods. It demonstrates state-of-the-art information retrieval combining BM25, neural search, and RRF for optimal result ranking in a conversational RAG system.

---

## Architecture Flow

```mermaid
graph TD
    A["ğŸŒ Initialize OpenSearch"] -->|Setup| B["âš™ï¸ Configure Cluster"]
    B -->|ML Commons| C["ğŸ“¦ Register Embedding"]
    
    C -->|Deploy| D["ğŸ”Œ Ingest Pipeline"]
    D -->|Setup| E["ğŸ“‡ Create Index"]
    E -->|Bulk Load| F["ğŸ“Š Index Data"]
    
    F -->|Embeddings| G["âœ… Data Ready"]
    
    G -->|LLM Phase| H["ğŸ¤ Model Group"]
    H -->|Create| I["ğŸ”— OpenAI Connector"]
    I -->|Register| J["ğŸ”‘ OpenAI Model"]
    
    J -->|Deploy| K["ğŸ§ª Test LLM"]
    K -->|Create| L["ğŸ¤– Agent with<br/>SearchIndexTool"]
    
    L -->|Execute| M["User Query"]
    
    M -->|Route| N["SearchIndexTool"]
    N -->|Query Type| O{Search Method}
    
    O -->|BM25| P["Keyword Ranking"]
    O -->|Neural| Q["Vector Ranking"]
    O -->|RRF| R["Reciprocal Rank Fusion"]
    
    P -->|Rank 1, 2, 3...| S["BM25 Rankings"]
    Q -->|Rank 1, 2, 3...| T["Neural Rankings"]
    
    S -->|Combine| U["RRF Fusion"]
    T -->|Combine| U
    
    U -->|Formula| V["1/(k + rank)"]
    V -->|Normalize| W["Final RRF Scores"]
    
    R -->|Direct RRF| W
    
    W -->|Top K| X["Final Results"]
    X -->|Context| Y["MLModelTool"]
    Y -->|Generate| Z["LLM Response"]
    Z -->|Return| AA["âœ… Answer"]
    
    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
    style L fill:#9c27b0,color:#fff
    style P fill:#4fc3f7
    style Q fill:#0288d1
    style U fill:#ff6f00,color:#fff
    style V fill:#ff5722,color:#fff
    style W fill:#bf360c,color:#fff
    style Z fill:#8e24aa,color:#fff
    style AA fill:#6a1b9a,color:#fff
```

---

## Reciprocal Rank Fusion (RRF) Architecture

```mermaid
graph TD
    subgraph Input["ğŸ” Input"]
        Q["User Query"]
    end
    
    subgraph BM25["ğŸ“ BM25 Search"]
        B1["Execute Query"]
        B2["Get Results"]
        B3["Assign Ranks<br/>1,2,3..."]
    end
    
    subgraph Neural["ğŸ§  Neural Search"]
        N1["Encode Query"]
        N2["Vector Search"]
        N3["Assign Ranks<br/>1,2,3..."]
    end
    
    subgraph Fusion["âš™ï¸ RRF Fusion"]
        F1["Calculate RRF Score"]
        F2["Formula: 1/(k + rank)"]
        F3["k=60 default"]
        F4["Sum Scores"]
    end
    
    subgraph Output["âœ… Output"]
        O1["Final Rankings"]
        O2["Top Results"]
    end
    
    Q --> B1
    Q --> N1
    
    B1 --> B2
    N1 --> N2
    
    B2 --> B3
    N2 --> N3
    
    B3 -->|Ranks| F1
    N3 -->|Ranks| F1
    
    F1 --> F2
    F2 --> F3
    F3 --> F4
    
    F4 --> O1
    O1 --> O2
    
    style Input fill:#e1f5ff
    style B1 fill:#4fc3f7
    style B2 fill:#29b6f6
    style B3 fill:#0288d1
    style N1 fill:#0277bd
    style N2 fill:#01579b
    style N3 fill:#004d99,color:#fff
    style F1 fill:#ffca28
    style F2 fill:#fbc02d
    style F3 fill:#f57f17
    style F4 fill:#ff6f00
    style Output fill:#e65100,color:#fff
```

---

## RRF Algorithm Deep Dive

### RRF Scoring Formula
```
RRF_Score(document) = Î£ (1 / (k + rank))

Where:
  - k = constant (typically 60)
  - rank = document's rank in each ranking list
  - Î£ = sum across all ranking methods
```

### Ranking Combination Example
```mermaid
graph TD
    A["BM25 Rankings"] --> A1["Doc1: Rank 1"]
    A --> A2["Doc2: Rank 2"]
    A --> A3["Doc3: Rank 4"]
    
    B["Neural Rankings"] --> B1["Doc2: Rank 1"]
    B --> B2["Doc3: Rank 2"]
    B --> B3["Doc1: Rank 3"]
    
    A1 -->|Score: 1/61| C["RRF Score"]
    B3 -->|Score: 1/63| C
    
    A2 -->|Score: 1/62| D["RRF Score"]
    B1 -->|Score: 1/61| D
    
    A3 -->|Score: 1/64| E["RRF Score"]
    B2 -->|Score: 1/62| E
    
    C -->|Doc1: 1/61+1/63| F["Final Ranking"]
    D -->|Doc2: 1/62+1/61| F
    E -->|Doc3: 1/64+1/62| F
    
    F -->|Sorted| G["1. Doc2: 0.0339"]
    F -->|Sorted| H["2. Doc1: 0.0319"]
    F -->|Sorted| I["3. Doc3: 0.0287"]
    
    style A fill:#4fc3f7
    style B fill:#0288d1
    style C fill:#ffca28
    style D fill:#fbc02d
    style E fill:#f57f17
    style F fill:#ff6f00,color:#fff
    style G fill:#bf360c,color:#fff
    style H fill:#bf360c,color:#fff
    style I fill:#bf360c,color:#fff
```

---

## Detailed Component Flows

### 1. **Setup & Deployment Pipeline**
```mermaid
sequenceDiagram
    participant Script
    participant OpenSearch as OS Cluster
    participant ML as ML Commons
    
    Script ->> ML: Register Embedding Model
    activate ML
    ML ->> ML: Start registration task
    Note over ML: task_id assigned
    deactivate ML
    
    loop Every 5 seconds
        Script ->> ML: Check status
        ML -->> Script: Task state
    end
    
    ML -->> Script: COMPLETED â†’ model_id
    
    Script ->> ML: Deploy model
    activate ML
    ML ->> ML: Allocate resources
    deactivate ML
    
    loop Every 10 seconds
        Script ->> ML: Monitor deployment
        ML -->> Script: Progress
    end
    
    ML -->> Script: COMPLETED
    Script ->> OpenSearch: Create ingest pipeline
    OpenSearch -->> Script: Pipeline ready
```

### 2. **Data Ingestion with Vector Generation**
```mermaid
graph TD
    A["Sample Documents<br/>Population Data"] -->|Bulk Insert| B["Ingest API"]
    
    B -->|Apply Pipeline| C["Text Embedding Processor"]
    C -->|Input| D["text field"]
    C -->|Model| E["Embedding Model"]
    E -->|Generate| F["384-dim vector"]
    
    F -->|Map to| G["embedding field"]
    G -->|Index| H["KNN Index"]
    
    I["Ingest Success"] --> J["âœ… Documents Indexed<br/>with Vectors"]
    
    B -->|Update| I
    
    style A fill:#b3e5fc
    style B fill:#81d4fa
    style C fill:#4fc3f7
    style E fill:#0288d1
    style F fill:#0277bd,color:#fff
    style J fill:#01579b,color:#fff
```

### 3. **BM25 Ranking Generation**
```mermaid
graph TD
    A["Query: Seattle<br/>population increase"] -->|Tokenize| B["Terms"]
    B -->|Search| C["Lucene Index"]
    
    C -->|Calculate| D["TF-IDF Scores"]
    D -->|Per Document| E["Score1: 8.5"]
    D -->|Per Document| F["Score2: 6.2"]
    D -->|Per Document| G["Score3: 4.1"]
    
    E -->|Rank| H["Rank 1"]
    F -->|Rank| I["Rank 2"]
    G -->|Rank| J["Rank 3"]
    
    H -->|BM25 Output| K["Ranked List for RRF"]
    I -->|BM25 Output| K
    J -->|BM25 Output| K
    
    style A fill:#4fc3f7
    style C fill:#29b6f6
    style D fill:#0288d1
    style E fill:#0277bd
    style F fill:#0277bd
    style G fill:#01579b,color:#fff
    style K fill:#01579b,color:#fff
```

### 4. **Neural Ranking Generation**
```mermaid
graph TD
    A["Query: Seattle<br/>population increase"] -->|Encode| B["Query Vector"]
    B -->|384-dim| C["Vector Representation"]
    
    C -->|HNSW Search| D["Vector Index"]
    D -->|Similarity Calc| E["L2 Distance/Cosine"]
    
    E -->|Doc1| F["Similarity: 0.92"]
    E -->|Doc2| G["Similarity: 0.85"]
    E -->|Doc3| H["Similarity: 0.78"]
    
    F -->|Rank| I["Rank 1"]
    G -->|Rank| J["Rank 2"]
    H -->|Rank| K["Rank 3"]
    
    I -->|Neural Output| L["Ranked List for RRF"]
    J -->|Neural Output| L
    K -->|Neural Output| L
    
    style A fill:#0288d1
    style B fill:#0277bd
    style C fill:#01579b,color:#fff
    style D fill:#004d99,color:#fff
    style F fill:#ffca28
    style G fill:#fbc02d
    style H fill:#f57f17
    style L fill:#ff6f00,color:#fff
```

### 5. **RRF Fusion Algorithm**
```mermaid
graph TD
    BM["BM25 Ranks<br/>1:Doc1, 2:Doc2, 3:Doc3"] -->|k=60| F1["RRF Scores"]
    Neural["Neural Ranks<br/>1:Doc3, 2:Doc1, 3:Doc2"] -->|k=60| F1
    
    F1 -->|Doc1| C1["1/(60+1) + 1/(60+2)"]
    F1 -->|Doc2| C2["1/(60+2) + 1/(60+3)"]
    F1 -->|Doc3| C3["1/(60+3) + 1/(60+1)"]
    
    C1 -->|Calculate| S1["= 1/61 + 1/62"]
    C2 -->|Calculate| S2["= 1/62 + 1/63"]
    C3 -->|Calculate| S3["= 1/63 + 1/61"]
    
    S1 -->|Result| R1["0.03115"]
    S2 -->|Result| R2["0.02916"]
    S3 -->|Result| R3["0.03131"]
    
    R1 -->|Sort| Final["1. Doc3: 0.03131"]
    R2 -->|Sort| Final
    R3 -->|Sort| Final
    
    Final -->|Sort| F["2. Doc1: 0.03115<br/>3. Doc2: 0.02916"]
    
    style BM fill:#4fc3f7
    style Neural fill:#0288d1
    style F1 fill:#ffca28,color:#000
    style C1 fill:#fbc02d
    style C2 fill:#fbc02d
    style C3 fill:#f57f17
    style Final fill:#ff6f00,color:#fff
    style F fill:#bf360c,color:#fff
```

### 6. **Query Execution Path**
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant SearchTool
    participant BM25
    participant Neural
    participant RRF
    participant LLM
    
    User ->> Agent: Question
    Agent ->> SearchTool: Execute
    
    par BM25 Search
        SearchTool ->> BM25: Search
        BM25 -->> SearchTool: Ranked results
    and Neural Search
        SearchTool ->> Neural: Search
        Neural -->> SearchTool: Ranked results
    end
    
    SearchTool ->> RRF: Combine rankings
    RRF ->> RRF: Apply RRF formula
    RRF -->> SearchTool: Final results
    
    SearchTool -->> Agent: Combined results
    
    Agent ->> LLM: Context + Question
    LLM ->> LLM: Generate response
    LLM -->> Agent: Answer
    
    Agent -->> User: Response
```

### 7. **Multi-turn Conversation Flow**
```mermaid
graph TD
    A["Turn 1:<br/>First Question"] -->|Execute| B["Search with RRF"]
    B -->|Get Results| C["Generate Answer"]
    C -->|Store| D["Memory: Turn 1"]
    D -->|Response| E["User Gets Answer"]
    
    E -->|Turn 2:<br/>Follow-up| F["Load Memory: Turn 1"]
    F -->|Context| G["Search with RRF"]
    G -->|Get Results| H["Include Previous Context"]
    H -->|Generate| I["Informed Answer"]
    I -->|Store| J["Memory: Turn 2"]
    J -->|Response| K["User Gets Answer"]
    
    K -->|Turn 3...| L["Continue Pattern"]
    
    style A fill:#c8e6c9
    style B fill:#81d4fa
    style C fill:#ffca28
    style D fill:#90caf9,color:#000
    style E fill:#a5d6a7
    style F fill:#90caf9,color:#000
    style G fill:#81d4fa
    style H fill:#ffca28
    style I fill:#ffc107,color:#000
    style J fill:#90caf9,color:#000
    style K fill:#a5d6a7
```

---

## Key Features

### ğŸ¯ **Advanced Ranking Capabilities**
- **BM25 Ranking**: Traditional keyword matching with TF-IDF
- **Neural Ranking**: Semantic understanding with vector similarity
- **Reciprocal Rank Fusion**: Optimal combination of multiple rankings
- **Parallel Execution**: Simultaneous BM25 and neural searches
- **Dynamic Parameters**: k-value tunable for RRF formula
- **Multi-turn Memory**: Full conversation context preservation

### âš™ï¸ **RRF Configuration**
```
k (constant) = 60 (default)
Higher k â†’ gives more weight to lower-ranked documents
Lower k â†’ favors top-ranked documents

RRF Score = 1/(k + rank_bm25) + 1/(k + rank_neural)
```

---

## Comparison: Search Methods

| Method | Strength | Weakness | Use Case |
|--------|----------|----------|----------|
| **BM25** | Exact term matching | Semantic blindness | Factual queries |
| **Neural** | Semantic similarity | Computational cost | Complex queries |
| **RRF** | Balanced best results | Parameter tuning | Production systems |

---

## Data Processing Pipeline

```
Input: User Query
  â†“
[Parallel Execution]
â”œâ”€ BM25: Generate keyword-based rankings
â””â”€ Neural: Generate semantic-based rankings
  â†“
[RRF Fusion]
â”œâ”€ Calculate RRF scores for each document
â”œâ”€ Apply formula: 1/(k + rank)
â””â”€ Combine and normalize
  â†“
[Result Selection]
â”œâ”€ Sort by final RRF score
â””â”€ Extract top K documents
  â†“
[Context Preparation]
â”œâ”€ Format document content
â””â”€ Prepare for LLM
  â†“
[LLM Generation]
â”œâ”€ Send context + question
â”œâ”€ Generate response
â””â”€ Return answer
  â†“
[Memory Storage]
â””â”€ Save to conversation index
```

---

## Execution Statistics

### Performance Benefits of RRF
- **Recall Improvement**: Captures results missed by individual methods
- **Precision Enhancement**: Redundant high-ranking documents score higher
- **Diversity**: Balances keyword-exact and semantic-similar matches
- **Robustness**: Reduces sensitivity to any single ranking method

### Typical RRF Score Distribution
```
Doc1 (appears in both): ~0.031 (high)
Doc2 (BM25 only): ~0.020 (medium)
Doc3 (Neural only): ~0.019 (medium)
Doc4 (neither): 0.000 (excluded)
```

---

## Technologies Used
- ğŸ” **OpenSearch**: Search & ranking engine
- ğŸ“Š **BM25 Algorithm**: Probabilistic ranking
- ğŸ¤– **Sentence Transformers**: Neural embeddings
- ğŸ§  **OpenAI GPT-3.5-turbo**: Response generation
- âš™ï¸ **Reciprocal Rank Fusion**: Result fusion algorithm
- ğŸ’¾ **Conversation Index**: Memory management

---

## Unique Aspects
1. **RRF Algorithm**: State-of-the-art ranking fusion
2. **Parallel Searches**: Simultaneous BM25 + neural execution
3. **Formula-based Combination**: Mathematically principled fusion
4. **k-value Tuning**: Adjustable weighting for RRF
5. **Production Ready**: Balanced approach for real-world systems
6. **Full Memory**: Multi-turn conversations with RRF ranking

---

## Advanced Usage

### Adjusting RRF Weight (k-value)
```python
k = 60  # Higher k â†’ more balanced
k = 20  # Lower k â†’ favors top results
```

### Understanding Final Scores
```
High RRF Score (0.03+): Document appears high in both rankings
Medium Score (0.02): Document appears in one ranking well
Low Score (0.01-): Document low-ranked or missing in one
```
