# RAG Conversational Flow Agent with Multiple Knowledge Bases

## Overview
This script extends the RAG agent with **multiple knowledge base support**, allowing the agent to query multiple indices and combine insights from different data sources. It demonstrates how to use multiple `VectorDBTool` instances within a single agent for comprehensive data analysis.

---

## Architecture Flow

```mermaid
graph TD
    A["üåê Initialize OpenSearch Client"] -->|Connection| B["‚öôÔ∏è Setup Cluster Settings"]
    B -->|ML Commons Config| C["üì¶ Register Embedding Model"]
    
    C -->|Deploy| D["‚è≥ Monitor Registration"]
    D -->|Model ID| E["üîå Create Ingest Pipeline"]
    E -->|Configure| F["üìá Create Vector Index"]
    
    F -->|Index Setup| G["üìä Load Sample Data"]
    G -->|Population Data| H["‚úÖ Data with Embeddings"]
    
    H -->|Setup Phase| I["ü§ù Register Model Group"]
    I -->|Group ID| J["üîó Create OpenAI Connector"]
    J -->|Connector ID| K["üîë Register OpenAI Model"]
    
    K -->|Deploy| L["üß™ Test OpenAI Model"]
    L -->|Test Complete| M["ü§ñ Create Multi-KB Agent"]
    
    M -->|Tool Config| N["Tool 1: VectorDBTool - KB1"]
    M -->|Tool Config| O["Tool 2: VectorDBTool - KB2"]
    M -->|Tool Config| P["Tool 3: MLModelTool - LLM"]
    
    N -->|Query| Q["Execute Agent Query"]
    O -->|Query| Q
    P -->|Context| Q
    
    Q -->|KB1 Results| R["Combine Context from Multiple KBs"]
    Q -->|KB2 Results| R
    
    R -->|Combined Context| S["Send to LLM"]
    S -->|Generate Response| T["Return Answer"]
    
    T -->|Store| U["üìù Save to Conversation Memory"]
    U -->|Continue| V["üîÑ Follow-up Query with Context"]
    
    V -->|Selected Tools| W["Execute with Specific KB Selection"]
    W -->|Optional| X["Query Only Selected Knowledge Bases"]
    
    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#29b6f6
    style F fill:#03a9f4
    style G fill:#039be5
    style H fill:#0288d1
    style M fill:#9c27b0,color:#fff
    style N fill:#ab47bc
    style O fill:#ba68c8
    style P fill:#ce93d8
    style Q fill:#ffd54f
    style R fill:#ffca28
    style S fill:#fbc02d
    style T fill:#f57f17
    style U fill:#ff6f00
    style V fill:#ff5722
    style W fill:#e64a19
    style X fill:#bf360c,color:#fff
```

---

## Multi-Knowledge Base Architecture

```mermaid
graph TD
    Agent["ü§ñ RAG Agent"]
    
    VT1["VectorDBTool - Context1"] -->|Index| KB1["Knowledge Base 1<br/>my_test_data"]
    VT2["VectorDBTool - Context2"] -->|Index| KB2["Knowledge Base 2<br/>my_test_data"]
    
    KB1 -->|Query| S1["Search Results 1"]
    KB2 -->|Query| S2["Search Results 2"]
    
    MLT["MLModelTool<br/>OpenAI GPT-3.5"]
    
    S1 -->|Output| MLT
    S2 -->|Output| MLT
    
    Agent -->|Route| VT1
    Agent -->|Route| VT2
    Agent -->|Route| MLT
    
    MLT -->|Context1 + Context2| R["Generate Response"]
    R -->|Answer| Agent
    
    Agent -->|Optional| STool["Selected Tools Filter"]
    STool -->|context1 only| VT1
    STool -->|context2 only| VT2
    
    style Agent fill:#9c27b0,color:#fff
    style VT1 fill:#4fc3f7
    style VT2 fill:#29b6f6
    style KB1 fill:#0288d1
    style KB2 fill:#0288d1
    style MLT fill:#fbc02d
    style R fill:#f57f17
    style STool fill:#ff6f00,color:#fff
```

---

## Detailed Component Flows

### 1. **Setup Phase**
```mermaid
sequenceDiagram
    participant Script
    participant OpenSearch
    
    Script ->> OpenSearch: Register Embedding Model
    activate OpenSearch
    Note over OpenSearch: Register task started
    deactivate OpenSearch
    
    loop Every 5 seconds
        Script ->> OpenSearch: Check Task Status
        OpenSearch -->> Script: Status update
    end
    
    OpenSearch -->> Script: Model ID (task COMPLETED)
    Script ->> OpenSearch: Deploy Model
    OpenSearch -->> Script: Deploy task ID
    
    loop Every 10 seconds
        Script ->> OpenSearch: Check Deployment Status
        OpenSearch -->> Script: Deployment progress
    end
    
    OpenSearch -->> Script: Deployment COMPLETED
```

### 2. **Data Ingestion with Auto-Embedding**
```mermaid
graph LR
    A["Ingest Pipeline"] -->|Processor| B["Text Embedding"]
    B -->|Model ID| C["384-dim Embedding"]
    C -->|Field Map| D["text ‚Üí embedding"]
    
    E["Sample Docs"] -->|Bulk Insert| F["OpenSearch"]
    F -->|Pipeline| B
    B -->|Auto-embed| G["KNN Vector Index"]
    G -->|Indexed| H["‚úÖ Ready for Search"]
    
    style A fill:#81d4fa
    style B fill:#29b6f6
    style C fill:#0288d1
    style D fill:#0277bd
    style G fill:#01579b,color:#fff
    style H fill:#01579b,color:#fff
```

### 3. **OpenAI Connector & Model Registration**
```mermaid
graph TD
    A["Model Group"] -->|Create| B["Model Group ID"]
    B -->|Reference| C["Connector"]
    
    C -->|Config| D["Name: OpenAI Chat Connector"]
    C -->|Config| E["Endpoint: api.openai.com"]
    C -->|Config| F["Model: gpt-3.5-turbo"]
    C -->|Config| G["Auth: Bearer Token"]
    
    D -->|Connector ID| H["Register OpenAI Model"]
    E -->|Connector ID| H
    F -->|Connector ID| H
    G -->|Connector ID| H
    
    H -->|Deploy=true| I["Monitor Registration"]
    I -->|COMPLETED| J["‚úÖ OpenAI Model Ready"]
    
    style A fill:#ce93d8
    style B fill:#ba68c8
    style C fill:#ab47bc
    style J fill:#4a148c,color:#fff
```

### 4. **Multi-KB Agent Configuration**
```mermaid
graph TD
    A["Agent Registration"] -->|Type| B["conversational_flow"]
    A -->|App| C["rag"]
    A -->|Memory| D["conversation_index"]
    
    E["Tool 1: VectorDBTool"]
    E -->|name| E1["context1"]
    E -->|model_id| E2["Embedding Model"]
    E -->|index| E3["my_test_data"]
    
    F["Tool 2: VectorDBTool"]
    F -->|name| F1["context2"]
    F -->|model_id| F2["Embedding Model"]
    F -->|index| F3["my_test_data"]
    
    G["Tool 3: MLModelTool"]
    G -->|model_id| G1["OpenAI GPT-3.5"]
    G -->|context| G2["${'$'}{parameters.context1.output}"]
    G -->|context| G3["${'$'}{parameters.context2.output}"]
    G -->|prompt| G4["Data Analyst System Prompt"]
    
    A -->|Register| H["Multi-KB Agent"]
    E -->|Tool| H
    F -->|Tool| H
    G -->|Tool| H
    
    H -->|Ready| I["‚úÖ Agent with Multiple KBs"]
    
    style A fill:#ffca28
    style B fill:#fbc02d
    style C fill:#f57f17
    style D fill:#ff6f00
    style H fill:#e65100,color:#fff
    style I fill:#e65100,color:#fff
```

### 5. **Query Execution with Multiple KBs**
```mermaid
graph TD
    A["User Question"] -->|Input| B["Agent Execute"]
    
    B -->|Call Tool 1| C["VectorDBTool - context1"]
    B -->|Call Tool 2| D["VectorDBTool - context2"]
    
    C -->|Query| E["Search KB1"]
    D -->|Query| F["Search KB2"]
    
    E -->|Results| G["context1.output"]
    F -->|Results| H["context2.output"]
    
    G -->|Combine| I["Build Prompt"]
    H -->|Combine| I
    
    I -->|Pass to| J["MLModelTool"]
    J -->|System Prompt| K["You are a data analyst"]
    J -->|Context| L["Context1 + Context2"]
    J -->|Question| M["Original Question"]
    
    K -->|Request| N["OpenAI API"]
    L -->|Request| N
    M -->|Request| N
    
    N -->|Response| O["Generate Answer"]
    O -->|Return| P["User Response"]
    
    P -->|Store| Q["üìù Conversation Memory"]
    
    style A fill:#c8e6c9
    style B fill:#a5d6a7
    style C fill:#4fc3f7
    style D fill:#29b6f6
    style E fill:#0288d1
    style F fill:#0277bd
    style I fill:#ffca28
    style J fill:#fbc02d
    style N fill:#f57f17
    style O fill:#ff6f00
    style P fill:#ff5722
    style Q fill:#bf360c,color:#fff
```

### 6. **Memory & Continuation Flow**
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Memory as Conversation Memory
    participant Search as Vector Search
    participant LLM
    
    User ->> Agent: Question 1
    Agent ->> Search: Query KB1 & KB2
    Search -->> Agent: Results
    Agent ->> LLM: Combined Context + Q1
    LLM -->> Agent: Answer 1
    Agent ->> Memory: Store (memory_id, message_id)
    Agent -->> User: Answer 1
    
    User ->> Agent: Question 2 (Follow-up)<br/>with memory_id
    Agent ->> Memory: Load Previous Context
    Memory -->> Agent: Conversation History
    Agent ->> Search: Query KB1 & KB2
    Search -->> Agent: Results
    Agent ->> LLM: History + New Context + Q2
    LLM -->> Agent: Answer 2
    Agent ->> Memory: Update Conversation
    Agent -->> User: Answer 2
    
    User ->> Agent: Query with selected_tools<br/>["context1"]
    Agent ->> Search: Query Only KB1
    Search -->> Agent: KB1 Results
    Agent ->> LLM: KB1 Context + Question
    LLM -->> Agent: Answer
    Agent -->> User: Answer (KB1 Only)
```

---

## Key Features

### üéØ **Advanced Capabilities**
- **Multiple Knowledge Bases**: Query different indices simultaneously
- **Context Aggregation**: Combine results from multiple sources
- **Selective Tool Usage**: Execute only specific tools via `selected_tools` parameter
- **Conversation Memory**: Full multi-turn dialogue support
- **Dynamic Tool Selection**: Choose which KBs to query per request
- **Parallel Search**: Execute searches against multiple indices concurrently

### üîÑ **Query Execution Options**

**Option 1: Query All Knowledge Bases**
```json
{
  "parameters": {
    "question": "What's the population data for major cities?"
  }
}
```

**Option 2: Query Specific Knowledge Bases**
```json
{
  "parameters": {
    "question": "What's the population data for major cities?",
    "selected_tools": ["context1", "context2"]
  }
}
```

**Option 3: Continue Conversation with Memory**
```json
{
  "parameters": {
    "question": "Compare with another city"
  },
  "memory_id": "previous_memory_id"
}
```

---

## Main Processing Functions

| Function | Purpose |
|----------|---------|
| `register_embedding_model()` | Register HuggingFace embedding model with auto-deploy |
| `create_ingest_pipeline()` | Setup auto-embedding pipeline for documents |
| `load_sample_data()` | Insert population data with automatic vector creation |
| `setup_openai_connector()` | Configure OpenAI API connection and credentials |
| `agent_registration_response` | Create agent with multiple VectorDBTool instances |
| `memory_id extraction` | Extract conversation ID from first response |

---

## Data Flow

```
User Query
    ‚Üì
Agent Receives Question
    ‚Üì
‚îú‚îÄ VectorDBTool(context1) ‚Üí KB1 Search
‚îú‚îÄ VectorDBTool(context2) ‚Üí KB2 Search
‚îî‚îÄ Combine Results
    ‚Üì
MLModelTool Receives:
  - System Prompt (Data Analyst)
  - Context from KB1
  - Context from KB2
  - Original Question
    ‚Üì
OpenAI GPT-3.5 Generation
    ‚Üì
Response to User
    ‚Üì
Store in Conversation Memory
    ‚Üì
(Optional) Load Memory for Follow-ups
```

---

## Configuration Parameters

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Embedding Model** | all-MiniLM-L12-v2 | 384-dimensional semantic embeddings |
| **Vector Dimension** | 384 | HNSW index dimension |
| **Space Type** | L2 | Euclidean distance metric |
| **Engine** | Lucene | KNN search engine |
| **LLM Model** | gpt-3.5-turbo | Language generation |
| **Tools** | 3 (2 VectorDB + 1 LLM) | Multi-KB + LLM generation |
| **Memory Type** | conversation_index | Conversation storage |

---

## Technologies Stack
- üîç **OpenSearch**: Vector database with agent framework
- ü§ñ **Sentence Transformers**: Embedding generation
- üß† **OpenAI GPT-3.5**: Response generation
- üíæ **Conversation Index**: Memory management
- ‚öôÔ∏è **Ingest Pipeline**: Automatic embedding on index

---

## Unique Aspects
1. **Multiple KB Integration**: Single agent queries multiple knowledge bases
2. **Context Combination**: Merges insights from different sources
3. **Selective Querying**: Can limit to specific KBs via `selected_tools`
4. **Scalable Design**: Easy to add more VectorDBTools for additional KBs
5. **Full Conversation Context**: Maintains memory across turns with multiple sources
