# RAG Chatbot Conversational Agent

## Overview
This script implements a **conversational RAG chatbot** with advanced agent capabilities, supporting multiple knowledge bases (tech news and population data) and full conversation memory. It demonstrates an intelligent agent that routes queries to appropriate knowledge bases and maintains context across multi-turn conversations.

---

## Architecture Flow

```mermaid
graph TD
    A["ğŸŒ Initialize OpenSearch"] -->|Configure| B["âš™ï¸ Setup Cluster"]
    B -->|ML Commons| C["ğŸ“¦ Register Model"]
    
    C -->|Deploy| D["ğŸ”Œ Ingest Pipeline"]
    D -->|Embedding| E["Field: text<br/>Field: passage"]
    E -->|Setup| F["ğŸ“‡ Create Indices"]
    
    F -->|Index 1| F1["population_data_kb"]
    F -->|Index 2| F2["test_tech_news"]
    
    F1 -->|Bulk Data| G["Population Data"]
    F2 -->|Bulk Data| H["Tech News Data"]
    
    G -->|Auto-embed| I["âœ… Data Indexed"]
    H -->|Auto-embed| I
    
    I -->|LLM Phase| J["ğŸ¤ Model Group"]
    J -->|Create| K["ğŸ”— OpenAI Connector"]
    K -->|Register| L["ğŸ”‘ OpenAI Model"]
    
    L -->|Deploy| M["ğŸ§ª Test LLM"]
    M -->|Config| N["System Instruction<br/>Response Filter"]
    
    N -->|Create| O["ğŸ¤– Conversational Agent"]
    O -->|Type| P["agent_type: conversational"]
    O -->|App| Q["app_type: chat_with_rag"]
    O -->|Memory| R["Type: conversation_index"]
    
    R -->|Tool 1| S["VectorDBTool<br/>Population KB"]
    R -->|Tool 2| T["VectorDBTool<br/>Tech News KB"]
    
    S -->|Execute| U["Multi-turn<br/>Conversation"]
    T -->|Execute| U
    
    U -->|Query Type| V{Routing}
    V -->|About Populations| W["Use Population KB"]
    V -->|About Tech| X["Use Tech News KB"]
    V -->|General| Y["Use Both KBs"]
    
    W -->|Search| Z["Generate Response"]
    X -->|Search| Z
    Y -->|Search| Z
    
    Z -->|Answer| AA["Return to User"]
    AA -->|Memory| AB["Store Conversation"]
    AB -->|Continue| AC["Next Turn"]
    
    style A fill:#e1f5ff
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style O fill:#9c27b0,color:#fff
    style P fill:#ba68c8
    style Q fill:#ce93d8
    style R fill:#ab47bc
    style S fill:#4fc3f7
    style T fill:#29b6f6
    style V fill:#ffca28
    style Z fill:#f57f17,color:#fff
    style AA fill:#ff6f00,color:#fff
    style AC fill:#bf360c,color:#fff
```

---

## Dual Knowledge Base Architecture

```mermaid
graph TD
    Agent["ğŸ¤– Conversational Agent"]
    
    subgraph KB1["Knowledge Base 1"]
        Index1["Index: population_data_kb"]
        Field1["Field: text"]
        Emb1["Field: embedding<br/>384-dim"]
        Data1["Docs: 6 city records"]
    end
    
    subgraph KB2["Knowledge Base 2"]
        Index2["Index: test_tech_news"]
        Field2["Field: passage"]
        Emb2["Field: passage_embedding<br/>384-dim"]
        Data2["Docs: 3 tech articles"]
    end
    
    subgraph Tools["Agent Tools"]
        Tool1["VectorDBTool<br/>population_knowledge_base"]
        Tool2["VectorDBTool<br/>tech_news_knowledge_base"]
    end
    
    Agent -->|Route to| Tool1
    Agent -->|Route to| Tool2
    
    Tool1 -->|Search| Index1
    Tool1 -->|Retrieve| Data1
    
    Tool2 -->|Search| Index2
    Tool2 -->|Retrieve| Data2
    
    Tool1 -->|Output| LLM["OpenAI LLM<br/>gpt-3.5-turbo"]
    Tool2 -->|Output| LLM
    
    LLM -->|Generate| Response["Chatbot Response"]
    
    Agent -->|Store| Memory["Conversation Memory<br/>conversation_index"]
    
    style Agent fill:#9c27b0,color:#fff
    style Tool1 fill:#4fc3f7
    style Tool2 fill:#0288d1
    style Index1 fill:#29b6f6
    style Index2 fill:#0277bd
    style Data1 fill:#b3e5fc
    style Data2 fill:#81d4fa
    style LLM fill:#fbc02d,color:#000
    style Response fill:#ff6f00,color:#fff
    style Memory fill:#ce93d8,color:#fff
```

---

## Detailed Component Flows

### 1. **Dual Ingest Pipeline Setup**
```mermaid
graph TD
    A["Create Ingest Pipeline"] -->|ID| B["test-pipeline-local-model"]
    
    B -->|Processor 1| C["Text Embedding"]
    C -->|Input Field| D["text"]
    C -->|Output Field| E["embedding"]
    
    B -->|Processor 2| F["Passage Embedding"]
    F -->|Input Field| G["passage"]
    F -->|Output Field| H["passage_embedding"]
    
    C -->|Model| I["Embedding Model<br/>all-MiniLM-L12-v2"]
    F -->|Model| I
    
    I -->|Generate| J["384-dim Vectors"]
    
    E -->|Field Type| K["knn_vector"]
    H -->|Field Type| K
    
    K -->|Index Type| L["Lucene Engine"]
    L -->|Distance| M["L2 Space"]
    
    style A fill:#81d4fa
    style B fill:#4fc3f7
    style C fill:#29b6f6
    style F fill:#0288d1
    style I fill:#0277bd,color:#fff
    style J fill:#01579b,color:#fff
```

### 2. **Index Creation Strategy**
```mermaid
graph TD
    A["Index 1:<br/>population_data_kb"] -->|Mappings| B["text: text"]
    A -->|Mappings| C["embedding: knn_vector"]
    A -->|Settings| D["default_pipeline"]
    A -->|Settings| E["knn: true"]
    
    F["Index 2:<br/>test_tech_news"] -->|Mappings| G["passage: text"]
    F -->|Mappings| H["passage_embedding: knn_vector"]
    F -->|Settings| I["default_pipeline"]
    F -->|Settings| J["knn: true"]
    
    D -->|Pipeline| K["test-pipeline-local-model"]
    I -->|Pipeline| K
    
    K -->|Auto-embed| L["âœ… Pipeline Ready"]
    
    style A fill:#4fc3f7
    style F fill:#0288d1
    style B fill:#29b6f6
    style G fill:#0277bd
    style L fill:#01579b,color:#fff
```

### 3. **Data Loading from Multiple Sources**
```mermaid
sequenceDiagram
    participant Script
    participant Pipeline
    participant Index1 as Population Index
    participant Index2 as Tech News Index
    
    Script ->> Pipeline: Configure pipeline
    activate Pipeline
    Pipeline -->> Script: Ready
    deactivate Pipeline
    
    par Load Population Data
        Script ->> Index1: Bulk insert 6 docs
        activate Index1
        Index1 ->> Pipeline: Apply pipeline
        Pipeline ->> Index1: Generate embeddings
        deactivate Index1
    and Load Tech Data
        Script ->> Index2: Bulk insert 3 docs
        activate Index2
        Index2 ->> Pipeline: Apply pipeline
        Pipeline ->> Index2: Generate embeddings
        deactivate Index2
    end
    
    Index1 -->> Script: Load complete
    Index2 -->> Script: Load complete
    
    Script ->> Script: Both KBs ready
```

### 4. **OpenAI Connector Configuration**
```mermaid
graph TD
    A["OpenAI Connector Setup"] -->|Parameter| B["endpoint: api.openai.com"]
    A -->|Parameter| C["model: gpt-3.5-turbo"]
    A -->|Parameter| D["response_filter"]
    A -->|Parameter| E["stop: custom tokens"]
    A -->|Parameter| F["system_instruction"]
    
    D -->|Filter| G["$.choices[0].message.content"]
    
    E -->|Stop Tokens| H["\\n\\nHuman:"]
    E -->|Stop Tokens| I["\\nObservation:"]
    
    F -->|Instruction| J["Assist with Questions"]
    
    A -->|Credential| K["openAI_key"]
    K -->|Token| L["Bearer Auth"]
    
    M["Request Action"] -->|Method| N["POST"]
    M -->|URL| O["OpenAI API"]
    M -->|Headers| P["Authorization"]
    M -->|Body| Q["Messages JSON"]
    
    style A fill:#ce93d8
    style G fill:#ba68c8
    style J fill:#ab47bc
    style L fill:#9c27b0,color:#fff
```

### 5. **Conversational Agent Structure**
```mermaid
graph TD
    A["Agent Configuration"] -->|Type| B["conversational"]
    A -->|App Type| C["chat_with_rag"]
    A -->|Description| D["Chat Agent with RAG"]
    
    E["LLM Configuration"] -->|Model| F["openai_model_id"]
    E -->|max_iteration| G["3"]
    E -->|response_filter| H["$.choices[0].message.content"]
    
    I["Memory Configuration"] -->|Type| J["conversation_index"]
    
    K["Tool 1: VectorDBTool"] -->|Name| L["population_knowledge_base"]
    K -->|Description| M["Population data of US cities"]
    K -->|Index| N["population_data_kb"]
    K -->|Embedding| O["embedding field"]
    K -->|doc_size| P["3 docs"]
    K -->|Source| Q["text field"]
    
    R["Tool 2: VectorDBTool"] -->|Name| S["tech_news_knowledge_base"]
    R -->|Description| T["Tech news data"]
    R -->|Index| U["test_tech_news"]
    R -->|Embedding| V["passage_embedding"]
    R -->|doc_size| W["2 docs"]
    R -->|Source| X["passage field"]
    
    E -->|Register| Y["âœ… Agent Ready"]
    I -->|Register| Y
    K -->|Register| Y
    R -->|Register| Y
    
    style A fill:#ffca28
    style B fill:#fbc02d
    style C fill:#f57f17
    style Y fill:#e65100,color:#fff
```

### 6. **Query Execution with KB Routing**
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Routing as Query Router
    participant KB1 as Pop KB
    participant KB2 as Tech KB
    participant LLM
    
    User ->> Agent: Q1: "What's vision pro?"
    Agent ->> Routing: Analyze query
    Routing ->> Routing: Detect: Tech topic
    Routing -->> Agent: Use Tech KB
    
    Agent ->> KB2: Search tech_news_kb
    KB2 -->> Agent: 2 docs (Apple, LLaMA, Bedrock)
    
    Agent ->> LLM: Vision Pro docs + Q1
    LLM -->> Agent: Tech answer
    Agent ->> Agent: Store in memory
    Agent -->> User: Answer about Vision Pro
    
    User ->> Agent: Q2: "Population increase<br/>Seattle 2021-2023?"
    Agent ->> Routing: Analyze query
    Routing ->> Routing: Detect: Population topic
    Routing -->> Agent: Use Population KB
    
    Agent ->> KB1: Search population_kb
    KB1 -->> Agent: 3 docs (Seattle data)
    
    Agent ->> LLM: Population docs + Q2 + Memory
    LLM -->> Agent: Population answer
    Agent ->> Agent: Store in memory
    Agent -->> User: Answer with context
```

### 7. **Multi-turn Conversation State**
```mermaid
graph TD
    A["Turn 1"] -->|User Query| B["What's vision pro?"]
    B -->|Execute| C["Search Tech KB"]
    C -->|Results| D["Generate Answer"]
    D -->|Store| E["Memory Entry 1"]
    E -->|Return| F["Answer 1"]
    
    F -->|Turn 2| G["What's LLaMA?"]
    G -->|Execute| H["Search Tech KB<br/>with Memory"]
    H -->|Results| I["Generate Answer<br/>with context"]
    I -->|Store| J["Memory Entry 2"]
    J -->|Return| K["Answer 2"]
    
    K -->|Turn 3| L["Population of Seattle?"]
    L -->|Execute| M["Search Population KB<br/>with Memory"]
    M -->|Results| N["Generate Answer<br/>with full context"]
    N -->|Store| O["Memory Entry 3"]
    O -->|Return| P["Answer 3"]
    
    E -->|Retain| Q["Conversation Context"]
    J -->|Retain| Q
    O -->|Retain| Q
    
    style A fill:#c8e6c9
    style B fill:#81c784
    style D fill:#ffca28
    style E fill:#90caf9,color:#000
    style G fill:#a5d6a7
    style I fill:#fbc02d
    style J fill:#90caf9,color:#000
    style L fill:#aed581
    style N fill:#f57f17
    style O fill:#90caf9,color:#000
    style Q fill:#5c6bc0,color:#fff
```

### 8. **Response Generation Pipeline**
```mermaid
graph TD
    A["KB Search Results"] -->|Population KB| B["Top 3 docs<br/>population data"]
    A -->|Tech KB| C["Top 2 docs<br/>tech articles"]
    
    B -->|Extract| D["Relevant text<br/>city data"]
    C -->|Extract| E["Relevant text<br/>tech info"]
    
    F["LLM Request Preparation"] -->|System| G["Role: Assist with<br/>various questions"]
    F -->|User Role| H["Question + Context"]
    
    D -->|Include| H
    E -->|Include| H
    
    I["Conversation Memory"] -->|Previous| J["Context from<br/>prior turns"]
    J -->|Include| H
    
    H -->|Send to| K["OpenAI API<br/>gpt-3.5-turbo"]
    
    K -->|Request| L["HTTP POST"]
    L -->|With| M["Bearer Token"]
    
    K -->|Response| N["JSON Response"]
    N -->|Filter| O["$.choices[0].message.content"]
    
    O -->|Extract| P["Generated Answer"]
    P -->|Return| Q["User Response"]
    
    style F fill:#ce93d8
    style G fill:#ba68c8
    style K fill:#fbc02d,color:#000
    style P fill:#ff6f00,color:#fff
    style Q fill:#bf360c,color:#fff
```

---

## Key Features

### ğŸ¯ **Advanced Chatbot Capabilities**
- **Dual Knowledge Bases**: Population data and tech news simultaneously available
- **Intelligent Routing**: Automatically selects appropriate KB based on query
- **Full Conversation Memory**: Maintains context across unlimited turns
- **Max Iteration Control**: Limits agent loops to prevent infinite execution
- **Response Filtering**: Extracts clean responses from OpenAI API
- **Custom Stop Tokens**: Prevents unwanted response continuation
- **Multi-field Embedding**: Different text fields for different content types

### ğŸ”„ **Knowledge Base Specifications**

**Population KB (population_data_kb)**
- Documents: 6 US metro areas
- Field: `text` (population statistics)
- Embedding: `embedding` field
- Return: Top 3 documents per query
- Data: Ogden-Layton, NYC, Chicago, Miami, Austin, Seattle

**Tech News KB (test_tech_news)**
- Documents: 3 tech topics
- Field: `passage` (tech articles)
- Embedding: `passage_embedding` field
- Return: Top 2 documents per query
- Data: Apple Vision Pro, LLaMA, Amazon Bedrock

---

## Agent Configuration Parameters

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Agent Type** | conversational | Multi-turn chat support |
| **App Type** | chat_with_rag | RAG-enabled chatbot |
| **LLM Model** | gpt-3.5-turbo | Response generation |
| **Max Iterations** | 3 | Loop prevention |
| **Memory Type** | conversation_index | Multi-turn storage |
| **Pop KB doc_size** | 3 | Retrieved documents |
| **Tech KB doc_size** | 2 | Retrieved documents |
| **Embedding Dim** | 384 | Vector dimension |

---

## Data Flow

```
User Query
    â†“
[Query Analysis]
â”œâ”€ Semantic understanding
â””â”€ KB relevance detection
    â†“
[KB Selection]
â”œâ”€ Route to Population KB (if about demographics)
â”œâ”€ Route to Tech KB (if about technology)
â””â”€ Route to Both (if general)
    â†“
[Parallel Search]
â”œâ”€ VectorDBTool: population_knowledge_base
â””â”€ VectorDBTool: tech_news_knowledge_base
    â†“
[Result Aggregation]
â”œâ”€ Combine retrieved documents
â””â”€ Maintain context order
    â†“
[LLM Request]
â”œâ”€ System: Assistant role
â”œâ”€ Context: Retrieved documents
â”œâ”€ Memory: Previous turns
â””â”€ Question: Current query
    â†“
[Response Generation]
â”œâ”€ OpenAI gpt-3.5-turbo
â”œâ”€ Filter response content
â””â”€ Stop at control tokens
    â†“
[Memory Storage]
â”œâ”€ Save question
â”œâ”€ Save answer
â””â”€ Link to conversation
    â†“
Return Answer to User
```

---

## Technologies Used
- ğŸ” **OpenSearch**: Multi-index search and agent orchestration
- ğŸ¤– **Sentence Transformers**: Embedding generation (all-MiniLM-L12-v2)
- ğŸ§  **OpenAI GPT-3.5-turbo**: Conversational response generation
- ğŸ’¾ **Conversation Index**: Memory management for multi-turn dialogue
- ğŸ“ **Ingest Pipeline**: Dual-field automatic embedding
- âš™ï¸ **ML Commons**: Model registration and deployment

---

## Unique Aspects
1. **Multiple Domain Support**: Seamlessly switches between population and tech domains
2. **Dual Field Embedding**: Different field mappings for different content types
3. **Configurable Doc Retrieval**: Different doc_size per KB (3 vs 2)
4. **Intelligent Routing**: Uses semantic understanding for KB selection
5. **Full Memory Integration**: Maintains complete conversation history
6. **Production Ready**: Includes response filtering and iteration limits
7. **Extensible Design**: Easy to add more knowledge bases and tools

---

## Example Conversation Flow

```
User: "What's vision pro?"
â†’ Agent detects tech topic
â†’ Queries test_tech_news (top 2 docs)
â†’ Returns Vision Pro overview

User: "When was it released?"
â†’ Agent uses memory
â†’ Queries tech KB with context
â†’ Returns release date info

User: "Compare with Seattle population"
â†’ Agent detects topic switch
â†’ Queries both KBs
â†’ Combines tech KB memory with population data
â†’ Returns comparative answer
```
