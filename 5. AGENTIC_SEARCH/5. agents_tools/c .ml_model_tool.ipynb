{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86a8d47",
   "metadata": {},
   "source": [
    "# ü§ñ MLModelTool - Remote LLM Inference\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#9B59B6', 'primaryTextColor':'#fff', 'primaryBorderColor':'#8E44AD', 'lineColor':'#F39C12', 'secondaryColor':'#3498DB', 'tertiaryColor':'#27AE60', 'fontSize':'16px'}}}%%\n",
    "graph TB\n",
    "    A[üë§ User Prompt<br/>Generate text] --> B[ü§ñ Flow Agent]\n",
    "    B --> C{üß† MLModelTool}\n",
    "    C --> D[üì° OpenAI Connector]\n",
    "    D --> E[üéØ GPT-4o-mini]\n",
    "    E --> F[üí¨ Generated Response]\n",
    "    F --> G[üì§ Formatted Output]\n",
    "    \n",
    "    style A fill:#3498DB,stroke:#2980B9,color:#fff\n",
    "    style C fill:#9B59B6,stroke:#8E44AD,color:#fff\n",
    "    style D fill:#E67E22,stroke:#D35400,color:#fff\n",
    "    style E fill:#E74C3C,stroke:#C0392B,color:#fff\n",
    "    style G fill:#27AE60,stroke:#229954,color:#fff\n",
    "```\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "1. ‚úÖ How to use **MLModelTool** for remote LLM inference\n",
    "2. ‚úÖ Configuring **OpenAI models** with custom prompts\n",
    "3. ‚úÖ Building conversational agents with context\n",
    "4. ‚úÖ Prompt engineering for different use cases\n",
    "5. ‚úÖ Integrating LLM capabilities into agent workflows\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is MLModelTool?\n",
    "\n",
    "**MLModelTool** enables agents to call remote LLM models (like OpenAI GPT) for text generation, summarization, analysis, and more. This is the foundation for:\n",
    "- üí¨ **Conversational AI**: Build chat interfaces\n",
    "- üìù **Text Generation**: Create content dynamically\n",
    "- üß† **Analysis**: Interpret and explain data\n",
    "- üîÑ **Transformation**: Rewrite, summarize, translate text\n",
    "\n",
    "**Key Features**:\n",
    "- Call any registered LLM model\n",
    "- Custom prompt templates\n",
    "- Parameter injection from user input\n",
    "- Response extraction and formatting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13b7b",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe97c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "# Add parent directory to path to import helper functions\n",
    "sys.path.append('..')\n",
    "from agent_helpers import (\n",
    "    get_os_client,\n",
    "    configure_cluster_for_openai,\n",
    "    create_openai_connector,\n",
    "    register_and_deploy_openai_model,\n",
    "    create_flow_agent,\n",
    "    execute_agent,\n",
    "    cleanup_resources\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8a4e1",
   "metadata": {},
   "source": [
    "## Step 2: Initialize OpenSearch Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142d164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to OpenSearch cluster: docker-cluster\n",
      "üìä Version: 3.3.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "client = get_os_client()\n",
    "\n",
    "# Verify connection\n",
    "info = client.info()\n",
    "print(f\"‚úÖ Connected to OpenSearch cluster: {info['cluster_name']}\")\n",
    "print(f\"üìä Version: {info['version']['number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e6962",
   "metadata": {},
   "source": [
    "## Step 3: Configure Cluster and Create OpenAI Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb54681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Configuring cluster settings for OpenAI connector...\n",
      "   ‚úì Cluster settings configured successfully\n",
      "‚úÖ Cluster configured for OpenAI\n",
      "   Creating OpenAI connector for gpt-4o-mini...\n",
      "   ‚úì Connector created: hbUOb5oBFJiTVjgy_ZQ1\n",
      "‚úÖ OpenAI connector created: hbUOb5oBFJiTVjgy_ZQ1\n"
     ]
    }
   ],
   "source": [
    "# Configure cluster to trust OpenAI endpoint\n",
    "configure_cluster_for_openai(client)\n",
    "print(\"‚úÖ Cluster configured for OpenAI\")\n",
    "\n",
    "# Create OpenAI connector\n",
    "connector_id = create_openai_connector(client)\n",
    "print(f\"‚úÖ OpenAI connector created: {connector_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b542dc0",
   "metadata": {},
   "source": [
    "## Step 4: Register and Deploy OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40b83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Creating model group...\n",
      "   ‚úì Model group created: hrUPb5oBFJiTVjgyBpT3\n",
      "   Registering gpt-4o-mini model...\n",
      "   ‚úì Model registered: iLUPb5oBFJiTVjgyB5QU\n",
      "   Deploying model...\n",
      "   ‚è≥ Waiting for model deployment...\n",
      "      Model status: DEPLOYING\n",
      "      Model status: DEPLOYED\n",
      "      ‚úì Model deployed successfully!\n",
      "‚úÖ OpenAI model deployed: iLUPb5oBFJiTVjgyB5QU\n"
     ]
    }
   ],
   "source": [
    "# Register and deploy GPT-4o-mini model\n",
    "model_id = register_and_deploy_openai_model(client, connector_id)\n",
    "print(f\"‚úÖ OpenAI model deployed: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67340863",
   "metadata": {},
   "source": [
    "## Step 5: Create Flow Agent with MLModelTool\n",
    "\n",
    "We'll create an agent with a simple prompt template that passes user questions to GPT-4o-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b080363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Registering flow agent: ML_Model_Agent...\n",
      "   ‚úì Agent registered: i7UQb5oBFJiTVjgyzZQ6\n",
      "‚úÖ Flow agent created with ID: i7UQb5oBFJiTVjgyzZQ6\n",
      "üîß Tool configured: MLModelTool\n",
      "üß† Model: GPT-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Define the tool configuration\n",
    "tools = [{\n",
    "    \"type\": \"MLModelTool\",\n",
    "    \"parameters\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a professional data analyst. You will always answer a question based on the given context first. If the answer is not directly shown in the context, you will analyze the data and find the answer. If you don't know the answer, just say you don't know.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Context:\\\\nQuestion: ${parameters.question}\\\"}]\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "# Create the flow agent\n",
    "agent_id = create_flow_agent(\n",
    "    client,\n",
    "    \"ML_Model_Agent\",\n",
    "    \"An agent that uses OpenAI GPT-4o-mini for text generation and analysis\",\n",
    "    tools\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Flow agent created with ID: {agent_id}\")\n",
    "print(f\"üîß Tool configured: MLModelTool\")\n",
    "print(f\"üß† Model: GPT-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa55b3",
   "metadata": {},
   "source": [
    "## Step 6: Test Case 1 - Simple Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce0c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What is OpenSearch and how is it different from Elasticsearch?\n",
      "============================================================\n",
      "\n",
      "üí¨ LLM Response:\n",
      "{\n",
      "  \"inference_results\": [\n",
      "    {\n",
      "      \"output\": [\n",
      "        {\n",
      "          \"name\": \"response\",\n",
      "          \"result\": \"{\\\"id\\\":\\\"chatcmpl-CaR5TFwm5mwbxp4612pUjBAYvTSgG\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"created\\\":1.762799967E9,\\\"model\\\":\\\"gpt-4o-mini-2024-07-18\\\",\\\"choices\\\":[{\\\"index\\\":0.0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"OpenSearch is an open-source search and analytics suite that was created as a fork of Elasticsearch and Kibana after Elastic changed the licensing of their software. The primary difference between OpenSearch and Elasticsearch lies in their licensing and governance. OpenSearch is governed by the OpenSearch community under an Apache 2.0 license, which allows for more open collaboration and contributions from the community. In contrast, Elasticsearch has moved to a more restrictive license model, which limits certain uses and contributions. Additionally, OpenSearch aims to maintain compatibility with Elasticsearch APIs while providing its own features and enhancements.\\\",\\\"refusal\\\":null,\\\"annotations\\\":[]},\\\"logprobs\\\":null,\\\"finish_reason\\\":\\\"stop\\\"}],\\\"usage\\\":{\\\"prompt_tokens\\\":81.0,\\\"completion_tokens\\\":114.0,\\\"total_tokens\\\":195.0,\\\"prompt_tokens_details\\\":{\\\"cached_tokens\\\":0.0,\\\"audio_tokens\\\":0.0},\\\"completion_tokens_details\\\":{\\\"reasoning_tokens\\\":0.0,\\\"audio_tokens\\\":0.0,\\\"accepted_prediction_tokens\\\":0.0,\\\"rejected_prediction_tokens\\\":0.0}},\\\"service_tier\\\":\\\"default\\\",\\\"system_fingerprint\\\":\\\"fp_560af6e559\\\"}\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Ask a simple question\n",
    "parameters = {\n",
    "    \"question\": \"What is OpenSearch and how is it different from Elasticsearch?\"\n",
    "}\n",
    "\n",
    "print(\"‚ùì Question: What is OpenSearch and how is it different from Elasticsearch?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = execute_agent(client, agent_id, parameters)\n",
    "\n",
    "print(\"\\nüí¨ LLM Response:\")\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9eae421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenSearch is an open-source search and analytics suite that was created as a fork of Elasticsearch and Kibana after Elastic changed the licensing of their software. The primary difference between OpenSearch and Elasticsearch lies in their licensing and governance. OpenSearch is governed by the OpenSearch community under an Apache 2.0 license, which allows for more open collaboration and contributions from the community. In contrast, Elasticsearch has moved to a more restrictive license model, which limits certain uses and contributions. Additionally, OpenSearch aims to maintain compatibility with Elasticsearch APIs while providing its own features and enhancements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Extract and display the response in human-readable format\n",
    "result_content = json.loads(response['inference_results'][0]['output'][0]['result'])['choices'][0]['message']['content']\n",
    "display(Markdown(result_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43ad88",
   "metadata": {},
   "source": [
    "## Step 7: Test Case 2 - Technical Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eeff3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Explain how vector embeddings work in semantic search\n",
      "============================================================\n",
      "\n",
      "üìö Technical Explanation:\n",
      "{\n",
      "  \"inference_results\": [\n",
      "    {\n",
      "      \"output\": [\n",
      "        {\n",
      "          \"name\": \"response\",\n",
      "          \"result\": \"{\\\"id\\\":\\\"chatcmpl-CaR6yIOBCPheH8iPLLmhZYVGKQI2o\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"created\\\":1.76280006E9,\\\"model\\\":\\\"gpt-4o-mini-2024-07-18\\\",\\\"choices\\\":[{\\\"index\\\":0.0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Vector embeddings are a way to represent words, phrases, or even entire documents as numerical vectors in a high-dimensional space. In semantic search, these embeddings capture the meaning and context of the text, allowing for more nuanced search results.\\\\n\\\\nHere's how they work in simple terms:\\\\n\\\\n1. **Representation**: Each word or phrase is converted into a vector, which is a list of numbers. The position of these vectors in the space reflects their semantic meaning. Words with similar meanings will have vectors that are close together.\\\\n\\\\n2. **Similarity Measurement**: When a user inputs a search query, it is also converted into a vector. The search system then measures the similarity between the query vector and the vectors of the documents in the database.\\\\n\\\\n3. **Ranking Results**: Documents that have vectors closest to the query vector are considered the most relevant and are ranked higher in the search results.\\\\n\\\\nBy using vector embeddings, semantic search can understand the context and meaning behind words, leading to more accurate and relevant search outcomes.\\\",\\\"refusal\\\":null,\\\"annotations\\\":[]},\\\"logprobs\\\":null,\\\"finish_reason\\\":\\\"stop\\\"}],\\\"usage\\\":{\\\"prompt_tokens\\\":82.0,\\\"completion_tokens\\\":201.0,\\\"total_tokens\\\":283.0,\\\"prompt_tokens_details\\\":{\\\"cached_tokens\\\":0.0,\\\"audio_tokens\\\":0.0},\\\"completion_tokens_details\\\":{\\\"reasoning_tokens\\\":0.0,\\\"audio_tokens\\\":0.0,\\\"accepted_prediction_tokens\\\":0.0,\\\"rejected_prediction_tokens\\\":0.0}},\\\"service_tier\\\":\\\"default\\\",\\\"system_fingerprint\\\":\\\"fp_560af6e559\\\"}\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Vector embeddings are a way to represent words, phrases, or even entire documents as numerical vectors in a high-dimensional space. In semantic search, these embeddings capture the meaning and context of the text, allowing for more nuanced search results.\n",
       "\n",
       "Here's how they work in simple terms:\n",
       "\n",
       "1. **Representation**: Each word or phrase is converted into a vector, which is a list of numbers. The position of these vectors in the space reflects their semantic meaning. Words with similar meanings will have vectors that are close together.\n",
       "\n",
       "2. **Similarity Measurement**: When a user inputs a search query, it is also converted into a vector. The search system then measures the similarity between the query vector and the vectors of the documents in the database.\n",
       "\n",
       "3. **Ranking Results**: Documents that have vectors closest to the query vector are considered the most relevant and are ranked higher in the search results.\n",
       "\n",
       "By using vector embeddings, semantic search can understand the context and meaning behind words, leading to more accurate and relevant search outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "# Ask for technical explanation\n",
    "parameters = {\n",
    "    \"question\": \"Explain how vector embeddings work in semantic search. Keep it simple.\"\n",
    "}\n",
    "\n",
    "print(\"‚ùì Question: Explain how vector embeddings work in semantic search\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = execute_agent(client, agent_id, parameters)\n",
    "\n",
    "print(\"\\nüìö Technical Explanation:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "# Extract and display the response in human-readable format\n",
    "result_content = json.loads(response['inference_results'][0]['output'][0]['result'])['choices'][0]['message']['content']\n",
    "display(Markdown(result_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85f042",
   "metadata": {},
   "source": [
    "## Step 8: Test Case 3 - Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26fd4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Write a Python function for cosine similarity\n",
      "============================================================\n",
      "\n",
      "üíª Generated Code:\n",
      "{\n",
      "  \"inference_results\": [\n",
      "    {\n",
      "      \"output\": [\n",
      "        {\n",
      "          \"name\": \"response\",\n",
      "          \"result\": \"{\\\"id\\\":\\\"chatcmpl-CaR7adLS2iDKPWZHZ0DIFIGxG8hP7\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"created\\\":1.762800098E9,\\\"model\\\":\\\"gpt-4o-mini-2024-07-18\\\",\\\"choices\\\":[{\\\"index\\\":0.0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"To calculate cosine similarity between two vectors in Python, you can use the following function:\\\\n\\\\n```python\\\\nimport numpy as np\\\\n\\\\ndef cosine_similarity(vec1, vec2):\\\\n    # Convert the input lists to numpy arrays\\\\n    vec1 = np.array(vec1)\\\\n    vec2 = np.array(vec2)\\\\n    \\\\n    # Calculate the dot product of the two vectors\\\\n    dot_product = np.dot(vec1, vec2)\\\\n    \\\\n    # Calculate the magnitude of each vector\\\\n    magnitude_vec1 = np.linalg.norm(vec1)\\\\n    magnitude_vec2 = np.linalg.norm(vec2)\\\\n    \\\\n    # Calculate cosine similarity\\\\n    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\\\\n        return 0.0  # Return 0 if either vector is zero\\\\n    else:\\\\n        return dot_product / (magnitude_vec1 * magnitude_vec2)\\\\n\\\\n# Example usage:\\\\nvec1 = [1, 2, 3]\\\\nvec2 = [4, 5, 6]\\\\nsimilarity = cosine_similarity(vec1, vec2)\\\\nprint(\\\\\\\"Cosine Similarity:\\\\\\\", similarity)\\\\n```\\\\n\\\\nThis function computes the cosine similarity by calculating the dot product of the two vectors and dividing it by the product of their magnitudes.\\\",\\\"refusal\\\":null,\\\"annotations\\\":[]},\\\"logprobs\\\":null,\\\"finish_reason\\\":\\\"stop\\\"}],\\\"usage\\\":{\\\"prompt_tokens\\\":81.0,\\\"completion_tokens\\\":252.0,\\\"total_tokens\\\":333.0,\\\"prompt_tokens_details\\\":{\\\"cached_tokens\\\":0.0,\\\"audio_tokens\\\":0.0},\\\"completion_tokens_details\\\":{\\\"reasoning_tokens\\\":0.0,\\\"audio_tokens\\\":0.0,\\\"accepted_prediction_tokens\\\":0.0,\\\"rejected_prediction_tokens\\\":0.0}},\\\"service_tier\\\":\\\"default\\\",\\\"system_fingerprint\\\":\\\"fp_560af6e559\\\"}\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To calculate cosine similarity between two vectors in Python, you can use the following function:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "\n",
       "def cosine_similarity(vec1, vec2):\n",
       "    # Convert the input lists to numpy arrays\n",
       "    vec1 = np.array(vec1)\n",
       "    vec2 = np.array(vec2)\n",
       "    \n",
       "    # Calculate the dot product of the two vectors\n",
       "    dot_product = np.dot(vec1, vec2)\n",
       "    \n",
       "    # Calculate the magnitude of each vector\n",
       "    magnitude_vec1 = np.linalg.norm(vec1)\n",
       "    magnitude_vec2 = np.linalg.norm(vec2)\n",
       "    \n",
       "    # Calculate cosine similarity\n",
       "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
       "        return 0.0  # Return 0 if either vector is zero\n",
       "    else:\n",
       "        return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
       "\n",
       "# Example usage:\n",
       "vec1 = [1, 2, 3]\n",
       "vec2 = [4, 5, 6]\n",
       "similarity = cosine_similarity(vec1, vec2)\n",
       "print(\"Cosine Similarity:\", similarity)\n",
       "```\n",
       "\n",
       "This function computes the cosine similarity by calculating the dot product of the two vectors and dividing it by the product of their magnitudes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "# Request code example\n",
    "parameters = {\n",
    "    \"question\": \"Write a Python function to calculate cosine similarity between two vectors.\"\n",
    "}\n",
    "\n",
    "print(\"‚ùì Question: Write a Python function for cosine similarity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = execute_agent(client, agent_id, parameters)\n",
    "\n",
    "print(\"\\nüíª Generated Code:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "# Extract and display the response in human-readable format\n",
    "result_content = json.loads(response['inference_results'][0]['output'][0]['result'])['choices'][0]['message']['content']\n",
    "display(Markdown(result_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce832d",
   "metadata": {},
   "source": [
    "## Step 9: Create Specialized Agent with System Prompt\n",
    "\n",
    "Let's create an agent with a specialized system prompt for OpenSearch expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c443ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Registering flow agent: OpenSearch_Expert_Agent...\n",
      "   ‚úì Agent registered: j7UZb5oBFJiTVjgyHJSn\n",
      "‚úÖ Specialized agent created: j7UZb5oBFJiTVjgyHJSn\n"
     ]
    }
   ],
   "source": [
    "# Create agent with specialized system prompt\n",
    "specialized_tools = [\n",
    "    {\n",
    "        \"type\": \"MLModelTool\",\n",
    "        \"parameters\": {\n",
    "            \"model_id\": model_id,\n",
    "            \"messages\": \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are an expert OpenSearch consultant with deep knowledge of:\\\\n- OpenSearch architecture and operations\\\\n- Vector search and ML capabilities\\\\n- Index management and optimization\\\\n- Query DSL and performance tuning\\\\n\\\\nProvide detailed, accurate answers with examples when appropriate.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Question: ${parameters.question}\\\"}]\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "specialized_agent_id = create_flow_agent(\n",
    "    client,\n",
    "    \"OpenSearch_Expert_Agent\",\n",
    "    \"An OpenSearch expert agent with specialized knowledge\",\n",
    "    specialized_tools\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Specialized agent created: {specialized_agent_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047ce1d",
   "metadata": {},
   "source": [
    "## Step 10: Test Specialized Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790376e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Configuring an OpenSearch cluster for production use, especially with an expected data ingestion of 100GB daily, requires careful planning and consideration of various factors, including hardware, index management, replication, sharding, and monitoring. Below are the key steps and recommendations to set up your OpenSearch cluster effectively:\n",
       "\n",
       "### 1. **Cluster Sizing and Hardware Configuration**\n",
       "\n",
       "#### a. **Node Types**\n",
       "- **Master Nodes**: Dedicated nodes for cluster management. Typically, you should have at least 3 master-eligible nodes to ensure high availability.\n",
       "- **Data Nodes**: Nodes that store the actual data and handle search and indexing requests. The number of data nodes will depend on your data volume and query load.\n",
       "- **Ingest Nodes**: Optional nodes that can preprocess documents before indexing. Useful if you have complex data transformations.\n",
       "\n",
       "#### b. **Hardware Specifications**\n",
       "- **CPU**: At least 8 cores per node for data nodes, more if you expect heavy query loads.\n",
       "- **Memory**: Allocate 50% of the available RAM to the JVM heap, but do not exceed 32GB for the heap size. For example, if you have 64GB of RAM, set the heap size to 32GB.\n",
       "- **Disk**: Use SSDs for better performance. Plan for at least 3 times your expected daily data ingestion for storage. For 100GB daily, aim for at least 300GB of disk space per data node, considering replicas and overhead.\n",
       "\n",
       "### 2. **Index Management**\n",
       "\n",
       "#### a. **Sharding Strategy**\n",
       "- **Primary Shards**: Start with a reasonable number of primary shards. A common approach is to have 1 primary shard for every 30-50GB of data. For 100GB daily, you might start with 3-5 primary shards per index.\n",
       "- **Replicas**: Set at least 1 replica for high availability. This means if you have 5 primary shards, you will have 5 replica shards, effectively doubling your storage needs.\n",
       "\n",
       "#### b. **Index Lifecycle Management (ILM)**\n",
       "- Implement ILM policies to manage indices over time. For example, you can roll over indices daily or weekly based on size or age, and delete older indices after a certain period to save space.\n",
       "\n",
       "### 3. **Data Ingestion and Processing**\n",
       "\n",
       "- Use **Bulk API** for data ingestion to optimize performance.\n",
       "- Consider using **Ingest Pipelines** for preprocessing data, such as parsing, enriching, or transforming documents before they are indexed.\n",
       "\n",
       "### 4. **Query Optimization**\n",
       "\n",
       "- Use **Query DSL** effectively. Optimize your queries by:\n",
       "  - Using filters instead of queries when possible, as filters are cached.\n",
       "  - Avoiding wildcard queries on large datasets.\n",
       "  - Utilizing aggregations wisely to minimize resource consumption.\n",
       "\n",
       "### 5. **Monitoring and Maintenance**\n",
       "\n",
       "- Implement monitoring tools like **OpenSearch Dashboards** or **Prometheus** to keep track of cluster health, node performance, and resource utilization.\n",
       "- Regularly check for slow queries and optimize them.\n",
       "- Set up alerts for critical metrics such as disk usage, CPU load, and memory usage.\n",
       "\n",
       "### 6. **Backup and Recovery**\n",
       "\n",
       "- Implement a snapshot strategy using the OpenSearch snapshot and restore feature. Schedule regular snapshots to a remote repository (e.g., S3) to ensure data durability.\n",
       "\n",
       "### 7. **Security Configuration**\n",
       "\n",
       "- Enable security features such as authentication, authorization, and encryption in transit and at rest. Use OpenSearch Security plugin for managing roles and permissions.\n",
       "\n",
       "### Example Configuration\n",
       "\n",
       "Assuming you have a cluster with 3 master nodes and 6 data nodes, here‚Äôs a simplified configuration:\n",
       "\n",
       "- **Master Nodes**: 3 nodes (e.g., m5.large instances)\n",
       "- **Data Nodes**: 6 nodes (e.g., r5.large instances with 64GB RAM)\n",
       "- **Shards**: 5 primary shards per index, 1 replica\n",
       "- **Index Lifecycle Policy**: Roll over daily, delete indices older than 30 days\n",
       "- **Snapshot Policy**: Daily snapshots to S3\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "By following these guidelines, you can configure your OpenSearch cluster to handle 100GB of data daily efficiently. Always monitor the performance and adjust the configuration as needed based on actual usage patterns and growth."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test specialized agent\n",
    "parameters = {\n",
    "    \"question\": \"How should I configure my OpenSearch cluster for production use? I expect 100GB of data daily.\"\n",
    "}\n",
    "\n",
    "print(\"‚ùì Question: OpenSearch cluster configuration advice\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = execute_agent(client, specialized_agent_id, parameters)\n",
    "\n",
    "print(\"\\nüéØ Expert Advice:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "# Extract and display the response in human-readable format\n",
    "result_content = json.loads(response['inference_results'][0]['output'][0]['result'])['choices'][0]['message']['content']\n",
    "display(Markdown(result_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ffc39",
   "metadata": {},
   "source": [
    "## Step 11: Test Case 4 - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd27d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Analyze sales trend data\n",
      "============================================================\n",
      "\n",
      "üìä Analysis:\n",
      "{\n",
      "  \"inference_results\": [\n",
      "    {\n",
      "      \"output\": [\n",
      "        {\n",
      "          \"name\": \"response\",\n",
      "          \"result\": \"{\\\"id\\\":\\\"chatcmpl-CaRFWPqVvEMCf6jgNqelrtvRkfplr\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"created\\\":1.76280059E9,\\\"model\\\":\\\"gpt-4o-mini-2024-07-18\\\",\\\"choices\\\":[{\\\"index\\\":0.0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Based on the provided data, we can observe the following trends in sales performance over the three quarters:\\\\n\\\\n1. **Positive Growth in Q1 and Q2**: \\\\n   - In Q1, sales increased by 25%, indicating a strong start to the year. This could be attributed to various factors such as seasonal demand, successful marketing campaigns, or the introduction of new products.\\\\n   - In Q2, sales continued to grow, albeit at a slower rate of 15%. This suggests that while the momentum from Q1 carried into Q2, the growth rate was beginning to stabilize, which is common as markets mature or as initial excitement wanes.\\\\n\\\\n2. **Decline in Q3**: \\\\n   - The drop of 5% in Q3 is a notable shift from the previous quarters of growth. This decline could be indicative of several potential issues or external factors.\\\\n\\\\n### Possible Explanations for the Q3 Decline:\\\\n\\\\n1. **Seasonality**: \\\\n   - Depending on the industry, Q3 might traditionally be a slower period for sales. For example, retail businesses often see a dip in sales during the summer months before the back-to-school season in Q4.\\\\n\\\\n2. **Market Saturation**: \\\\n   - If the product or service has reached a saturation point in the market, the initial growth may not be sustainable. Customers who were likely to purchase may have already done so, leading to a natural decline in new sales.\\\\n\\\\n3. **Increased Competition**: \\\\n   - The entry of new competitors or aggressive marketing strategies from existing competitors could have diverted customers away, resulting in a decrease in sales.\\\\n\\\\n4. **Economic Factors**: \\\\n   - Broader economic conditions, such as inflation, changes in consumer spending habits, or economic downturns, could impact consumer confidence and spending, leading to a decline in sales.\\\\n\\\\n5. **Product Issues**: \\\\n   - There may have been issues with the product itself, such as quality concerns, negative reviews, or a lack of new features that could attract customers.\\\\n\\\\n6. **Marketing and Sales Strategy**: \\\\n   - A potential decline in marketing efforts or changes in sales strategy could also contribute to the drop. If the company reduced its advertising budget or shifted focus away from certain products, this could lead to decreased visibility and sales.\\\\n\\\\n### Conclusion:\\\\nThe trends indicate a strong start to the year with a concerning decline in Q3. To address the decline, it would be beneficial for the company to conduct a thorough analysis of market conditions, customer feedback, and competitive landscape to identify the root causes and develop strategies to mitigate the decline in future quarters.\\\",\\\"refusal\\\":null,\\\"annotations\\\":[]},\\\"logprobs\\\":null,\\\"finish_reason\\\":\\\"stop\\\"}],\\\"usage\\\":{\\\"prompt_tokens\\\":107.0,\\\"completion_tokens\\\":531.0,\\\"total_tokens\\\":638.0,\\\"prompt_tokens_details\\\":{\\\"cached_tokens\\\":0.0,\\\"audio_tokens\\\":0.0},\\\"completion_tokens_details\\\":{\\\"reasoning_tokens\\\":0.0,\\\"audio_tokens\\\":0.0,\\\"accepted_prediction_tokens\\\":0.0,\\\"rejected_prediction_tokens\\\":0.0}},\\\"service_tier\\\":\\\"default\\\",\\\"system_fingerprint\\\":\\\"fp_560af6e559\\\"}\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided data, we can observe the following trends in sales performance over the three quarters:\n",
       "\n",
       "1. **Positive Growth in Q1 and Q2**: \n",
       "   - In Q1, sales increased by 25%, indicating a strong start to the year. This could be attributed to various factors such as seasonal demand, successful marketing campaigns, or the introduction of new products.\n",
       "   - In Q2, sales continued to grow, albeit at a slower rate of 15%. This suggests that while the momentum from Q1 carried into Q2, the growth rate was beginning to stabilize, which is common as markets mature or as initial excitement wanes.\n",
       "\n",
       "2. **Decline in Q3**: \n",
       "   - The drop of 5% in Q3 is a notable shift from the previous quarters of growth. This decline could be indicative of several potential issues or external factors.\n",
       "\n",
       "### Possible Explanations for the Q3 Decline:\n",
       "\n",
       "1. **Seasonality**: \n",
       "   - Depending on the industry, Q3 might traditionally be a slower period for sales. For example, retail businesses often see a dip in sales during the summer months before the back-to-school season in Q4.\n",
       "\n",
       "2. **Market Saturation**: \n",
       "   - If the product or service has reached a saturation point in the market, the initial growth may not be sustainable. Customers who were likely to purchase may have already done so, leading to a natural decline in new sales.\n",
       "\n",
       "3. **Increased Competition**: \n",
       "   - The entry of new competitors or aggressive marketing strategies from existing competitors could have diverted customers away, resulting in a decrease in sales.\n",
       "\n",
       "4. **Economic Factors**: \n",
       "   - Broader economic conditions, such as inflation, changes in consumer spending habits, or economic downturns, could impact consumer confidence and spending, leading to a decline in sales.\n",
       "\n",
       "5. **Product Issues**: \n",
       "   - There may have been issues with the product itself, such as quality concerns, negative reviews, or a lack of new features that could attract customers.\n",
       "\n",
       "6. **Marketing and Sales Strategy**: \n",
       "   - A potential decline in marketing efforts or changes in sales strategy could also contribute to the drop. If the company reduced its advertising budget or shifted focus away from certain products, this could lead to decreased visibility and sales.\n",
       "\n",
       "### Conclusion:\n",
       "The trends indicate a strong start to the year with a concerning decline in Q3. To address the decline, it would be beneficial for the company to conduct a thorough analysis of market conditions, customer feedback, and competitive landscape to identify the root causes and develop strategies to mitigate the decline in future quarters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze data scenario\n",
    "parameters = {\n",
    "    \"question\": \"\"\"Given this data: Sales increased 25% in Q1, 15% in Q2, but dropped 5% in Q3. \n",
    "    What trends do you see and what might explain the Q3 decline?\"\"\"\n",
    "}\n",
    "\n",
    "print(\"‚ùì Question: Analyze sales trend data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = execute_agent(client, specialized_agent_id, parameters)\n",
    "\n",
    "print(\"\\nüìä Analysis:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "# Extract and display the response in human-readable format\n",
    "result_content = json.loads(response['inference_results'][0]['output'][0]['result'])['choices'][0]['message']['content']\n",
    "display(Markdown(result_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd0f63",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **MLModelTool Capabilities**:\n",
    "   - ‚úÖ Call remote LLM models (OpenAI, Bedrock, etc.)\n",
    "   - ‚úÖ Custom prompt templates with parameter injection\n",
    "   - ‚úÖ Flexible for many use cases (QA, generation, analysis)\n",
    "   - ‚úÖ Foundation for intelligent agent behaviors\n",
    "\n",
    "2. **Prompt Engineering Patterns**:\n",
    "   ```python\n",
    "   # Simple prompt\n",
    "   \"prompt\": \"${parameters.question}\"\n",
    "   \n",
    "   # Structured prompt\n",
    "   \"prompt\": \"Human: ${parameters.question}\\n\\nAssistant:\"\n",
    "   \n",
    "   # System prompt\n",
    "   \"prompt\": \"\"\"You are an expert in X.\n",
    "   User: ${parameters.question}\n",
    "   Expert:\"\"\"\n",
    "   \n",
    "   # Few-shot learning\n",
    "   \"prompt\": \"\"\"Example 1: Q: ... A: ...\n",
    "   Example 2: Q: ... A: ...\n",
    "   Now answer: ${parameters.question}\"\"\"\n",
    "   ```\n",
    "\n",
    "3. **Practical Use Cases**:\n",
    "   - üí¨ **Conversational Agents**: Build chat interfaces\n",
    "   - üìù **Content Generation**: Create summaries, descriptions\n",
    "   - üß† **Data Analysis**: Interpret trends and patterns\n",
    "   - üîÑ **Text Transformation**: Rewrite, translate, format\n",
    "   - üéì **Question Answering**: Provide explanations and guidance\n",
    "\n",
    "4. **Model Configuration**:\n",
    "   ```python\n",
    "   # Basic configuration\n",
    "   {\n",
    "       \"type\": \"MLModelTool\",\n",
    "       \"parameters\": {\n",
    "           \"model_id\": \"model_id_here\",\n",
    "           \"prompt\": \"Your prompt template\"\n",
    "       }\n",
    "   }\n",
    "   \n",
    "   # With response filtering\n",
    "   {\n",
    "       \"type\": \"MLModelTool\",\n",
    "       \"parameters\": {\n",
    "           \"model_id\": \"model_id_here\",\n",
    "           \"prompt\": \"Your prompt\",\n",
    "           \"response_filter\": \"$.choices[0].message.content\"\n",
    "       }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- ‚úÖ **Clear Instructions**: Be specific in your prompts\n",
    "- ‚úÖ **Role Definition**: Define the AI's role/expertise\n",
    "- ‚úÖ **Output Format**: Specify desired response format\n",
    "- ‚úÖ **Context**: Provide relevant background information\n",
    "- ‚úÖ **Constraints**: Set boundaries (length, style, scope)\n",
    "\n",
    "### Prompt Engineering Tips:\n",
    "\n",
    "- üéØ **Be Specific**: \"Explain X in 3 bullet points\" vs \"Explain X\"\n",
    "- üéØ **Use Examples**: Few-shot learning improves accuracy\n",
    "- üéØ **Set Context**: Define role, audience, purpose\n",
    "- üéØ **Iterate**: Test and refine prompts\n",
    "- üéØ **Chain Thoughts**: Break complex tasks into steps\n",
    "\n",
    "### Performance Considerations:\n",
    "\n",
    "- ‚ö° **Model Selection**: Balance capability vs cost/latency\n",
    "- ‚ö° **Prompt Length**: Longer prompts = higher costs\n",
    "- ‚ö° **Caching**: Cache responses for common queries\n",
    "- ‚ö° **Timeouts**: Set appropriate timeout values\n",
    "- ‚ö° **Rate Limits**: Respect API rate limits\n",
    "\n",
    "### Combining with Other Tools:\n",
    "\n",
    "```python\n",
    "# Multi-tool agent workflow\n",
    "tools = [\n",
    "    {\"type\": \"SearchIndexTool\", ...},  # 1. Get data\n",
    "    {\"type\": \"MLModelTool\", ...}       # 2. Analyze with LLM\n",
    "]\n",
    "\n",
    "# RAG pattern\n",
    "tools = [\n",
    "    {\"type\": \"VectorDBTool\", ...},     # 1. Retrieve context\n",
    "    {\"type\": \"MLModelTool\", ...}       # 2. Generate answer\n",
    "]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd46a11",
   "metadata": {},
   "source": [
    "## üßπ Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell to clean up resources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete agents and models\n",
    "# cleanup_resources(\n",
    "#     client=client,\n",
    "#     model_ids=[model_id],\n",
    "#     agent_ids=[agent_id, specialized_agent_id],\n",
    "#     connector_ids=[connector_id]\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a0c2c",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you understand MLModelTool, explore:\n",
    "- **RAGTool**: Combine retrieval with LLM generation\n",
    "- **QueryPlanningTool**: Use LLM to generate DSL queries\n",
    "- **PPLTool**: Generate PPL queries from natural language\n",
    "- **AgentTool**: Build multi-agent systems\n",
    "\n",
    "---\n",
    "\n",
    "üìö **Resources**:\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [ML Commons Agent Tools](https://opensearch.org/docs/latest/ml-commons-plugin/agents-tools/)\n",
    "- [MLModelTool Documentation](https://opensearch.org/docs/latest/ml-commons-plugin/agents-tools/tools/ml-model-tool/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-search-with-opensearch (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
