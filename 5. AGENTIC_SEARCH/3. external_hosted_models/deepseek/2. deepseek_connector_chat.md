# DeepSeek Connector Chat - Production Integration Guide

## ğŸ“– Overview

This script demonstrates a **production-ready DeepSeek chat integration** with OpenSearch. It's the full-featured version that includes complete error handling, status monitoring, and resource cleanup - everything needed for enterprise deployments.

### ğŸ¯ What You'll Learn
- Complete connector lifecycle management
- Error handling in production workflows
- Model deployment status monitoring with polling
- Resource cleanup with exception handling
- Best practices for API integrations

---

## ğŸ—ï¸ Full Production Architecture

```mermaid
graph TB
    A["OpenSearch Cluster<br/>Production Setup"] 
    B["Cluster Configuration<br/>Trust & Memory Settings"]
    C["Model Group<br/>Organization Layer"]
    D["HTTP Connector<br/>DeepSeek Bridge"]
    E["Model Registration<br/>Remote Function"]
    F["Deployment Management<br/>Health Monitoring"]
    G["Test Inference<br/>Verify Functionality"]
    H["Resource Cleanup<br/>Exception Safe"]
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    
    D -.->|API Request| I["DeepSeek API<br/>api.deepseek.com"]
    G -.->|Test Query| I
    I -.->|Response| G
    
    style A fill:#e1f5ff
    style I fill:#fff3e0
    style H fill:#e8f5e9
```

---

## ğŸ”„ Complete 6-Step Workflow

### Step 1: Initialize & Configure Cluster

```python
def get_os_client(cluster_url=CLUSTER_URL, 
                  username=DEFAULT_USERNAME, 
                  password=DEFAULT_PASSWORD):
    """
    Create production-ready OpenSearch client
    """
    client = OpenSearch(
        hosts=[cluster_url],
        http_auth=(username, password),
        verify_certs=False,              # For testing/local
        ssl_assert_hostname=False,       # Skip hostname check
        ssl_show_warn=False,             # Suppress SSL warnings
        use_ssl=True,                    # Always use HTTPS
        max_retries=10,                  # Retry failed requests
        retry_on_timeout=True,           # Handle timeouts
        timeout=300                      # 5-minute timeout for API calls
    )
    return client
```

**Production Configuration:**

| Setting | Value | Purpose |
|---------|-------|---------|
| `max_retries` | 10 | Handle transient failures |
| `retry_on_timeout` | True | Retry on timeout errors |
| `timeout` | 300s | Allow external APIs time |
| `use_ssl` | True | Encrypt all traffic |

### Step 2: Configure Cluster Settings

```python
cluster_settings = {
    "persistent": {
        # Accept any trusted endpoint
        "plugins.ml_commons.trusted_connector_endpoints_regex": [".*"],
        # Run ML tasks on any node
        "plugins.ml_commons.only_run_on_ml_node": "false",
        # Enable conversation memory
        "plugins.ml_commons.memory_feature_enabled": "true",
        # Allow private IP connectors
        "plugins.ml_commons.connector.private_ip_enabled": "true"
    }
}
client.cluster.put_settings(body=cluster_settings)
```

**Configuration Rationale:**

```mermaid
graph LR
    A["Setting"] --> B["Value"] --> C["Use Case"]
    
    A1["trusted_connector<br/>_endpoints_regex"] --> B1[".*"] --> C1["Accept any endpoint"]
    A2["only_run_on<br/>_ml_node"] --> B2["false"] --> C2["Flexibility"]
    A3["memory_feature<br/>_enabled"] --> B3["true"] --> C3["Stateful conversations"]
    A4["connector.private<br/>_ip_enabled"] --> B4["true"] --> C4["Self-hosted APIs"]
```

---

## ğŸ“‹ Step-by-Step Implementation

### Step 3: Create Model Group

```python
model_group_name = f"deepseek_chat_group_{int(time.time())}"
model_group_body = {
    "name": model_group_name,
    "description": "Model group for deepseek chat"
}
model_group_response = client.transport.perform_request(
    'POST',
    '/_plugins/_ml/model_groups/_register',
    body=model_group_body
)
model_group_id = model_group_response['model_group_id']
```

**Why timestamp-based naming?**
- âœ… Prevents naming conflicts
- âœ… Enables parallel deployments
- âœ… Provides chronological ordering
- âœ… Example: `deepseek_chat_group_1699564800`

### Step 4: Create DeepSeek Connector

```python
connector_body = {
    "name": "DeepSeek Chat",
    "description": "Connector for DeepSeek Chat API",
    "version": "1",
    "protocol": "http",
    "parameters": {
        "endpoint": "api.deepseek.com",
        "model": "deepseek-chat"
    },
    "credential": {
        "deepSeek_key": os.getenv("DEEPSEEK_API_KEY")
    },
    "actions": [
        {
            "action_type": "predict",
            "method": "POST",
            "url": "https://${parameters.endpoint}/v1/chat/completions",
            "headers": {
                "Content-Type": "application/json",
                "Authorization": "Bearer ${credential.deepSeek_key}"
            },
            "request_body": "{ \"model\": \"${parameters.model}\", \"messages\": ${parameters.messages} }"
        }
    ]
}
```

**Connector Components Explained:**

```mermaid
graph TB
    A["Connector Structure"]
    
    A --> B["Basic Metadata"]
    A --> C["Parameters"]
    A --> D["Credentials"]
    A --> E["Actions"]
    
    B --> B1["name: DeepSeek Chat<br/>version: 1"]
    C --> C1["endpoint: api.deepseek.com<br/>model: deepseek-chat"]
    D --> D1["deepSeek_key: From .env<br/>Secure storage"]
    E --> E1["action_type: predict<br/>method: POST<br/>URL template<br/>Headers template"]
    
    style A fill:#e1f5ff
    style B1 fill:#fff3e0
    style C1 fill:#fff3e0
    style D1 fill:#e8f5e9
    style E1 fill:#fff3e0
```

### Step 5: Register Model

```python
model_body = {
    "name": "deepseek_chat_model",
    "function_name": "remote",              # Use external API
    "model_group_id": model_group_id,       # Organization
    "description": f"{DEEPSEEK_MODEL} chat model",
    "connector_id": connector_id,            # Link to connector
    "model_format": "TORCH_SCRIPT"          # Standard format
}
model_response = client.transport.perform_request(
    'POST',
    '/_plugins/_ml/models/_register',
    body=model_body
)
model_id = model_response['model_id']
```

### Step 6: Deploy Model with Monitoring

```python
# Deploy model
deploy_body = {"deployment_plan": [{"model_id": model_id, "workers": 1}]}
try:
    client.transport.perform_request(
        'POST',
        f'/_plugins/_ml/models/{model_id}/_deploy',
        body=deploy_body
    )
    print("âœ“ Model deployment initiated")
except Exception as e:
    print(f"âš  Error deploying model: {e}")
    return

# Monitor deployment with polling
print("â³ Waiting for model deployment to complete...")
while True:
    status_response = client.transport.perform_request(
        'GET',
        f'/_plugins/_ml/models/{model_id}'
    )
    current_status = status_response['model_state']
    print(f"   Current status: {current_status}")
    
    if current_status == 'DEPLOYED':
        print("âœ“ Model deployed successfully!")
        break
    elif current_status == 'FAILED':
        print("âœ— Model deployment failed!")
        return
    
    time.sleep(5)  # Check every 5 seconds
```

**Deployment State Flow:**

```mermaid
stateDiagram-v2
    [*] --> REGISTERING
    REGISTERING --> REGISTERED: Success
    REGISTERING --> FAILED: Error
    
    REGISTERED --> DEPLOYING: Deploy requested
    DEPLOYING --> DEPLOYED: Success
    DEPLOYING --> FAILED: Error
    
    DEPLOYED --> UNDEPLOYING: Undeploy requested
    UNDEPLOYING --> REGISTERED: Complete
    
    FAILED --> [*]
    REGISTERED --> [*]
    
    note right of DEPLOYED
        Model ready for inference
    end note
    
    note right of DEPLOYING
        Polling for status
    end note
```

---

## ğŸ§ª Testing with Error Handling

```python
print("Step 5: Testing Model with Sample Data...")
predict_body = {
    "parameters": {
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Why is the sky blue?"}
        ]
    }
}

try:
    predict_response = client.transport.perform_request(
        'POST',
        f'/_plugins/_ml/models/{model_id}/_predict',
        body=predict_body
    )
    print("âœ“ Model prediction successful!")
    print(json.dumps(predict_response, indent=2))
except Exception as e:
    print(f"âš  Error during prediction: {e}\n")
```

**Expected Response Structure:**

```json
{
  "inference_results": [
    {
      "output": [
        {
          "name": "response",
          "dataAsString": "The sky appears blue due to Rayleigh scattering of sunlight..."
        }
      ]
    }
  ]
}
```

---

## ğŸ§¹ Production-Grade Resource Cleanup

```python
def cleanup_resources(client, model_id, connector_id, model_group_id):
    """
    Clean up resources with exception handling
    Ensures cleanup continues even if individual steps fail
    """
    
    # Step 1: Undeploy model (always first!)
    try:
        client.transport.perform_request(
            'POST',
            f'/_plugins/_ml/models/{model_id}/_undeploy'
        )
        print(f"âœ“ Undeployed model with ID: {model_id}")
    except Exception as e:
        print(f"âš  Error undeploying model: {e}")
    
    # Step 2: Delete model
    try:
        client.transport.perform_request(
            'DELETE',
            f'/_plugins/_ml/models/{model_id}'
        )
        print(f"âœ“ Deleted model with ID: {model_id}")
    except Exception as e:
        print(f"âš  Error deleting model: {e}")
    
    # Step 3: Delete connector
    try:
        client.transport.perform_request(
            'DELETE',
            f'/_plugins/_ml/connectors/{connector_id}'
        )
        print(f"âœ“ Deleted connector with ID: {connector_id}")
    except Exception as e:
        print(f"âš  Error deleting connector: {e}")
    
    # Step 4: Delete model group
    try:
        client.transport.perform_request(
            'DELETE',
            f'/_plugins/_ml/model_groups/{model_group_id}'
        )
        print(f"âœ“ Deleted model group with ID: {model_group_id}")
    except Exception as e:
        print(f"âš  Error deleting model group: {e}")
    
    print("âœ“ Cleanup completed!\n")
```

**Cleanup Order is Critical:**

```mermaid
graph TB
    A["Start Cleanup"]
    B["Undeploy Model<br/>Free resources"]
    C["Delete Model<br/>Remove metadata"]
    D["Delete Connector<br/>Remove API bridge"]
    E["Delete Model Group<br/>Remove organization"]
    F["Cleanup Complete"]
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    
    B -.->|If fails| C
    C -.->|Continue anyway| D
    D -.->|Continue anyway| E
    
    style B fill:#fff3e0
    style F fill:#e8f5e9
```

**Why This Order?**
1. **Undeploy first** - Stops using the resource
2. **Delete model** - Remove inference capability
3. **Delete connector** - Remove API bridge
4. **Delete group** - Remove organization

---

## ğŸ’¡ Key Patterns in This Script

### Pattern 1: Exception Handling in Cleanup

```python
# Continue cleanup even if one step fails
try:
    # Attempt operation
    ...
except Exception as e:
    # Log but continue
    print(f"âš  Error: {e}")
    # Don't return - proceed to next cleanup step
```

### Pattern 2: Status Polling with Timeout

```python
max_attempts = 120  # 5 minutes at 5-second intervals
attempts = 0

while attempts < max_attempts:
    status_response = client.transport.perform_request(
        'GET',
        f'/_plugins/_ml/models/{model_id}'
    )
    
    if status_response['model_state'] == 'DEPLOYED':
        break
    
    time.sleep(5)
    attempts += 1

if attempts >= max_attempts:
    print("Error: Deployment timeout")
```

### Pattern 3: Early Return on Critical Errors

```python
# Deploy model
try:
    client.transport.perform_request(...)
    print("âœ“ Deployment initiated")
except Exception as e:
    print(f"âš  Error deploying model: {e}")
    return  # Early exit - can't proceed without deployment
```

---

## ğŸ“Š Production Checklist

### Before Deployment

- [ ] DEEPSEEK_API_KEY set in .env
- [ ] OpenSearch cluster accessible
- [ ] Network allows outbound HTTPS to api.deepseek.com
- [ ] Credentials are correct (test manually first)
- [ ] Model group naming doesn't conflict

### During Deployment

- [ ] Monitor deployment status logs
- [ ] Check for error messages
- [ ] Verify model_id is returned
- [ ] Confirm deployment reaches DEPLOYED state

### After Testing

- [ ] Cleanup resources completed successfully
- [ ] Verify no orphaned resources remain
- [ ] Check cluster logs for warnings
- [ ] Document any issues encountered

---

## âš ï¸ Troubleshooting Guide

| Problem | Cause | Solution |
|---------|-------|----------|
| **API Key Error** | Invalid/missing key | Verify DEEPSEEK_API_KEY in .env |
| **Connection Refused** | Endpoint unreachable | Check network connectivity to api.deepseek.com |
| **Timeout Error** | API too slow | Increase timeout from 300 to 600 seconds |
| **Model Not Deployed** | Resource limits | Check cluster health, increase heap if needed |
| **Prediction Failed** | Model not ready | Verify model_state is 'DEPLOYED' |
| **SSL Warning** | Certificate verification | Expected with verify_certs=False |
| **Cleanup Fails** | Resources already deleted | Safe - exceptions are caught |

---

## ğŸ”„ Complete Workflow Example

```
â”Œâ”€ STEP 1: Initialize
â”‚  â”œâ”€ Create OpenSearch client
â”‚  â””â”€ Configure cluster settings
â”‚
â”œâ”€ STEP 2: Setup
â”‚  â”œâ”€ Create model group (deepseek_chat_group_1699564800)
â”‚  â”œâ”€ Create connector (conn_12345)
â”‚  â””â”€ Register model (model_98765)
â”‚
â”œâ”€ STEP 3: Deploy
â”‚  â”œâ”€ Deploy model
â”‚  â”œâ”€ Poll status (every 5 seconds)
â”‚  â””â”€ Wait for DEPLOYED state
â”‚
â”œâ”€ STEP 4: Test
â”‚  â”œâ”€ Send test message
â”‚  â”œâ”€ Receive response
â”‚  â””â”€ Verify format
â”‚
â””â”€ STEP 5: Cleanup
   â”œâ”€ Undeploy model
   â”œâ”€ Delete model
   â”œâ”€ Delete connector
   â””â”€ Delete model group
```

---

## ğŸ“ Comparison: Lite vs Full Version

| Feature | Lite Version | Full Version |
|---------|-------------|-------------|
| **Error Handling** | Basic | Exception handling throughout |
| **Status Monitoring** | Simple polling | Robust polling with checks |
| **Cleanup** | Basic deletion | Exception-safe cleanup |
| **Production Ready** | â­â­ | â­â­â­â­â­ |
| **Lines of Code** | ~100 | ~250 |
| **Use Case** | Learning | Enterprise |

---

## ğŸš€ Production Best Practices

### 1. Timeout Management

```python
# External APIs need more time
timeout=300  # 5 minutes
retry_on_timeout=True
max_retries=10
```

### 2. Graceful Degradation

```python
try:
    # Attempt operation
except Exception as e:
    # Log and continue
    logger.warning(f"Non-critical error: {e}")
    # Don't crash - let cleanup happen
```

### 3. Resource Identification

```python
# Use unique identifiers for debugging
model_group_name = f"deepseek_chat_group_{int(time.time())}"
# Makes it easy to track resources in logs
```

### 4. Status Verification

```python
# Don't assume success - verify state
if status_response['model_state'] == 'DEPLOYED':
    print("Verified deployment successful")
elif status_response['model_state'] == 'FAILED':
    print("Deployment failed - investigate")
```

---

## ğŸ“ˆ Performance Metrics

### Typical Execution Times

```
Initialize Client:              100ms
Configure Cluster:              500ms
Create Model Group:            1000ms
Create Connector:              1000ms
Register Model:                2000ms
Deploy Model:               30000ms (30 sec)
Test Prediction:             2000ms
Cleanup:                      5000ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                   ~42 seconds
```

---

## ğŸ”— Related Topics

- ğŸ”— [DeepSeek Lite Version](./deepseek_connector_chat_lite.md)
- ğŸ”— [OpenAI Integration](../openai/README_OPENAI.md)
- ğŸ”— [Anthropic Connector](../anthropic/anthropic_connector_chat.md)
- ğŸ”— [External Models Overview](../README_EXTERNAL_MODELS.md)

---

## âœ¨ Summary

This production version demonstrates:
- âœ… **Complete error handling** throughout workflow
- âœ… **Robust status monitoring** with polling
- âœ… **Exception-safe cleanup** ensuring no orphaned resources
- âœ… **Production-ready code** with best practices
- âœ… **Enterprise patterns** for reliability
- âœ… **Proper resource management** lifecycle

**Perfect for deploying DeepSeek integration in production environments!** ğŸš€

