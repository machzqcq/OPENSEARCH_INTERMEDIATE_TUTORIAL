# Ingest pipelines

An ingest pipeline is a sequence of processors that are applied to documents as they are ingested into an index. Each processor in a pipeline performs a specific task, such as filtering, transforming, or enriching data.

Processors are customizable tasks that run in a sequential order as they appear in the request body. This order is important, as each processor depends on the output of the previous processor. The modified documents appear in your index after the processors are applied.

## OpenSearch ingest pipelines compared to Data Prepper

OpenSearch ingest pipelines run within the OpenSearch cluster, whereas Data Prepper is an external component that runs on the OpenSearch cluster.

- **OpenSearch ingest pipelines** are recommended for simple data pre-processing and small datasets.
- **Data Prepper** is recommended for any data processing tasks it supports, particularly when dealing with large datasets and complex data pre-processing requirements.

## Handling pipeline failures
Each ingest pipeline consists of a series of processors that are applied to the documents in sequence. If a processor fails, the entire pipeline will fail. You have two options for handling failures:

Fail the entire pipeline: If a processor fails, the entire pipeline will fail and the document will not be indexed.
Fail the current processor and continue with the next processor: This can be useful if you want to continue processing the document even if one of the processors fails

By default, an ingest pipeline stops if one of its processors fails. If you want the pipeline to continue running when a processor fails, you can set the ignore_failure parameter for that processor to true when creating the pipeline:

```
PUT _ingest/pipeline/my-pipeline/
{
  "description": "Rename 'provider' field to 'cloud.provider'",
  "processors": [
    {
      "rename": {
        "field": "provider",
        "target_field": "cloud.provider",
        "ignore_failure": true
      }
    }
  ]
}
```

You can specify the on_failure parameter to run immediately after a processor fails. If you have specified on_failure, OpenSearch will run the other processors in the pipeline even if the on_failure configuration is empty:

```
PUT _ingest/pipeline/my-pipeline/
{
  "description": "Add timestamp to the document",
  "processors": [
    {
      "date": {
        "field": "timestamp_field",
        "formats": ["yyyy-MM-dd HH:mm:ss"],
        "target_field": "@timestamp",
        "on_failure": [
          {
            "set": {
              "field": "ingest_error",
              "value": "failed"
            }
          }
        ]
      }
    }
  ]
}
```

## List of ingest processors

| Processor                | Purpose                                                  | Use Case                                                |
|--------------------------|----------------------------------------------------------|---------------------------------------------------------|
| â• append              | Adds one or more values to a field                       | Add tags or metadata fields to documents                 |
| ğŸ”¢ bytes               | Converts human-readable byte values to bytes            | Normalize storage size fields                            |
| ğŸ”— community_id        | Generates network flow hash identifiers                  | Network traffic analysis and correlation                 |
| ğŸ”„ convert             | Changes the data type of a field                          | Convert string to integer or date fields                 |
| ğŸ“‹ copy                | Copies an entire object from one field to another        | Duplicate fields for indexing or transformation          |
| ğŸ“Š csv                 | Extracts CSV data into individual fields                 | Parse CSV-formatted data embedded in one field           |
| ğŸ“… date                | Parses and sets date fields                               | Normalize timestamp formats                              |
| ğŸ“† date_index_name     | Automatically index into time-based indices              | Manage rollover indices by timestamp                      |
| ğŸ” dissect             | Extracts fields using a defined text pattern             | Structured log parsing                                   |
| ğŸ“ dot_expander        | Converts dotted field names into objects                  | Convert flat keys with dots into nested objects           |
| ğŸ—‘ï¸ drop                | Drops a document from being indexed                      | Filter out unwanted documents                             |
| â›” fail                | Raises an error and stops pipeline execution             | Fail analysis or validation checks                        |
| ğŸ‘† fingerprint         | Generates a hash fingerprint of specified fields         | Deduplicate or fingerprint documents                      |
| ğŸ” foreach             | Applies another processor to each element of an array    | Process arrays of nested objects                          |
| ğŸŒ geoip               | Adds geolocation info based on IP address                 | Enrich log data with geospatial information              |
| ğŸ—ºï¸ geojson-feature     | Indexes GeoJSON data into geospatial fields              | Index spatial data for geo searches                       |
| ğŸ“ grok                | Extracts structured fields via regex patterns            | Parse logs and unstructured text                          |
| âœï¸ gsub                | Substitutes or deletes substrings in a field             | Clean or normalize textual fields                         |
| ğŸ§¹ html_strip          | Removes HTML tags from text                               | Clean HTML content before indexing                        |
| ğŸŒ ip2geo              | Adds geolocation info for IP addresses                    | Similar to geoip processor                                |
| ğŸ”— join                | Joins array elements into a string                        | Convert arrays to concatenated strings                    |
| ğŸ§© json                | Parses JSON strings into structured objects              | Convert string fields containing JSON                     |
| ğŸ”‘ kv                  | Parses key-value pairs into fields                        | Parse logs or structured key-value data                   |
| ğŸ”¡ lowercase           | Converts field text to lowercase                          | Normalize keywords or tags                                |
| ğŸ—ï¸ pipeline            | Runs an inner pipeline                                    | Compose complex pipelines from simpler ones              |
| âŒ remove              | Removes fields from documents                             | Eliminate unwanted or sensitive data                      |
| ğŸ¯ remove_by_pattern   | Removes fields matching a pattern                         | Dynamic field cleanup                                     |
| ğŸ”€ rename              | Renames fields                                            | Rename fields for consistency                             |
| âš™ï¸ script              | Runs a script on documents                                | Custom transformations and enrichments                    |
| âœ… set                 | Sets a field to a constant value                          | Add default values                                       |
| ğŸ“ˆ sort                | Sorts array elements                                      | Order arrays within documents                            |
| ğŸ§  sparse_encoding     | Generates sparse vectors for neural sparse search        | Prepare text for vector-based search                      |
| âœ‚ï¸ split               | Splits a field string into an array                       | Parse CSV lists into arrays                               |
| ğŸ“– text_chunking       | Splits text into chunks                                   | Handle large text fields for processing                   |
| ğŸ”® text_embedding      | Generates vector embeddings from text                     | Semantic search and machine learning workflows            |
| ğŸ¨ text_image_embedding | Generates combined text/image vector embeddings          | Multimodal AI search                                     |
| âœ‚ï¸ trim                | Trims whitespace from strings                             | Clean string fields                                      |
| ğŸ”¤ uppercase           | Converts text to uppercase                                | Normalize tags or keywords                               |
| ğŸ”“ urldecode           | Decodes URL-encoded strings                               | Clean URL parameters                                     |
| ğŸ‘¤ user_agent          | Extracts browser and device info from user agent fields  | Enrich web access logs                                  |
