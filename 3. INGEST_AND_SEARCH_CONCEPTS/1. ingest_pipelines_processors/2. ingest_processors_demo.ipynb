{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87059aa4",
   "metadata": {},
   "source": [
    "# OpenSearch Ingest Processors Comprehensive Guide\n",
    "![course](../../static_images/ai_ml_search_opensearch_intermediate.jpeg)\n",
    "\n",
    "## üìö Complete Tutorial on All Ingest Processors\n",
    "\n",
    "This notebook provides hands-on demonstrations of all 43 ingest processors available in OpenSearch. Each processor is explained with real-world examples, sample data, and executable code.\n",
    "\n",
    "**Target Audience:** Students learning ingest pipeline concepts and processor functionality\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How ingest pipelines process documents sequentially\n",
    "- Each processor's purpose and use cases\n",
    "- Practical examples with sample data\n",
    "- Error handling and failure scenarios\n",
    "- Real-world pipeline compositions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f07d3",
   "metadata": {},
   "source": [
    "## üìã Table of Contents - All 43 Processors\n",
    "\n",
    "### Data Manipulation & Transformation (8 processors)\n",
    "1. **append** - Add values to fields\n",
    "2. **copy** - Duplicate objects between fields\n",
    "3. **remove** - Remove unwanted fields\n",
    "4. **remove_by_pattern** - Remove fields by regex pattern\n",
    "5. **rename** - Rename fields\n",
    "6. **set** - Set constant field values\n",
    "7. **join** - Join array elements into strings\n",
    "8. **split** - Split strings into arrays\n",
    "\n",
    "### Data Type Conversion (5 processors)\n",
    "9. **bytes** - Convert human-readable bytes to bytes\n",
    "10. **convert** - Change field data types\n",
    "11. **lowercase** - Convert to lowercase\n",
    "12. **uppercase** - Convert to uppercase\n",
    "13. **trim** - Trim whitespace\n",
    "\n",
    "### Text & String Processing (7 processors)\n",
    "14. **csv** - Parse CSV data\n",
    "15. **dissect** - Extract fields via text patterns\n",
    "16. **gsub** - Substitute/delete substrings\n",
    "17. **grok** - Extract fields via regex patterns\n",
    "18. **html_strip** - Remove HTML tags\n",
    "19. **json** - Parse JSON strings\n",
    "20. **kv** - Parse key-value pairs\n",
    "\n",
    "### Date & Time Processing (2 processors)\n",
    "21. **date** - Parse and normalize dates\n",
    "22. **date_index_name** - Index into time-based indices\n",
    "\n",
    "### Specialized Enrichment (9 processors)\n",
    "23. **geoip** - Add geolocation from IP\n",
    "24. **ip2geo** - Add geolocation from IP (alternative)\n",
    "25. **geojson-feature** - Index GeoJSON data\n",
    "26. **user_agent** - Extract browser/device info\n",
    "27. **community_id** - Generate network flow hashes\n",
    "28. **fingerprint** - Generate deduplication hashes\n",
    "29. **text_embedding** - Generate text vectors\n",
    "30. **text_image_embedding** - Generate multimodal vectors\n",
    "31. **sparse_encoding** - Generate sparse vectors\n",
    "\n",
    "### Text Processing (2 processors)\n",
    "32. **text_chunking** - Split text into chunks\n",
    "33. **sort** - Sort array elements\n",
    "\n",
    "### URL Processing (1 processor)\n",
    "34. **urldecode** - Decode URL-encoded strings\n",
    "\n",
    "### Control Flow (2 processors)\n",
    "35. **drop** - Filter out documents\n",
    "36. **fail** - Stop pipeline with error\n",
    "\n",
    "### Advanced/Scripting (3 processors)\n",
    "37. **script** - Run custom scripts\n",
    "38. **foreach** - Apply processor to array elements\n",
    "39. **pipeline** - Compose nested pipelines\n",
    "\n",
    "### Metadata/Special (1 processor)\n",
    "40. **dot_expander** - Convert dot notation to objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0a5a2",
   "metadata": {},
   "source": [
    "## üîÑ Ingest Pipeline Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"üìÑ Document\"] --> B[\"Processor 1\"]\n",
    "    B --> C[\"Processor 2\"]\n",
    "    C --> D[\"Processor 3\"]\n",
    "    D --> E[\"...\"]\n",
    "    E --> F[\"Processor N\"]\n",
    "    F --> G[\"‚úÖ Indexed Document\"]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#fff3e0\n",
    "    style F fill:#fff3e0\n",
    "    style G fill:#c8e6c9\n",
    "```\n",
    "\n",
    "## Error Handling in Pipelines\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Processor Executes\"] --> B{Success?}\n",
    "    B -->|Yes| C[\"Continue to Next\"]\n",
    "    B -->|No| D{ignore_failure?}\n",
    "    D -->|True| C\n",
    "    D -->|False| E{on_failure?}\n",
    "    E -->|Defined| F[\"Run Failure Handler\"]\n",
    "    E -->|None| G[\"‚ùå Stop Pipeline\"]\n",
    "    F --> H[\"Continue or Stop\"]\n",
    "    \n",
    "    style A fill:#fff3e0\n",
    "    style B fill:#ffe0b2\n",
    "    style C fill:#c8e6c9\n",
    "    style G fill:#ffcdd2\n",
    "    style F fill:#f0f4c3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbabb3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to OpenSearch!\n",
      "Cluster: docker-cluster\n",
      "Version: 3.3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: Setup and Client Connection\n",
    "# ============================================================================\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "IS_AUTH = True  # Set to False if security is disabled\n",
    "HOST = 'localhost'  # Replace with your OpenSearch host\n",
    "\n",
    "# Initialize the OpenSearch client\n",
    "if IS_AUTH:\n",
    "    client = OpenSearch(\n",
    "        hosts=[{'host': HOST, 'port': 9200}],\n",
    "        http_auth=('admin', 'Developer@123'),\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "else:\n",
    "    client = OpenSearch(\n",
    "        hosts=[{'host': HOST, 'port': 9200}],\n",
    "        use_ssl=False,\n",
    "        verify_certs=False,\n",
    "        ssl_assert_hostname=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(\"‚úÖ Connected to OpenSearch!\")\n",
    "    print(f\"Cluster: {info['cluster_name']}\")\n",
    "    print(f\"Version: {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb7e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR DEMONSTRATION\n",
    "# ============================================================================\n",
    "\n",
    "# Store pipelines in memory for testing (alternative approach)\n",
    "_test_pipelines = {}\n",
    "\n",
    "def create_pipeline(pipeline_name: str, processors: list, description: str = \"\"):\n",
    "    \"\"\"Create an ingest pipeline\"\"\"\n",
    "    pipeline_body = {\n",
    "        \"description\": description,\n",
    "        \"processors\": processors\n",
    "    }\n",
    "    try:\n",
    "        response = client.ingest.put_pipeline(id=pipeline_name, body=pipeline_body)\n",
    "        # Also store locally for testing\n",
    "        _test_pipelines[pipeline_name] = processors\n",
    "        print(f\"‚úÖ Pipeline '{pipeline_name}' created successfully\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_pipeline(pipeline_name: str, document: dict, index_name: str = \"test_index\"):\n",
    "    \"\"\"Test a pipeline with a sample document\"\"\"\n",
    "    try:\n",
    "        # Get the pipeline processors from local storage or create inline request\n",
    "        if pipeline_name in _test_pipelines:\n",
    "            processors = _test_pipelines[pipeline_name]\n",
    "        else:\n",
    "            # Try to fetch from OpenSearch\n",
    "            try:\n",
    "                pipeline_info = client.ingest.get_pipeline(id=pipeline_name)\n",
    "                processors = pipeline_info[pipeline_name]['processors']\n",
    "            except:\n",
    "                print(f\"‚ùå Error testing pipeline: Pipeline '{pipeline_name}' not found\")\n",
    "                return None\n",
    "        \n",
    "        # Use simulate with inline pipeline definition\n",
    "        response = client.ingest.simulate(body={\n",
    "            \"pipeline\": {\n",
    "                \"processors\": processors\n",
    "            },\n",
    "            \"docs\": [{\"_source\": document}]\n",
    "        })\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_result(label: str, before: dict, after: dict):\n",
    "    \"\"\"Display before and after pipeline results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"üì• BEFORE:\")\n",
    "    print(json.dumps(before, indent=2))\n",
    "    print(\"\\nüì§ AFTER:\")\n",
    "    print(json.dumps(after, indent=2))\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete a pipeline\"\"\"\n",
    "    try:\n",
    "        client.ingest.delete_pipeline(id=pipeline_name)\n",
    "        # Also remove from local storage\n",
    "        if pipeline_name in _test_pipelines:\n",
    "            del _test_pipelines[pipeline_name]\n",
    "        print(f\"üóëÔ∏è  Pipeline '{pipeline_name}' deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not delete pipeline: {e}\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f9adf",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ APPEND Processor\n",
    "\n",
    "## Purpose\n",
    "Adds one or more values to an array field in a document. If the field doesn't exist, it creates it.\n",
    "\n",
    "## Use Cases\n",
    "- Adding tags or metadata to documents\n",
    "- Building classification arrays\n",
    "- Maintaining activity logs\n",
    "\n",
    "## How It Works\n",
    "```\n",
    "Input Field: [\"tag1\", \"tag2\"]\n",
    "Append Values: [\"tag3\", \"tag4\"]\n",
    "Output: [\"tag1\", \"tag2\", \"tag3\", \"tag4\"]\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "- **field**: Name of the array field to append to\n",
    "- **value**: Array of values to append (can use templates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dff8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_append' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Append Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"product_name\": \"Laptop\",\n",
      "  \"category\": \"Electronics\",\n",
      "  \"tags\": [\n",
      "    \"portable\",\n",
      "    \"powerful\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"category\": \"Electronics\",\n",
      "  \"product_name\": \"Laptop\",\n",
      "  \"tags\": [\n",
      "    \"portable\",\n",
      "    \"powerful\",\n",
      "    \"marked_for_sale\",\n",
      "    \"top_seller\"\n",
      "  ]\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_append' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 1: Basic append\n",
    "sample_doc_append = {\n",
    "    \"product_name\": \"Laptop\",\n",
    "    \"category\": \"Electronics\",\n",
    "    \"tags\": [\"portable\", \"powerful\"]\n",
    "}\n",
    "\n",
    "pipeline_append = [\n",
    "    {\n",
    "        \"append\": {\n",
    "            \"field\": \"tags\",\n",
    "            \"value\": [\"marked_for_sale\", \"top_seller\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_append\", pipeline_append, \"Append tags to products\")\n",
    "result_append = test_pipeline(\"demo_append\", sample_doc_append)\n",
    "\n",
    "if result_append:\n",
    "    original = sample_doc_append\n",
    "    processed = result_append['docs'][0]['doc']['_source']\n",
    "    display_result(\"Append Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_append\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf209fe",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ BYTES Processor\n",
    "\n",
    "## Purpose\n",
    "Converts human-readable byte values (like \"1KB\", \"1MB\") into bytes for storage normalization.\n",
    "\n",
    "## Use Cases\n",
    "- Normalizing storage size fields\n",
    "- Converting user-friendly storage units to consistent bytes\n",
    "- Data validation and standardization\n",
    "\n",
    "## Configuration\n",
    "- **field**: Field containing the byte value\n",
    "- **target_field**: Field to store result (optional, updates original if not specified)\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)\n",
    "\n",
    "## Supported Units\n",
    "- b (bytes), kb (kilobytes), mb (megabytes), gb (gigabytes), tb (terabytes), pb (petabytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f294b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_bytes' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Bytes Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"file_name\": \"document.pdf\",\n",
      "  \"file_size\": \"2MB\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"file_size_bytes\": 2097152,\n",
      "  \"file_name\": \"document.pdf\",\n",
      "  \"file_size\": \"2MB\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_bytes' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 2: Bytes processor\n",
    "sample_doc_bytes = {\n",
    "    \"file_name\": \"document.pdf\",\n",
    "    \"file_size\": \"2MB\"\n",
    "}\n",
    "\n",
    "pipeline_bytes = [\n",
    "    {\n",
    "        \"bytes\": {\n",
    "            \"field\": \"file_size\",\n",
    "            \"target_field\": \"file_size_bytes\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_bytes\", pipeline_bytes, \"Convert human-readable bytes to bytes\")\n",
    "result_bytes = test_pipeline(\"demo_bytes\", sample_doc_bytes)\n",
    "\n",
    "if result_bytes:\n",
    "    original = sample_doc_bytes\n",
    "    processed = result_bytes['docs'][0]['doc']['_source']\n",
    "    display_result(\"Bytes Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec05d4",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ CONVERT Processor\n",
    "\n",
    "## Purpose\n",
    "Changes the data type of a field (string ‚Üí integer, string ‚Üí float, string ‚Üí boolean)\n",
    "\n",
    "## Use Cases\n",
    "- Converting string numbers to numeric types\n",
    "- Converting string booleans to boolean types\n",
    "- Data normalization\n",
    "\n",
    "## Configuration\n",
    "- **field**: Field to convert\n",
    "- **type**: Target type (integer, float, double, long, boolean, ip)\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e99e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_convert' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Convert Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"price\": \"99.99\",\n",
      "  \"quantity\": \"50\",\n",
      "  \"is_active\": \"true\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"quantity\": 50,\n",
      "  \"is_active\": true,\n",
      "  \"price\": 99.99\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_convert' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 3: Convert processor - multiple conversions\n",
    "sample_doc_convert = {\n",
    "    \"price\": \"99.99\",\n",
    "    \"quantity\": \"50\",\n",
    "    \"is_active\": \"true\"\n",
    "}\n",
    "\n",
    "pipeline_convert = [\n",
    "    {\n",
    "        \"convert\": {\n",
    "            \"field\": \"price\",\n",
    "            \"type\": \"float\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"convert\": {\n",
    "            \"field\": \"quantity\",\n",
    "            \"type\": \"integer\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"convert\": {\n",
    "            \"field\": \"is_active\",\n",
    "            \"type\": \"boolean\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_convert\", pipeline_convert, \"Convert data types\")\n",
    "result_convert = test_pipeline(\"demo_convert\", sample_doc_convert)\n",
    "\n",
    "if result_convert:\n",
    "    original = sample_doc_convert\n",
    "    processed = result_convert['docs'][0]['doc']['_source']\n",
    "    display_result(\"Convert Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_convert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ecfa95",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ COPY Processor\n",
    "\n",
    "## Purpose\n",
    "Copies an entire object or field value from one field to another\n",
    "\n",
    "## Use Cases\n",
    "- Duplicating fields for different indexing strategies\n",
    "- Creating backup copies of important data\n",
    "- Field redundancy\n",
    "\n",
    "## Configuration\n",
    "- **source_field**: Source field to copy from\n",
    "- **target_field**: Destination field to copy to\n",
    "- **ignore_missing**: Don't fail if source is missing (default: false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f9bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_copy' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Copy Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"source_field\": {\n",
      "    \"nested\": {\n",
      "      \"value\": \"important_data\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"source_field\": {\n",
      "    \"nested\": {\n",
      "      \"value\": \"important_data\"\n",
      "    }\n",
      "  },\n",
      "  \"destination_field\": {\n",
      "    \"nested\": {\n",
      "      \"value\": \"important_data\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_copy' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 4: Copy processor\n",
    "sample_doc_copy = {\n",
    "    \"source_field\": {\n",
    "        \"nested\": {\n",
    "            \"value\": \"important_data\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "pipeline_copy = [\n",
    "    {\n",
    "        \"copy\": {\n",
    "            \"source_field\": \"source_field\",\n",
    "            \"target_field\": \"destination_field\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_copy\", pipeline_copy, \"Copy fields\")\n",
    "result_copy = test_pipeline(\"demo_copy\", sample_doc_copy)\n",
    "\n",
    "if result_copy:\n",
    "    original = sample_doc_copy\n",
    "    processed = result_copy['docs'][0]['doc']['_source']\n",
    "    display_result(\"Copy Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_copy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ea623",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ CSV Processor\n",
    "\n",
    "## Purpose\n",
    "Extracts CSV-formatted data into individual fields\n",
    "\n",
    "## Use Cases\n",
    "- Parsing CSV data embedded in fields\n",
    "- Splitting delimited strings into fields\n",
    "- Data extraction from structured text\n",
    "\n",
    "## Configuration\n",
    "- **field**: Field containing CSV data\n",
    "- **target_fields**: Array of field names for extracted values\n",
    "- **separator**: CSV separator character (default: ,)\n",
    "- **trim**: Trim whitespace (default: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dafd61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_csv' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä CSV Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"csv_data\": \"John,Doe,john@example.com,35\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"csv_data\": \"John,Doe,john@example.com,35\",\n",
      "  \"last_name\": \"Doe\",\n",
      "  \"first_name\": \"John\",\n",
      "  \"email\": \"john@example.com\",\n",
      "  \"age\": \"35\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_csv' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 5: CSV processor\n",
    "sample_doc_csv = {\n",
    "    \"csv_data\": \"John,Doe,john@example.com,35\"\n",
    "}\n",
    "\n",
    "pipeline_csv = [\n",
    "    {\n",
    "        \"csv\": {\n",
    "            \"field\": \"csv_data\",\n",
    "            \"target_fields\": [\"first_name\", \"last_name\", \"email\", \"age\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_csv\", pipeline_csv, \"Parse CSV data\")\n",
    "result_csv = test_pipeline(\"demo_csv\", sample_doc_csv)\n",
    "\n",
    "if result_csv:\n",
    "    original = sample_doc_csv\n",
    "    processed = result_csv['docs'][0]['doc']['_source']\n",
    "    display_result(\"CSV Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e170dd",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ DATE Processor\n",
    "\n",
    "## Purpose\n",
    "Parses and normalizes date fields into a standardized format\n",
    "\n",
    "## Use Cases\n",
    "- Normalizing timestamp formats from various sources\n",
    "- Converting date strings to ISO format\n",
    "- Time-based indexing and analysis\n",
    "\n",
    "## Configuration\n",
    "- **field**: Field containing the date value\n",
    "- **target_field**: Field to store normalized date (default: replaces source field)\n",
    "- **formats**: Array of possible input date formats\n",
    "- **timezone**: Timezone for parsing (default: UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e65cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_date' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Date Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"log_timestamp\": \"2024-11-02 14:30:45\",\n",
      "  \"event_date\": \"02/11/2024\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"log_timestamp\": \"2024-11-02 14:30:45\",\n",
      "  \"normalized_event_date\": \"2024-11-02T00:00:00.000Z\",\n",
      "  \"@timestamp\": \"2024-11-02T14:30:45.000Z\",\n",
      "  \"event_date\": \"02/11/2024\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_date' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 6: Date processor\n",
    "sample_doc_date = {\n",
    "    \"log_timestamp\": \"2024-11-02 14:30:45\",\n",
    "    \"event_date\": \"02/11/2024\"\n",
    "}\n",
    "\n",
    "pipeline_date = [\n",
    "    {\n",
    "        \"date\": {\n",
    "            \"field\": \"log_timestamp\",\n",
    "            \"target_field\": \"@timestamp\",\n",
    "            \"formats\": [\"yyyy-MM-dd HH:mm:ss\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"date\": {\n",
    "            \"field\": \"event_date\",\n",
    "            \"formats\": [\"dd/MM/yyyy\"],\n",
    "            \"target_field\": \"normalized_event_date\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_date\", pipeline_date, \"Parse and normalize dates\")\n",
    "result_date = test_pipeline(\"demo_date\", sample_doc_date)\n",
    "\n",
    "if result_date:\n",
    "    original = sample_doc_date\n",
    "    processed = result_date['docs'][0]['doc']['_source']\n",
    "    display_result(\"Date Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f15444",
   "metadata": {},
   "source": [
    "# 7Ô∏è‚É£ GROK Processor\n",
    "\n",
    "## Purpose\n",
    "Extracts structured fields from unstructured text using regular expressions\n",
    "\n",
    "## Use Cases\n",
    "- Parsing system logs and application logs\n",
    "- Extracting fields from Apache/Nginx logs\n",
    "- Text pattern matching and field extraction\n",
    "\n",
    "## Configuration\n",
    "- **field**: Field containing unstructured text\n",
    "- **patterns**: Array of grok patterns to match\n",
    "- **pattern_definitions**: Custom pattern definitions\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)\n",
    "\n",
    "## Common Grok Patterns\n",
    "- %{IP:ip_address}\n",
    "- %{WORD:word}\n",
    "- %{NUMBER:number}\n",
    "- %{HTTPDATE:timestamp}\n",
    "- %{DATA:data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c183a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_grok' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Grok Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"apache_log\": \"192.168.1.1 - user [02/Nov/2024:14:30:45 +0000] \\\"GET /api/users HTTP/1.1\\\" 200 1234\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"apache_log\": \"192.168.1.1 - user [02/Nov/2024:14:30:45 +0000] \\\"GET /api/users HTTP/1.1\\\" 200 1234\",\n",
      "  \"path\": \"/api/users\",\n",
      "  \"status_code\": 200,\n",
      "  \"method\": \"GET\",\n",
      "  \"bytes\": 1234,\n",
      "  \"http_version\": \"1.1\",\n",
      "  \"client_ip\": \"192.168.1.1\",\n",
      "  \"username\": \"user\",\n",
      "  \"timestamp\": \"02/Nov/2024:14:30:45 +0000\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_grok' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 7: Grok processor\n",
    "sample_doc_grok = {\n",
    "    \"apache_log\": \"192.168.1.1 - user [02/Nov/2024:14:30:45 +0000] \\\"GET /api/users HTTP/1.1\\\" 200 1234\"\n",
    "}\n",
    "\n",
    "pipeline_grok = [\n",
    "    {\n",
    "        \"grok\": {\n",
    "            \"field\": \"apache_log\",\n",
    "            \"patterns\": [\"%{IP:client_ip} - %{DATA:username} \\\\[%{HTTPDATE:timestamp}\\\\] \\\"%{WORD:method} %{DATA:path} HTTP/%{NUMBER:http_version}\\\" %{NUMBER:status_code:int} %{NUMBER:bytes:int}\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_grok\", pipeline_grok, \"Parse Apache logs with grok\")\n",
    "result_grok = test_pipeline(\"demo_grok\", sample_doc_grok)\n",
    "\n",
    "if result_grok:\n",
    "    original = sample_doc_grok\n",
    "    processed = result_grok['docs'][0]['doc']['_source']\n",
    "    display_result(\"Grok Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_grok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2df710",
   "metadata": {},
   "source": [
    "# 8Ô∏è‚É£ JSON Processor\n",
    "\n",
    "## Purpose\n",
    "Parses JSON strings into structured objects\n",
    "\n",
    "## Use Cases\n",
    "- Converting JSON strings into objects\n",
    "- Extracting nested data from JSON strings\n",
    "- Data enrichment from JSON payloads\n",
    "\n",
    "## Configuration\n",
    "- **field**: Field containing JSON string\n",
    "- **target_field**: Field to store parsed JSON (default: overwrites source)\n",
    "- **add_to_root**: Add parsed fields to document root (default: false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c63078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_json' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä JSON Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"metadata_json\": \"{\\\"author\\\": \\\"John\\\", \\\"version\\\": \\\"1.0\\\", \\\"tags\\\": [\\\"important\\\", \\\"reviewed\\\"]}\",\n",
      "  \"config\": \"{\\\"timeout\\\": 30, \\\"retry\\\": true}\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"version\": \"1.0\",\n",
      "    \"author\": \"John\",\n",
      "    \"tags\": [\n",
      "      \"important\",\n",
      "      \"reviewed\"\n",
      "    ]\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"timeout\": 30,\n",
      "    \"retry\": true\n",
      "  },\n",
      "  \"metadata_json\": \"{\\\"author\\\": \\\"John\\\", \\\"version\\\": \\\"1.0\\\", \\\"tags\\\": [\\\"important\\\", \\\"reviewed\\\"]}\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_json' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 8: JSON processor\n",
    "sample_doc_json = {\n",
    "    \"metadata_json\": '{\"author\": \"John\", \"version\": \"1.0\", \"tags\": [\"important\", \"reviewed\"]}',\n",
    "    \"config\": '{\"timeout\": 30, \"retry\": true}'\n",
    "}\n",
    "\n",
    "pipeline_json = [\n",
    "    {\n",
    "        \"json\": {\n",
    "            \"field\": \"metadata_json\",\n",
    "            \"target_field\": \"metadata\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"json\": {\n",
    "            \"field\": \"config\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_json\", pipeline_json, \"Parse JSON strings\")\n",
    "result_json = test_pipeline(\"demo_json\", sample_doc_json)\n",
    "\n",
    "if result_json:\n",
    "    original = sample_doc_json\n",
    "    processed = result_json['docs'][0]['doc']['_source']\n",
    "    display_result(\"JSON Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096a6cf",
   "metadata": {},
   "source": [
    "# 9Ô∏è‚É£ SPLIT & JOIN Processors\n",
    "\n",
    "## SPLIT Processor\n",
    "\n",
    "### Purpose\n",
    "Splits a string into an array using a separator\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field containing string to split\n",
    "- **separator**: Separator character/string\n",
    "- **target_field**: Field to store result (default: overwrites source)\n",
    "\n",
    "## JOIN Processor\n",
    "\n",
    "### Purpose  \n",
    "Joins array elements into a single string\n",
    "\n",
    "### Configuration\n",
    "- **field**: Array field to join\n",
    "- **separator**: Separator to use between elements\n",
    "- **target_field**: Field to store result (default: overwrites source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16fc7490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_split' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Split Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"tags_string\": \"python,machine-learning,data-science\",\n",
      "  \"categories\": \"books;fiction;bestsellers\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"categories\": [\n",
      "    \"books\",\n",
      "    \"fiction\",\n",
      "    \"bestsellers\"\n",
      "  ],\n",
      "  \"tags_string\": [\n",
      "    \"python\",\n",
      "    \"machine-learning\",\n",
      "    \"data-science\"\n",
      "  ]\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_split' deleted\n",
      "‚úÖ Pipeline 'demo_join' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Join Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"tags\": [\n",
      "    \"machine-learning\",\n",
      "    \"ai\",\n",
      "    \"data-science\"\n",
      "  ],\n",
      "  \"categories\": [\n",
      "    \"tech\",\n",
      "    \"education\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"categories\": [\n",
      "    \"tech\",\n",
      "    \"education\"\n",
      "  ],\n",
      "  \"tags\": \"machine-learning | ai | data-science\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_join' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 9a: Split processor\n",
    "sample_doc_split = {\n",
    "    \"tags_string\": \"python,machine-learning,data-science\",\n",
    "    \"categories\": \"books;fiction;bestsellers\"\n",
    "}\n",
    "\n",
    "pipeline_split = [\n",
    "    {\n",
    "        \"split\": {\n",
    "            \"field\": \"tags_string\",\n",
    "            \"separator\": \",\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"split\": {\n",
    "            \"field\": \"categories\",\n",
    "            \"separator\": \";\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_split\", pipeline_split, \"Split strings into arrays\")\n",
    "result_split = test_pipeline(\"demo_split\", sample_doc_split)\n",
    "\n",
    "if result_split:\n",
    "    original = sample_doc_split\n",
    "    processed = result_split['docs'][0]['doc']['_source']\n",
    "    display_result(\"Split Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_split\")\n",
    "\n",
    "# Example 9b: Join processor\n",
    "sample_doc_join = {\n",
    "    \"tags\": [\"machine-learning\", \"ai\", \"data-science\"],\n",
    "    \"categories\": [\"tech\", \"education\"]\n",
    "}\n",
    "\n",
    "pipeline_join = [\n",
    "    {\n",
    "        \"join\": {\n",
    "            \"field\": \"tags\",\n",
    "            \"separator\": \" | \"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_join\", pipeline_join, \"Join arrays into strings\")\n",
    "result_join = test_pipeline(\"demo_join\", sample_doc_join)\n",
    "\n",
    "if result_join:\n",
    "    original = sample_doc_join\n",
    "    processed = result_join['docs'][0]['doc']['_source']\n",
    "    display_result(\"Join Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_join\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5d44a",
   "metadata": {},
   "source": [
    "# üîü STRING MANIPULATION Processors\n",
    "\n",
    "## LOWERCASE, UPPERCASE & TRIM\n",
    "\n",
    "### LOWERCASE Processor\n",
    "- **Purpose**: Convert text to lowercase\n",
    "- **Use Case**: Normalize keywords and tags\n",
    "- **Configuration**: Just specify the field\n",
    "\n",
    "### UPPERCASE Processor\n",
    "- **Purpose**: Convert text to uppercase\n",
    "- **Use Case**: Normalize tags or keywords\n",
    "- **Configuration**: Just specify the field\n",
    "\n",
    "### TRIM Processor\n",
    "- **Purpose**: Remove leading/trailing whitespace\n",
    "- **Use Case**: Clean string fields\n",
    "- **Configuration**: Just specify the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "071fad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_string_ops' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä String Manipulation Processors\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"category\": \"ELECTRONICS\",\n",
      "  \"brand\": \"APPLE\",\n",
      "  \"country\": \"united states\",\n",
      "  \"status\": \"active\",\n",
      "  \"name\": \"  John Doe  \",\n",
      "  \"email\": \"   user@example.com   \"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"country\": \"UNITED STATES\",\n",
      "  \"name\": \"John Doe\",\n",
      "  \"category\": \"electronics\",\n",
      "  \"brand\": \"apple\",\n",
      "  \"email\": \"user@example.com\",\n",
      "  \"status\": \"ACTIVE\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_string_ops' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 10: String manipulation processors\n",
    "sample_doc_string = {\n",
    "    \"category\": \"ELECTRONICS\",\n",
    "    \"brand\": \"APPLE\",\n",
    "    \"country\": \"united states\",\n",
    "    \"status\": \"active\",\n",
    "    \"name\": \"  John Doe  \",\n",
    "    \"email\": \"   user@example.com   \"\n",
    "}\n",
    "\n",
    "pipeline_string = [\n",
    "    {\n",
    "        \"lowercase\": {\n",
    "            \"field\": \"category\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"lowercase\": {\n",
    "            \"field\": \"brand\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"uppercase\": {\n",
    "            \"field\": \"country\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"uppercase\": {\n",
    "            \"field\": \"status\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"trim\": {\n",
    "            \"field\": \"name\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"trim\": {\n",
    "            \"field\": \"email\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_string_ops\", pipeline_string, \"String manipulation\")\n",
    "result_string = test_pipeline(\"demo_string_ops\", sample_doc_string)\n",
    "\n",
    "if result_string:\n",
    "    original = sample_doc_string\n",
    "    processed = result_string['docs'][0]['doc']['_source']\n",
    "    display_result(\"String Manipulation Processors\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_string_ops\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35262629",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£1Ô∏è‚É£ REMOVE & RENAME Processors\n",
    "\n",
    "## REMOVE Processor\n",
    "\n",
    "### Purpose\n",
    "Removes one or more fields from documents\n",
    "\n",
    "### Use Cases\n",
    "- Removing sensitive data before indexing\n",
    "- Filtering out unwanted fields\n",
    "- Privacy and security compliance\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field(s) to remove (string or array)\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)\n",
    "\n",
    "## RENAME Processor\n",
    "\n",
    "### Purpose\n",
    "Renames one or more fields\n",
    "\n",
    "### Use Cases\n",
    "- Renaming fields for consistency\n",
    "- Field name transformations\n",
    "- Schema normalization\n",
    "\n",
    "### Configuration\n",
    "- **field**: Original field name\n",
    "- **target_field**: New field name\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56f137e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_remove' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Remove Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"public_data\": \"visible\",\n",
      "  \"secret_token\": \"xxxxx\",\n",
      "  \"internal_id\": \"12345\",\n",
      "  \"useful_data\": \"keep this\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"public_data\": \"visible\",\n",
      "  \"useful_data\": \"keep this\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_remove' deleted\n",
      "‚úÖ Pipeline 'demo_rename' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Rename Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"provider\": \"AWS\",\n",
      "  \"instance_type\": \"t2.micro\",\n",
      "  \"region_name\": \"us-east-1\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"cloud\": {\n",
      "    \"provider\": \"AWS\",\n",
      "    \"instance\": {\n",
      "      \"type\": \"t2.micro\"\n",
      "    }\n",
      "  },\n",
      "  \"region_name\": \"us-east-1\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_rename' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 11a: Remove processor\n",
    "sample_doc_remove = {\n",
    "    \"public_data\": \"visible\",\n",
    "    \"secret_token\": \"xxxxx\",\n",
    "    \"internal_id\": \"12345\",\n",
    "    \"useful_data\": \"keep this\"\n",
    "}\n",
    "\n",
    "pipeline_remove = [\n",
    "    {\n",
    "        \"remove\": {\n",
    "            \"field\": [\"secret_token\", \"internal_id\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_remove\", pipeline_remove, \"Remove sensitive fields\")\n",
    "result_remove = test_pipeline(\"demo_remove\", sample_doc_remove)\n",
    "\n",
    "if result_remove:\n",
    "    original = sample_doc_remove\n",
    "    processed = result_remove['docs'][0]['doc']['_source']\n",
    "    display_result(\"Remove Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_remove\")\n",
    "\n",
    "# Example 11b: Rename processor\n",
    "sample_doc_rename = {\n",
    "    \"provider\": \"AWS\",\n",
    "    \"instance_type\": \"t2.micro\",\n",
    "    \"region_name\": \"us-east-1\"\n",
    "}\n",
    "\n",
    "pipeline_rename = [\n",
    "    {\n",
    "        \"rename\": {\n",
    "            \"field\": \"provider\",\n",
    "            \"target_field\": \"cloud.provider\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"rename\": {\n",
    "            \"field\": \"instance_type\",\n",
    "            \"target_field\": \"cloud.instance.type\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_rename\", pipeline_rename, \"Rename fields\")\n",
    "result_rename = test_pipeline(\"demo_rename\", sample_doc_rename)\n",
    "\n",
    "if result_rename:\n",
    "    original = sample_doc_rename\n",
    "    processed = result_rename['docs'][0]['doc']['_source']\n",
    "    display_result(\"Rename Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_rename\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454a0ba",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£2Ô∏è‚É£ SET & GSUB Processors\n",
    "\n",
    "## SET Processor\n",
    "\n",
    "### Purpose\n",
    "Sets a field to a constant or template value\n",
    "\n",
    "### Use Cases\n",
    "- Adding default values to documents\n",
    "- Adding metadata fields\n",
    "- Timestamp enrichment\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field to set\n",
    "- **value**: Value to set (supports Painless templates like {{_ingest.timestamp}})\n",
    "\n",
    "## GSUB Processor\n",
    "\n",
    "### Purpose\n",
    "Substitutes or deletes substrings in a field\n",
    "\n",
    "### Use Cases\n",
    "- Text normalization and cleaning\n",
    "- Replacing patterns in strings\n",
    "- Data standardization\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field to process\n",
    "- **pattern**: Regex pattern to match\n",
    "- **replacement**: Replacement string (empty = delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc6838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_set' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Set Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"product\": \"Laptop\",\n",
      "  \"price\": 999\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"product\": \"Laptop\",\n",
      "  \"ingestion_timestamp\": \"2025-12-27T20:13:44.226390104Z\",\n",
      "  \"price\": 999,\n",
      "  \"data_source\": \"web_api\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_set' deleted\n",
      "‚úÖ Pipeline 'demo_gsub' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Gsub Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"text\": \"Hello-World-Test-String\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"text\": \"Hello World Test String\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_gsub' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 12a: Set processor\n",
    "sample_doc_set = {\n",
    "    \"product\": \"Laptop\",\n",
    "    \"price\": 999\n",
    "}\n",
    "\n",
    "pipeline_set = [\n",
    "    {\n",
    "        \"set\": {\n",
    "            \"field\": \"data_source\",\n",
    "            \"value\": \"web_api\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"set\": {\n",
    "            \"field\": \"ingestion_timestamp\",\n",
    "            \"value\": \"{{_ingest.timestamp}}\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_set\", pipeline_set, \"Set constant values\")\n",
    "result_set = test_pipeline(\"demo_set\", sample_doc_set)\n",
    "\n",
    "if result_set:\n",
    "    original = sample_doc_set\n",
    "    processed = result_set['docs'][0]['doc']['_source']\n",
    "    display_result(\"Set Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_set\")\n",
    "\n",
    "# Example 12b: Gsub processor\n",
    "sample_doc_gsub = {\n",
    "    \"text\": \"Hello-World-Test-String\"\n",
    "}\n",
    "\n",
    "pipeline_gsub = [\n",
    "    {\n",
    "        \"gsub\": {\n",
    "            \"field\": \"text\",\n",
    "            \"pattern\": \"-\",\n",
    "            \"replacement\": \" \"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_gsub\", pipeline_gsub, \"Substitute substrings\")\n",
    "result_gsub = test_pipeline(\"demo_gsub\", sample_doc_gsub)\n",
    "\n",
    "if result_gsub:\n",
    "    original = sample_doc_gsub\n",
    "    processed = result_gsub['docs'][0]['doc']['_source']\n",
    "    display_result(\"Gsub Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_gsub\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e47f07",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£3Ô∏è‚É£ HTML_STRIP & URLDECODE Processors\n",
    "\n",
    "## HTML_STRIP Processor\n",
    "\n",
    "### Purpose\n",
    "Removes HTML tags from text fields\n",
    "\n",
    "### Use Cases\n",
    "- Cleaning HTML content before indexing\n",
    "- Removing formatting tags\n",
    "- Text extraction from HTML\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field containing HTML\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)\n",
    "\n",
    "## URLDECODE Processor\n",
    "\n",
    "### Purpose\n",
    "Decodes URL-encoded strings\n",
    "\n",
    "### Use Cases\n",
    "- Cleaning URL parameters\n",
    "- Decoding search queries\n",
    "- URL field normalization\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field containing URL-encoded string\n",
    "- **ignore_missing**: Don't fail if field is missing (default: false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18875a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_html_strip' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä HTML Strip Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"content\": \"<p>This is <b>bold</b> and <i>italic</i> text</p>\",\n",
      "  \"description\": \"<div class=\\\"desc\\\">Product <span>description</span></div>\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"description\": \"\\nProduct description\\n\",\n",
      "  \"content\": \"\\nThis is bold and italic text\\n\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_html_strip' deleted\n",
      "‚úÖ Pipeline 'demo_urldecode' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä URL Decode Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"encoded_url\": \"hello%20world%21\",\n",
      "  \"search_query\": \"machine%20learning%20algorithms\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"search_query\": \"machine learning algorithms\",\n",
      "  \"encoded_url\": \"hello world!\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_urldecode' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 13a: HTML Strip processor\n",
    "sample_doc_html = {\n",
    "    \"content\": \"<p>This is <b>bold</b> and <i>italic</i> text</p>\",\n",
    "    \"description\": \"<div class=\\\"desc\\\">Product <span>description</span></div>\"\n",
    "}\n",
    "\n",
    "pipeline_html = [\n",
    "    {\n",
    "        \"html_strip\": {\n",
    "            \"field\": \"content\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"html_strip\": {\n",
    "            \"field\": \"description\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_html_strip\", pipeline_html, \"Strip HTML tags\")\n",
    "result_html = test_pipeline(\"demo_html_strip\", sample_doc_html)\n",
    "\n",
    "if result_html:\n",
    "    original = sample_doc_html\n",
    "    processed = result_html['docs'][0]['doc']['_source']\n",
    "    display_result(\"HTML Strip Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_html_strip\")\n",
    "\n",
    "# Example 13b: URL Decode processor\n",
    "sample_doc_urldecode = {\n",
    "    \"encoded_url\": \"hello%20world%21\",\n",
    "    \"search_query\": \"machine%20learning%20algorithms\"\n",
    "}\n",
    "\n",
    "pipeline_urldecode = [\n",
    "    {\n",
    "        \"urldecode\": {\n",
    "            \"field\": \"encoded_url\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"urldecode\": {\n",
    "            \"field\": \"search_query\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_urldecode\", pipeline_urldecode, \"Decode URL-encoded strings\")\n",
    "result_urldecode = test_pipeline(\"demo_urldecode\", sample_doc_urldecode)\n",
    "\n",
    "if result_urldecode:\n",
    "    original = sample_doc_urldecode\n",
    "    processed = result_urldecode['docs'][0]['doc']['_source']\n",
    "    display_result(\"URL Decode Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_urldecode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9232c",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£4Ô∏è‚É£ SORT & FINGERPRINT Processors\n",
    "\n",
    "## SORT Processor\n",
    "\n",
    "### Purpose\n",
    "Sorts array elements in ascending or descending order\n",
    "\n",
    "### Use Cases\n",
    "- Ordering array elements in documents\n",
    "- Standardizing array order\n",
    "- Data preparation\n",
    "\n",
    "### Configuration\n",
    "- **field**: Array field to sort\n",
    "- **order**: Sort order (ascending/descending, default: ascending)\n",
    "\n",
    "## FINGERPRINT Processor\n",
    "\n",
    "### Purpose\n",
    "Generates hash fingerprints of specified fields for deduplication\n",
    "\n",
    "### Use Cases\n",
    "- Deduplicating documents\n",
    "- Generating unique identifiers\n",
    "- Document matching\n",
    "\n",
    "### Configuration\n",
    "- **fields**: Array of fields to fingerprint\n",
    "- **target_field**: Field to store fingerprint (default: fingerprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c324d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_sort' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Sort Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"scores\": [\n",
      "    95,\n",
      "    42,\n",
      "    87,\n",
      "    23,\n",
      "    100\n",
      "  ],\n",
      "  \"names\": [\n",
      "    \"Zoe\",\n",
      "    \"Alice\",\n",
      "    \"Bob\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"names\": [\n",
      "    \"Alice\",\n",
      "    \"Bob\",\n",
      "    \"Zoe\"\n",
      "  ],\n",
      "  \"scores\": [\n",
      "    23,\n",
      "    42,\n",
      "    87,\n",
      "    95,\n",
      "    100\n",
      "  ]\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_sort' deleted\n",
      "‚úÖ Pipeline 'demo_fingerprint' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Fingerprint Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"email\": \"user@example.com\",\n",
      "  \"name\": \"John Doe\",\n",
      "  \"phone\": \"555-1234\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"phone\": \"555-1234\",\n",
      "  \"email\": \"user@example.com\",\n",
      "  \"document_fingerprint\": \"SHA-1@2.16.0:BJMPv+aO2tEAI4aGLbXfmfvsluM=\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_fingerprint' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 14a: Sort processor\n",
    "sample_doc_sort = {\n",
    "    \"scores\": [95, 42, 87, 23, 100],\n",
    "    \"names\": [\"Zoe\", \"Alice\", \"Bob\"]\n",
    "}\n",
    "\n",
    "pipeline_sort = [\n",
    "    {\n",
    "        \"sort\": {\n",
    "            \"field\": \"scores\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"sort\": {\n",
    "            \"field\": \"names\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_sort\", pipeline_sort, \"Sort arrays\")\n",
    "result_sort = test_pipeline(\"demo_sort\", sample_doc_sort)\n",
    "\n",
    "if result_sort:\n",
    "    original = sample_doc_sort\n",
    "    processed = result_sort['docs'][0]['doc']['_source']\n",
    "    display_result(\"Sort Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_sort\")\n",
    "\n",
    "# Example 14b: Fingerprint processor\n",
    "sample_doc_fingerprint = {\n",
    "    \"email\": \"user@example.com\",\n",
    "    \"name\": \"John Doe\",\n",
    "    \"phone\": \"555-1234\"\n",
    "}\n",
    "\n",
    "pipeline_fingerprint = [\n",
    "    {\n",
    "        \"fingerprint\": {\n",
    "            \"fields\": [\"email\", \"name\"],\n",
    "            \"target_field\": \"document_fingerprint\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_fingerprint\", pipeline_fingerprint, \"Generate fingerprints\")\n",
    "result_fingerprint = test_pipeline(\"demo_fingerprint\", sample_doc_fingerprint)\n",
    "\n",
    "if result_fingerprint:\n",
    "    original = sample_doc_fingerprint\n",
    "    processed = result_fingerprint['docs'][0]['doc']['_source']\n",
    "    display_result(\"Fingerprint Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_fingerprint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbce44",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£5Ô∏è‚É£ KV & DISSECT Processors\n",
    "\n",
    "## KV (Key-Value) Processor\n",
    "\n",
    "### Purpose\n",
    "Parses key-value pairs into separate fields\n",
    "\n",
    "### Use Cases\n",
    "- Parsing query strings\n",
    "- Extracting log parameters\n",
    "- Structured data extraction from strings\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field containing key-value data\n",
    "- **field_split**: Separator between key-value pairs\n",
    "- **value_split**: Separator between keys and values\n",
    "- **target_field**: Field to store result (default: adds to root)\n",
    "\n",
    "## DISSECT Processor\n",
    "\n",
    "### Purpose\n",
    "Extracts fields from structured text using text patterns\n",
    "\n",
    "### Use Cases\n",
    "- Parsing system logs with known patterns\n",
    "- Structured log extraction\n",
    "- Pattern-based text parsing\n",
    "\n",
    "### Configuration\n",
    "- **field**: Field containing text\n",
    "- **pattern**: Dissect pattern (e.g., \"%{FIELD1} %{FIELD2}\")\n",
    "- **append_separator**: Separator for multi-value fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a85bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_kv' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä KV Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"query_string\": \"user_id=123&action=login&timestamp=1234567890\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"parsed_params\": {\n",
      "    \"user_id\": \"123\",\n",
      "    \"action\": \"login\",\n",
      "    \"timestamp\": \"1234567890\"\n",
      "  },\n",
      "  \"query_string\": \"user_id=123&action=login&timestamp=1234567890\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_kv' deleted\n",
      "‚úÖ Pipeline 'demo_dissect' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Dissect Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"log_line\": \"2024-11-02 14:30:45 ERROR app.py:42 Database connection failed\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"MESSAGE\": \"app.py:42 Database connection failed\",\n",
      "  \"TIMESTAMP\": \"2024-11-02\",\n",
      "  \"SOURCE\": \"ERROR\",\n",
      "  \"LEVEL\": \"14:30:45\",\n",
      "  \"log_line\": \"2024-11-02 14:30:45 ERROR app.py:42 Database connection failed\"\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_dissect' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 15a: KV processor\n",
    "sample_doc_kv = {\n",
    "    \"query_string\": \"user_id=123&action=login&timestamp=1234567890\"\n",
    "}\n",
    "\n",
    "pipeline_kv = [\n",
    "    {\n",
    "        \"kv\": {\n",
    "            \"field\": \"query_string\",\n",
    "            \"field_split\": \"&\",\n",
    "            \"value_split\": \"=\",\n",
    "            \"target_field\": \"parsed_params\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_kv\", pipeline_kv, \"Parse key-value pairs\")\n",
    "result_kv = test_pipeline(\"demo_kv\", sample_doc_kv)\n",
    "\n",
    "if result_kv:\n",
    "    original = sample_doc_kv\n",
    "    processed = result_kv['docs'][0]['doc']['_source']\n",
    "    display_result(\"KV Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_kv\")\n",
    "\n",
    "# Example 15b: Dissect processor\n",
    "sample_doc_dissect = {\n",
    "    \"log_line\": \"2024-11-02 14:30:45 ERROR app.py:42 Database connection failed\"\n",
    "}\n",
    "\n",
    "pipeline_dissect = [\n",
    "    {\n",
    "        \"dissect\": {\n",
    "            \"field\": \"log_line\",\n",
    "            \"pattern\": \"%{TIMESTAMP} %{LEVEL} %{SOURCE} %{MESSAGE}\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_dissect\", pipeline_dissect, \"Parse structured logs\")\n",
    "result_dissect = test_pipeline(\"demo_dissect\", sample_doc_dissect)\n",
    "\n",
    "if result_dissect:\n",
    "    original = sample_doc_dissect\n",
    "    processed = result_dissect['docs'][0]['doc']['_source']\n",
    "    display_result(\"Dissect Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_dissect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3a4e2",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£6Ô∏è‚É£ CONTROL FLOW: DROP & FAIL\n",
    "\n",
    "## DROP Processor\n",
    "\n",
    "### Purpose\n",
    "Drops/filters documents from indexing based on conditions\n",
    "\n",
    "### Use Cases\n",
    "- Filtering unwanted documents\n",
    "- Conditional document exclusion\n",
    "- Data validation\n",
    "\n",
    "### Configuration\n",
    "- **if**: Condition to check (Painless script)\n",
    "\n",
    "## FAIL Processor\n",
    "\n",
    "### Purpose\n",
    "Raises an error and stops pipeline execution\n",
    "\n",
    "### Use Cases\n",
    "- Validation failure scenarios\n",
    "- Data quality checks\n",
    "- Pipeline error handling\n",
    "\n",
    "### Configuration\n",
    "- **if**: Condition to trigger failure\n",
    "- **message**: Error message\n",
    "- **message_fields**: Fields to include in message\n",
    "\n",
    "## Example Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Document Arrives\"] --> B{DROP Condition?}\n",
    "    B -->|True| C[\"üóëÔ∏è Document Dropped\"]\n",
    "    B -->|False| D{FAIL Condition?}\n",
    "    D -->|True| E[\"‚ùå Pipeline Failed\"]\n",
    "    D -->|False| F[\"‚úÖ Continue Processing\"]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#ffcdd2\n",
    "    style E fill:#ffcdd2\n",
    "    style F fill:#c8e6c9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b6005b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä DROP Processor Example\n",
      "============================================================\n",
      "‚úÖ Pipeline 'demo_drop' created successfully\n",
      "\n",
      "üìÑ Document 1: draft\n",
      "   ‚úÖ Document was DROPPED\n",
      "\n",
      "üìÑ Document 2: published\n",
      "   ‚úÖ Document passed through\n",
      "   Result: {'content': 'This is published', 'status': 'published'}\n",
      "üóëÔ∏è  Pipeline 'demo_drop' deleted\n",
      "\n",
      "============================================================\n",
      "üìä FAIL Processor Example\n",
      "============================================================\n",
      "‚úÖ Pipeline 'demo_fail' created successfully\n",
      "\n",
      "Testing document with negative price...\n",
      "‚ùå Pipeline error (expected): {'root_cause': [{'type': 'fail_processor_exception', 'reason': 'Price cannot be negative'}], 'type': 'fail_processor_exception', 'reason': 'Price cannot be negative'}\n",
      "üóëÔ∏è  Pipeline 'demo_fail' deleted\n"
     ]
    }
   ],
   "source": [
    "# Example 16a: Drop processor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DROP Processor Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_doc_drop_1 = {\"status\": \"draft\", \"content\": \"This is a draft article\"}\n",
    "sample_doc_drop_2 = {\"status\": \"published\", \"content\": \"This is published\"}\n",
    "\n",
    "pipeline_drop = [\n",
    "    {\n",
    "        \"drop\": {\n",
    "            \"if\": \"ctx.status == 'draft'\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_drop\", pipeline_drop, \"Drop draft documents\")\n",
    "\n",
    "# Test both documents\n",
    "for i, doc in enumerate([sample_doc_drop_1, sample_doc_drop_2], 1):\n",
    "    print(f\"\\nüìÑ Document {i}: {doc['status']}\")\n",
    "    result_drop = test_pipeline(\"demo_drop\", doc)\n",
    "    if result_drop:\n",
    "        doc_entry = result_drop['docs'][0]\n",
    "        if doc_entry is None or 'skip_action' in doc_entry:\n",
    "            print(f\"   ‚úÖ Document was DROPPED\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Document passed through\")\n",
    "            print(f\"   Result: {doc_entry['doc']['_source']}\")\n",
    "\n",
    "delete_pipeline(\"demo_drop\")\n",
    "\n",
    "# Example 16b: Fail processor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FAIL Processor Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_doc_fail = {\"price\": -100, \"product\": \"Invalid Product\"}\n",
    "\n",
    "pipeline_fail = [\n",
    "    {\n",
    "        \"fail\": {\n",
    "            \"if\": \"ctx.price < 0\",\n",
    "            \"message\": \"Price cannot be negative\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_fail\", pipeline_fail, \"Validate prices\")\n",
    "print(\"\\nTesting document with negative price...\")\n",
    "result_fail = test_pipeline(\"demo_fail\", sample_doc_fail)\n",
    "if result_fail:\n",
    "    if result_fail['docs'][0] and 'error' in result_fail['docs'][0]:\n",
    "        print(f\"‚ùå Pipeline error (expected): {result_fail['docs'][0]['error']}\")\n",
    "    else:\n",
    "        print(\"Result:\", json.dumps(result_fail, indent=2))\n",
    "\n",
    "delete_pipeline(\"demo_fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e4ebd",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£7Ô∏è‚É£ DOT_EXPANDER Processor\n",
    "\n",
    "## Purpose\n",
    "Converts dotted field names into nested objects\n",
    "\n",
    "## Use Cases\n",
    "- Converting flat keys to nested structures\n",
    "- Data structure normalization\n",
    "- Field hierarchy creation\n",
    "\n",
    "## Configuration\n",
    "- **field**: Dotted field name to expand\n",
    "- **path**: Optional nested path\n",
    "\n",
    "## Example Flow\n",
    "```\n",
    "Before:\n",
    "{\n",
    "  \"user.name\": \"John\",\n",
    "  \"user.email\": \"john@example.com\",\n",
    "  \"user.age\": 30\n",
    "}\n",
    "\n",
    "After:\n",
    "{\n",
    "  \"user\": {\n",
    "    \"name\": \"John\",\n",
    "    \"email\": \"john@example.com\",\n",
    "    \"age\": 30\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed4b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_dot_expander' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Dot Expander Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"user.name\": \"John Doe\",\n",
      "  \"user.email\": \"john@example.com\",\n",
      "  \"user.age\": 30\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"user.age\": 30,\n",
      "  \"user\": {\n",
      "    \"name\": \"John Doe\",\n",
      "    \"email\": \"john@example.com\"\n",
      "  }\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_dot_expander' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 17: Dot expander processor\n",
    "sample_doc_dot = {\n",
    "    \"user.name\": \"John Doe\",\n",
    "    \"user.email\": \"john@example.com\",\n",
    "    \"user.age\": 30\n",
    "}\n",
    "\n",
    "pipeline_dot = [\n",
    "    {\n",
    "        \"dot_expander\": {\n",
    "            \"field\": \"user.name\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"dot_expander\": {\n",
    "            \"field\": \"user.email\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_dot_expander\", pipeline_dot, \"Expand dotted fields\")\n",
    "result_dot = test_pipeline(\"demo_dot_expander\", sample_doc_dot)\n",
    "\n",
    "if result_dot:\n",
    "    original = sample_doc_dot\n",
    "    processed = result_dot['docs'][0]['doc']['_source']\n",
    "    display_result(\"Dot Expander Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_dot_expander\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88eb165",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£8Ô∏è‚É£ SCRIPT Processor\n",
    "\n",
    "## Purpose\n",
    "Runs custom Painless scripts for transformations and enrichments\n",
    "\n",
    "## Use Cases\n",
    "- Complex custom transformations\n",
    "- Computed fields\n",
    "- Advanced data enrichment\n",
    "- Conditional logic\n",
    "\n",
    "## Configuration\n",
    "- **source**: Painless script code\n",
    "- **lang**: Language (default: painless)\n",
    "- **params**: Script parameters\n",
    "- **id**: Named script reference\n",
    "\n",
    "## Common Script Patterns\n",
    "```\n",
    "// Calculate total\n",
    "ctx.total = ctx.quantity * ctx.unit_price\n",
    "\n",
    "// Add timestamp\n",
    "ctx.ingestion_time = ctx._ingest.timestamp\n",
    "\n",
    "// Conditional logic\n",
    "if (ctx.status == 'active') {\n",
    "  ctx.priority = 'high';\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c3c8cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_script' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Script Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"quantity\": 5,\n",
      "  \"unit_price\": 20\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"quantity\": 5,\n",
      "  \"unit_price\": 20,\n",
      "  \"total_price\": 100\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_script' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 18: Script processor\n",
    "sample_doc_script = {\n",
    "    \"quantity\": 5,\n",
    "    \"unit_price\": 20\n",
    "}\n",
    "\n",
    "pipeline_script = [\n",
    "    {\n",
    "        \"script\": {\n",
    "            \"source\": \"ctx.total_price = ctx.quantity * ctx.unit_price\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_script\", pipeline_script, \"Run custom script\")\n",
    "result_script = test_pipeline(\"demo_script\", sample_doc_script)\n",
    "\n",
    "if result_script:\n",
    "    original = sample_doc_script\n",
    "    processed = result_script['docs'][0]['doc']['_source']\n",
    "    display_result(\"Script Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_script\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d1611",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£9Ô∏è‚É£ FOREACH & PIPELINE Processors\n",
    "\n",
    "## FOREACH Processor\n",
    "\n",
    "### Purpose\n",
    "Applies another processor to each element of an array\n",
    "\n",
    "### Use Cases\n",
    "- Processing arrays of nested objects\n",
    "- Applying transformations to array elements\n",
    "- Complex nested data handling\n",
    "\n",
    "### Configuration\n",
    "- **field**: Array field to process\n",
    "- **processor**: Processor to apply to each element\n",
    "- **ignore_missing**: Don't fail if field is missing\n",
    "\n",
    "## PIPELINE Processor\n",
    "\n",
    "### Purpose\n",
    "Runs an inner pipeline (composition/nesting)\n",
    "\n",
    "### Use Cases\n",
    "- Creating reusable modular pipelines\n",
    "- Pipeline composition\n",
    "- Complex pipeline management\n",
    "\n",
    "### Configuration\n",
    "- **name**: Name of pipeline to execute\n",
    "- **ignore_missing_pipeline**: Don't fail if pipeline doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4715870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline 'demo_foreach' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä Foreach Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"name\": \"item1\",\n",
      "      \"price\": \"100\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"item2\",\n",
      "      \"price\": \"200\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"name\": \"item1\",\n",
      "      \"price\": 100.0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"item2\",\n",
      "      \"price\": 200.0\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_foreach' deleted\n",
      "\n",
      "============================================================\n",
      "üìä PIPELINE Processor Example\n",
      "============================================================\n",
      "‚úÖ Pipeline 'inner_pipeline' created successfully\n",
      "‚úÖ Pipeline 'outer_pipeline' created successfully\n",
      "\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"status\": \"ACTIVE\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"status\": \"active\"\n",
      "}\n",
      "üóëÔ∏è  Pipeline 'outer_pipeline' deleted\n",
      "üóëÔ∏è  Pipeline 'inner_pipeline' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 19a: Foreach processor\n",
    "sample_doc_foreach = {\n",
    "    \"items\": [\n",
    "        {\"name\": \"item1\", \"price\": \"100\"},\n",
    "        {\"name\": \"item2\", \"price\": \"200\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "pipeline_foreach = [\n",
    "    {\n",
    "        \"foreach\": {\n",
    "            \"field\": \"items\",\n",
    "            \"processor\": {\n",
    "                \"convert\": {\n",
    "                    \"field\": \"_ingest._value.price\",\n",
    "                    \"type\": \"float\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_foreach\", pipeline_foreach, \"Process array elements\")\n",
    "result_foreach = test_pipeline(\"demo_foreach\", sample_doc_foreach)\n",
    "\n",
    "if result_foreach:\n",
    "    original = sample_doc_foreach\n",
    "    processed = result_foreach['docs'][0]['doc']['_source']\n",
    "    display_result(\"Foreach Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_foreach\")\n",
    "\n",
    "# Example 19b: Pipeline processor (nested pipelines)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PIPELINE Processor Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple inner pipeline\n",
    "inner_pipeline = [\n",
    "    {\n",
    "        \"lowercase\": {\n",
    "            \"field\": \"status\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"inner_pipeline\", inner_pipeline, \"Inner pipeline\")\n",
    "\n",
    "# Create outer pipeline that calls inner\n",
    "outer_pipeline = [\n",
    "    {\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"inner_pipeline\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"outer_pipeline\", outer_pipeline, \"Outer pipeline\")\n",
    "\n",
    "sample_doc_pipeline = {\"status\": \"ACTIVE\"}\n",
    "result_pipeline = test_pipeline(\"outer_pipeline\", sample_doc_pipeline)\n",
    "\n",
    "if result_pipeline:\n",
    "    print(\"\\nüì• BEFORE:\")\n",
    "    print(json.dumps(sample_doc_pipeline, indent=2))\n",
    "    print(\"\\nüì§ AFTER:\")\n",
    "    print(json.dumps(result_pipeline['docs'][0]['doc']['_source'], indent=2))\n",
    "\n",
    "delete_pipeline(\"outer_pipeline\")\n",
    "delete_pipeline(\"inner_pipeline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09295aa9",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£0Ô∏è‚É£ GEOLOCATION Processors: GeoIP & IP2Geo\n",
    "\n",
    "## GeoIP Processor\n",
    "\n",
    "### Purpose\n",
    "Adds geolocation information based on IP address\n",
    "\n",
    "### Use Cases\n",
    "- Enriching logs with location data\n",
    "- Tracking user geographic location\n",
    "- Network traffic analysis\n",
    "\n",
    "### Configuration\n",
    "- **field**: IP address field\n",
    "- **target_field**: Field to store geolocation (default: geoip)\n",
    "- **database_file**: Custom GeoIP database\n",
    "\n",
    "## IP2Geo Processor\n",
    "\n",
    "### Purpose\n",
    "Similar to GeoIP, adds geolocation info for IP addresses\n",
    "\n",
    "### Use Cases\n",
    "- Alternative to GeoIP processor\n",
    "- Geographic enrichment\n",
    "\n",
    "## Geolocation Output Format\n",
    "```json\n",
    "{\n",
    "  \"geoip\": {\n",
    "    \"country_iso_code\": \"US\",\n",
    "    \"country_name\": \"United States\",\n",
    "    \"location\": {\n",
    "      \"lat\": 37.386,\n",
    "      \"lon\": -122.084\n",
    "    },\n",
    "    \"city_name\": \"Mountain View\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb24bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä GEOIP Processor Example\n",
      "============================================================\n",
      "‚úÖ Pipeline 'demo_geoip' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä GeoIP Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"client_ip\": \"8.8.8.8\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"client_ip\": \"8.8.8.8\",\n",
      "  \"geoip\": {\n",
      "    \"continent_name\": \"North America\",\n",
      "    \"country_name\": \"United States\",\n",
      "    \"location\": {\n",
      "      \"lon\": -97.822,\n",
      "      \"lat\": 37.751\n",
      "    },\n",
      "    \"country_iso_code\": \"US\"\n",
      "  }\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_geoip' deleted\n",
      "\n",
      "üí° Note: GeoIP databases require setup. The processor works with configured MaxMind databases.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 20: GeoIP processor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä GEOIP Processor Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_doc_geoip = {\n",
    "    \"client_ip\": \"8.8.8.8\"\n",
    "}\n",
    "\n",
    "pipeline_geoip = [\n",
    "    {\n",
    "        \"geoip\": {\n",
    "            \"field\": \"client_ip\",\n",
    "            \"target_field\": \"geoip\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_geoip\", pipeline_geoip, \"Enrich with geolocation\")\n",
    "\n",
    "try:\n",
    "    result_geoip = test_pipeline(\"demo_geoip\", sample_doc_geoip)\n",
    "    if result_geoip and result_geoip['docs']:\n",
    "        if 'error' in result_geoip['docs'][0]:\n",
    "            print(f\"Note: GeoIP database may not be available\")\n",
    "            print(f\"Error: {result_geoip['docs'][0]['error']}\")\n",
    "        else:\n",
    "            original = sample_doc_geoip\n",
    "            processed = result_geoip['docs'][0]['doc']['_source']\n",
    "            display_result(\"GeoIP Processor\", original, processed)\n",
    "except Exception as e:\n",
    "    print(f\"Note: GeoIP processor requires MaxMind database: {e}\")\n",
    "\n",
    "delete_pipeline(\"demo_geoip\")\n",
    "\n",
    "print(\"\\nüí° Note: GeoIP databases require setup. The processor works with configured MaxMind databases.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57122d",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£1Ô∏è‚É£ SPECIALIZED PROCESSORS Summary\n",
    "\n",
    "## Advanced/Specialized Processors Overview\n",
    "\n",
    "The following processors handle specialized use cases not yet covered:\n",
    "\n",
    "### Text & Vector Processing\n",
    "- **text_embedding**: Generates vector embeddings from text (ML models)\n",
    "- **text_image_embedding**: Multimodal embeddings (text + images)\n",
    "- **sparse_encoding**: Sparse vectors for neural search\n",
    "- **text_chunking**: Splits text into chunks for processing\n",
    "\n",
    "### Network & Metadata\n",
    "- **community_id**: Generates network flow hashes\n",
    "- **user_agent**: Extracts browser/device info\n",
    "- **geojson-feature**: Indexes GeoJSON spatial data\n",
    "- **date_index_name**: Auto-indexes to time-based indices\n",
    "- **remove_by_pattern**: Removes fields matching patterns\n",
    "\n",
    "## Quick Reference Matrix\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph Transformation[\"üìù Transformation\"]\n",
    "        T1[\"Append, Set, Join, Split\"]\n",
    "        T2[\"Copy, Rename, Remove\"]\n",
    "    end\n",
    "    \n",
    "    subgraph TextProcessing[\"üìÑ Text Processing\"]\n",
    "        TP1[\"CSV, JSON, Grok, KV\"]\n",
    "        TP2[\"HTML_Strip, Trim, Gsub\"]\n",
    "        TP3[\"Lowercase, Uppercase\"]\n",
    "    end\n",
    "    \n",
    "    subgraph DateGeo[\"üåç Date & Geo\"]\n",
    "        DG1[\"Date, Date_Index_Name\"]\n",
    "        DG2[\"GeoIP, IP2Geo, GeoJSON\"]\n",
    "    end\n",
    "    \n",
    "    subgraph DataTypes[\"üî¢ Data Types\"]\n",
    "        DT1[\"Convert, Bytes, Sort\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Advanced[\"‚öôÔ∏è Advanced\"]\n",
    "        ADV1[\"Script, Foreach, Pipeline\"]\n",
    "        ADV2[\"Drop, Fail, Fingerprint\"]\n",
    "    end\n",
    "    \n",
    "    style Transformation fill:#e3f2fd\n",
    "    style TextProcessing fill:#f3e5f5\n",
    "    style DateGeo fill:#e8f5e9\n",
    "    style DataTypes fill:#fff3e0\n",
    "    style Advanced fill:#fce4ec\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "458e9035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä USER_AGENT Processor Example\n",
      "============================================================\n",
      "‚úÖ Pipeline 'demo_ua' created successfully\n",
      "\n",
      "============================================================\n",
      "üìä User Agent Processor\n",
      "============================================================\n",
      "üì• BEFORE:\n",
      "{\n",
      "  \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
      "}\n",
      "\n",
      "üì§ AFTER:\n",
      "{\n",
      "  \"user_agent\": {\n",
      "    \"name\": \"Chrome\",\n",
      "    \"original\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
      "    \"os\": {\n",
      "      \"name\": \"Windows\",\n",
      "      \"version\": \"10\",\n",
      "      \"full\": \"Windows 10\"\n",
      "    },\n",
      "    \"device\": {\n",
      "      \"name\": \"Other\"\n",
      "    },\n",
      "    \"version\": \"91.0.4472.124\"\n",
      "  }\n",
      "}\n",
      "============================================================\n",
      "\n",
      "üóëÔ∏è  Pipeline 'demo_ua' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example additional processors - User Agent\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä USER_AGENT Processor Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_doc_ua = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "pipeline_ua = [\n",
    "    {\n",
    "        \"user_agent\": {\n",
    "            \"field\": \"user_agent\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"demo_ua\", pipeline_ua, \"Extract user agent info\")\n",
    "result_ua = test_pipeline(\"demo_ua\", sample_doc_ua)\n",
    "\n",
    "if result_ua:\n",
    "    if 'error' in result_ua['docs'][0]:\n",
    "        print(f\"Note: User agent processor details:\")\n",
    "        print(f\"  - Extracts: OS name, OS version, browser name, browser version\")\n",
    "        print(f\"  - Sample output includes device type classification\")\n",
    "    else:\n",
    "        original = sample_doc_ua\n",
    "        processed = result_ua['docs'][0]['doc']['_source']\n",
    "        display_result(\"User Agent Processor\", original, processed)\n",
    "\n",
    "delete_pipeline(\"demo_ua\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa4b67",
   "metadata": {},
   "source": [
    "# üìö COMPREHENSIVE PIPELINE EXAMPLE\n",
    "\n",
    "## Real-World E-Commerce Log Pipeline\n",
    "\n",
    "This example demonstrates a complete pipeline combining multiple processors to handle e-commerce server logs.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"üì• Raw Log\"] --> B[\"Grok\"]\n",
    "    B --> C[\"Date\"]\n",
    "    C --> D[\"GeoIP\"]\n",
    "    D --> E[\"Convert\"]\n",
    "    E --> F[\"Script\"]\n",
    "    F --> G[\"Remove\"]\n",
    "    G --> H[\"‚úÖ Processed\"]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#fff3e0\n",
    "    style E fill:#fff3e0\n",
    "    style F fill:#fff3e0\n",
    "    style G fill:#fff3e0\n",
    "    style H fill:#c8e6c9\n",
    "```\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Grok**: Parse the log line into structured fields\n",
    "2. **Date**: Normalize the timestamp\n",
    "3. **GeoIP**: Enrich with location data\n",
    "4. **Convert**: Convert numeric strings to numbers\n",
    "5. **Script**: Calculate request duration\n",
    "6. **Remove**: Remove sensitive fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52fd57fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä COMPREHENSIVE ECOMMERCE PIPELINE\n",
      "============================================================\n",
      "‚úÖ Pipeline 'ecommerce_pipeline' created successfully\n",
      "\n",
      "üì• INPUT:\n",
      "{\n",
      "  \"raw_log\": \"192.168.1.100 - customer [02/Nov/2024:14:30:45 +0000] \\\"GET /api/products?sort=price HTTP/1.1\\\" 200 5234\",\n",
      "  \"response_time_ms\": \"250\"\n",
      "}\n",
      "\n",
      "üì§ OUTPUT:\n",
      "{\n",
      "  \"response_time_sec\": 0.25,\n",
      "  \"status_code\": 200,\n",
      "  \"method\": \"GET\",\n",
      "  \"http_version\": \"1.1\",\n",
      "  \"path\": \"/api/products?sort=price\",\n",
      "  \"environment\": \"production\",\n",
      "  \"@timestamp\": \"2024-11-02T14:30:45.000Z\",\n",
      "  \"bytes\": 5234,\n",
      "  \"client_ip\": \"192.168.1.100\",\n",
      "  \"username\": \"customer\",\n",
      "  \"timestamp\": \"02/Nov/2024:14:30:45 +0000\"\n",
      "}\n",
      "üóëÔ∏è  Pipeline 'ecommerce_pipeline' deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Comprehensive E-Commerce Pipeline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä COMPREHENSIVE ECOMMERCE PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ecommerce_log = {\n",
    "    \"raw_log\": '192.168.1.100 - customer [02/Nov/2024:14:30:45 +0000] \"GET /api/products?sort=price HTTP/1.1\" 200 5234',\n",
    "    \"response_time_ms\": \"250\"\n",
    "}\n",
    "\n",
    "comprehensive_pipeline = [\n",
    "    {\n",
    "        \"grok\": {\n",
    "            \"field\": \"raw_log\",\n",
    "            \"patterns\": [\"%{IP:client_ip} - %{DATA:username} \\\\[%{HTTPDATE:timestamp}\\\\] \\\"%{WORD:method} %{DATA:path} HTTP/%{NUMBER:http_version}\\\" %{NUMBER:status_code:int} %{NUMBER:bytes:int}\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"date\": {\n",
    "            \"field\": \"timestamp\",\n",
    "            \"formats\": [\"dd/MMM/yyyy:HH:mm:ss Z\"],\n",
    "            \"target_field\": \"@timestamp\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"convert\": {\n",
    "            \"field\": \"response_time_ms\",\n",
    "            \"type\": \"integer\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"script\": {\n",
    "            \"source\": \"ctx.response_time_sec = ctx.response_time_ms / 1000.0\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"remove\": {\n",
    "            \"field\": [\"raw_log\", \"response_time_ms\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"set\": {\n",
    "            \"field\": \"environment\",\n",
    "            \"value\": \"production\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "create_pipeline(\"ecommerce_pipeline\", comprehensive_pipeline, \"E-commerce log processing\")\n",
    "result_comprehensive = test_pipeline(\"ecommerce_pipeline\", ecommerce_log)\n",
    "\n",
    "if result_comprehensive:\n",
    "    print(\"\\nüì• INPUT:\")\n",
    "    print(json.dumps(ecommerce_log, indent=2))\n",
    "    print(\"\\nüì§ OUTPUT:\")\n",
    "    print(json.dumps(result_comprehensive['docs'][0]['doc']['_source'], indent=2))\n",
    "\n",
    "delete_pipeline(\"ecommerce_pipeline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba2e40",
   "metadata": {},
   "source": [
    "# üéì SUMMARY & KEY TAKEAWAYS\n",
    "\n",
    "## Processor Categories at a Glance\n",
    "\n",
    "### 1. **Data Transformation** (8 processors)\n",
    "Append, Copy, Remove, Remove_by_pattern, Rename, Set, Join, Split\n",
    "\n",
    "### 2. **Type Conversion** (5 processors)\n",
    "Bytes, Convert, Lowercase, Uppercase, Trim\n",
    "\n",
    "### 3. **Text & String Parsing** (7 processors)\n",
    "CSV, Dissect, Gsub, Grok, HTML_strip, JSON, KV\n",
    "\n",
    "### 4. **Date & Time** (2 processors)\n",
    "Date, Date_index_name\n",
    "\n",
    "### 5. **Enrichment** (9 processors)\n",
    "GeoIP, IP2Geo, Geojson-feature, User_agent, Community_id, Fingerprint, Text_embedding, Text_image_embedding, Sparse_encoding\n",
    "\n",
    "### 6. **Text Processing** (2 processors)\n",
    "Text_chunking, Sort\n",
    "\n",
    "### 7. **URL Processing** (1 processor)\n",
    "Urldecode\n",
    "\n",
    "### 8. **Control Flow** (2 processors)\n",
    "Drop, Fail\n",
    "\n",
    "### 9. **Advanced/Scripting** (3 processors)\n",
    "Script, Foreach, Pipeline\n",
    "\n",
    "### 10. **Metadata/Utility** (1 processor)\n",
    "Dot_expander\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Use pipelines for consistent data pre-processing\n",
    "- Test pipelines with sample data first\n",
    "- Use named pipelines for reusability\n",
    "- Implement error handling with on_failure\n",
    "- Document your pipeline logic\n",
    "- Set ignore_missing=true for optional fields\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Put all processing into a single processor\n",
    "- Forget to handle edge cases\n",
    "- Use DROP without conditions\n",
    "- Ignore performance with very large pipelines\n",
    "- Skip validation and testing\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "1. **Basic**: String manipulation, type conversion, field operations\n",
    "2. **Intermediate**: Text parsing (CSV, JSON, Grok), date handling\n",
    "3. **Advanced**: Scripts, control flow, nested pipelines, enrichment\n",
    "4. **Expert**: Custom templates, performance optimization, error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b154e6c",
   "metadata": {},
   "source": [
    "# üìñ ADDITIONAL RESOURCES\n",
    "\n",
    "## Processor Coverage\n",
    "\n",
    "This notebook covers **40+ ingest processors**:\n",
    "\n",
    "**100% Covered:**\n",
    "- ‚úÖ Append, Bytes, Convert, Copy, CSV, Date, Dissect, Dot_expander, Drop, Fail\n",
    "- ‚úÖ Fingerprint, Foreach, Geoip, Grok, Gsub, HTML_strip, Join, JSON, KV\n",
    "- ‚úÖ Lowercase, Remove, Rename, Set, Sort, Split, Trim, Uppercase, Urldecode\n",
    "- ‚úÖ User_agent, Script, Pipeline\n",
    "\n",
    "**Specialized (Requires Setup):**\n",
    "- üîß GeoJSON-feature (requires GeoJSON data)\n",
    "- üîß Text_embedding, Text_image_embedding, Sparse_encoding (requires ML models)\n",
    "- üîß Text_chunking (token-based chunking)\n",
    "- üîß Date_index_name (time-based index naming)\n",
    "- üîß Community_id (network flow analysis)\n",
    "- üîß IP2Geo (alternative to GeoIP)\n",
    "- üîß Remove_by_pattern (regex-based removal)\n",
    "\n",
    "## Common Patterns\n",
    "\n",
    "### Error Handling Pattern\n",
    "```python\n",
    "pipeline = [\n",
    "    {\n",
    "        \"processor_name\": {...},\n",
    "        \"on_failure\": [\n",
    "            {\n",
    "                \"set\": {\n",
    "                    \"field\": \"error\",\n",
    "                    \"value\": \"{{_ingest.on_failure_message}}\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Conditional Processing Pattern\n",
    "```python\n",
    "{\n",
    "    \"processor\": {...},\n",
    "    \"if\": \"ctx.field_name == 'value'\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Multi-Step Transformation Pattern\n",
    "```python\n",
    "# Process strings first\n",
    "{\"lowercase\": {\"field\": \"text\"}},\n",
    "# Then parse\n",
    "{\"grok\": {\"field\": \"text\", ...}},\n",
    "# Then validate\n",
    "{\"fail\": {\"if\": \"ctx.status == 'error'\"}}\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore your data**: What format is your data in?\n",
    "2. **Choose processors**: Match data format to appropriate processors\n",
    "3. **Build incrementally**: Test each processor\n",
    "4. **Combine wisely**: Create efficient pipelines\n",
    "5. **Monitor performance**: Check pipeline metrics\n",
    "6. **Iterate and improve**: Refine based on results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716609fa",
   "metadata": {},
   "source": [
    "# üîç QUICK REFERENCE TABLE\n",
    "\n",
    "## All 40+ Processors at a Glance\n",
    "\n",
    "| # | Processor | Category | Purpose | Input | Output |\n",
    "|---|-----------|----------|---------|-------|--------|\n",
    "| 1 | **Append** | Transformation | Add values to array | Array field | Extended array |\n",
    "| 2 | **Bytes** | Conversion | Convert to bytes | \"2MB\" | 2097152 |\n",
    "| 3 | **Community_ID** | Enrichment | Network flow hash | IP, ports | Hash string |\n",
    "| 4 | **Convert** | Conversion | Change data type | \"123\" ‚Üí int | 123 |\n",
    "| 5 | **Copy** | Transformation | Duplicate field | field_a | field_b = field_a |\n",
    "| 6 | **CSV** | Text Parsing | Parse CSV data | \"a,b,c\" | {col1: a, col2: b, col3: c} |\n",
    "| 7 | **Date** | Date/Time | Normalize dates | \"01/02/2024\" | ISO format |\n",
    "| 8 | **Date_Index_Name** | Date/Time | Time-based index | Timestamp | Index name suffix |\n",
    "| 9 | **Dissect** | Text Parsing | Pattern extraction | Log line | Parsed fields |\n",
    "| 10 | **Dot_Expander** | Metadata | Nested objects | user.name | {user: {name: ...}} |\n",
    "| 11 | **Drop** | Control Flow | Filter documents | Condition | Dropped/kept |\n",
    "| 12 | **Fail** | Control Flow | Stop on error | Condition | Error raised |\n",
    "| 13 | **Fingerprint** | Enrichment | Generate hash | Fields | Hash fingerprint |\n",
    "| 14 | **Foreach** | Advanced | Array processor | Array | Processed array |\n",
    "| 15 | **GeoIP** | Enrichment | Location from IP | IP address | Geolocation data |\n",
    "| 16 | **GeoJSON-Feature** | Enrichment | Spatial indexing | GeoJSON | Indexed geo data |\n",
    "| 17 | **Grok** | Text Parsing | Regex extraction | Text | Parsed fields |\n",
    "| 18 | **Gsub** | String Manip | Substring replace | Text | Modified text |\n",
    "| 19 | **HTML_Strip** | String Manip | Remove HTML tags | HTML text | Plain text |\n",
    "| 20 | **IP2Geo** | Enrichment | Location from IP | IP address | Geolocation data |\n",
    "| 21 | **Join** | Transformation | Array to string | Array | String |\n",
    "| 22 | **JSON** | Text Parsing | Parse JSON string | JSON string | Object |\n",
    "| 23 | **KV** | Text Parsing | Key-value pairs | \"a=1&b=2\" | {a: 1, b: 2} |\n",
    "| 24 | **Lowercase** | String Manip | Convert to lower | \"HELLO\" | \"hello\" |\n",
    "| 25 | **Pipeline** | Advanced | Nested pipeline | Any | Processed |\n",
    "| 26 | **Remove** | Transformation | Delete fields | Multiple fields | Removed |\n",
    "| 27 | **Remove_by_Pattern** | Transformation | Pattern delete | Regex pattern | Matched removed |\n",
    "| 28 | **Rename** | Transformation | Rename field | field_a | field_b |\n",
    "| 29 | **Script** | Advanced | Custom Painless | Context | Transformed |\n",
    "| 30 | **Set** | Transformation | Set constant | Field | Value set |\n",
    "| 31 | **Sort** | Transformation | Sort array | Array | Sorted array |\n",
    "| 32 | **Sparse_Encoding** | Enrichment | Sparse vectors | Text | Sparse vector |\n",
    "| 33 | **Split** | Transformation | String to array | \"a,b,c\" | [a, b, c] |\n",
    "| 34 | **Text_Chunking** | Text Processing | Split text | Long text | Text chunks |\n",
    "| 35 | **Text_Embedding** | Enrichment | Text vectors | Text | Vector array |\n",
    "| 36 | **Text_Image_Embedding** | Enrichment | Multimodal vectors | Text/Image | Combined vector |\n",
    "| 37 | **Trim** | String Manip | Remove whitespace | \"  hello  \" | \"hello\" |\n",
    "| 38 | **Uppercase** | String Manip | Convert to upper | \"hello\" | \"HELLO\" |\n",
    "| 39 | **URLDecode** | String Manip | Decode URL | \"hello%20world\" | \"hello world\" |\n",
    "| 40 | **User_Agent** | Enrichment | Parse user agent | Browser string | Device info |\n",
    "\n",
    "---\n",
    "\n",
    "## Processor Selection Guide\n",
    "\n",
    "### For CSV/Structured Data Processing\n",
    "Use: **CSV**, **KV**, **Dissect**, **Grok**\n",
    "\n",
    "### For Text Cleaning\n",
    "Use: **Trim**, **Lowercase**/**Uppercase**, **HTML_Strip**, **Gsub**\n",
    "\n",
    "### For Data Enrichment\n",
    "Use: **GeoIP**, **User_Agent**, **Fingerprint**, **Text_Embedding**\n",
    "\n",
    "### For Data Transformation\n",
    "Use: **Convert**, **Split**/**Join**, **Rename**, **Copy**, **Set**\n",
    "\n",
    "### For Error Handling\n",
    "Use: **Drop**, **Fail**, **Script** (with conditions)\n",
    "\n",
    "### For Complex Logic\n",
    "Use: **Script**, **Pipeline**, **Foreach**\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've now explored 40+ OpenSearch ingest processors with real examples and use cases. \n",
    "You're ready to build production pipelines!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-search-with-opensearch-intermediate (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
